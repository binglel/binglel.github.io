<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Kaldi语音识别实战_陈果果&amp;都家宇&amp;那兴宇&amp;张俊博</title>
      <link href="2021/02/23/Book006/"/>
      <url>2021/02/23/Book006/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p align="left">  <img src= "/img/loading.gif" data-lazy-src="/img/books/book_006-02.jpg" width = 240 height = 320/></p><ul><li><p>第一章 语音识别技术基础</p></li><li><p>第二章 Kaldi概要介绍</p></li><li><p>第三章 数据整理</p></li><li><p>第四章 经典声学建模技术</p></li><li><p>第五章 构图和解码</p></li><li><p>第六章 深度学习声学建模技术</p></li><li><p>第七章 关键词搜索与语音唤醒</p></li><li><p>第八章 说话人识别</p></li><li><p>第九章 语音识别应用实践</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Book </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语音唤醒 </tag>
            
            <tag> 说话人识别 </tag>
            
            <tag> Kaldi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>解析深度学习：语音识别实践_俞栋&amp;邓力（俞凯&amp;钱彦旻等译）</title>
      <link href="2021/02/23/Book005/"/>
      <url>2021/02/23/Book005/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p align="left">  <img src= "/img/loading.gif" data-lazy-src="/img/books/book_005-02.jpg" width = 240 height = 320/></p><ul><li><p>第一章 简介</p></li><li><p>第二章 混合高斯模型</p></li><li><p>第三章 隐马尔可夫模型及其变体</p></li><li><p>第四章 深度神经网络</p></li><li><p>第五章 高级模型初始化技术</p></li><li><p>第六章 深度神经网络-隐马尔可夫模型混合系统</p></li><li><p>第七章 训练和解码的加速</p></li><li><p>第八章 深度神经网络序列鉴别性训练</p></li><li><p>第九章 深度神经网络中的特征表示学习</p></li><li><p>第十章 深度神经网络和混合高斯模型的融合</p></li><li><p>第十一章 深度神经网络的自适应技术</p></li><li><p>第十二章 深度神经网络中的表征共享和迁移</p></li><li><p>第十三章 循环神经网络及相关模型</p></li><li><p>第十四章 计算型网络</p></li><li><p>第十五章 总结及未来研究方向</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Book </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> GMM </tag>
            
            <tag> HMM </tag>
            
            <tag> GMM-HMM </tag>
            
            <tag> DNN-HMM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数位语音处理概论_李琳山_国立台湾大学</title>
      <link href="2021/02/23/Tutorial003/"/>
      <url>2021/02/23/Tutorial003/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/104S204">数位语音处理概论（官网：视频+课件）</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 信号处理 </tag>
            
            <tag> 说话人认证 </tag>
            
            <tag> 声学模型 </tag>
            
            <tag> 语言模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>现代语音信号处理_胡航</title>
      <link href="2021/02/22/Book002/"/>
      <url>2021/02/22/Book002/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p align="left">  <img src= "/img/loading.gif" data-lazy-src="/img/books/book_002-02.jpg" width = 240 height = 320/></p><ul><li><p>第一章 绪论</p></li><li><p>第二章 语音信号处理的基础知识</p></li><li><p>第三章 时域分析</p></li><li><p>第四章 短时傅里叶分析</p></li><li><p>第五章 倒谱分析与同态滤波</p></li><li><p>第六章 线性预测分析</p></li><li><p>第七章 语音信号的非线性分析</p></li><li><p>第八章 语音特征参数估计</p></li><li><p>第九章 矢量量化</p></li><li><p>第十章 隐马尔可夫模型</p></li><li><p>第十一章 语音编码</p></li><li><p>第十二章 语音合成</p></li><li><p>第十三章 语音识别</p></li><li><p>第十四章 说话人识别</p></li><li><p>第十五章 智能信息处理技术在语音信号处理中的应用</p></li><li><p>第十六章 语音增强</p></li><li><p>第十七章 基于麦克风阵列的语音信号处理</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Book </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习_周志华</title>
      <link href="2021/02/22/Book001/"/>
      <url>2021/02/22/Book001/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p align="left">  <img src= "/img/loading.gif" data-lazy-src="/img/books/book_001-02.jpg" width = 240 height = 320/></p><ul><li><p>第一章 绪论</p></li><li><p>第二章 模型评估与选择</p></li><li><p>第三章 线性模型</p></li><li><p>第四章 决策树</p></li><li><p>第五章 神经网络</p></li><li><p>第六章 支持向量机</p></li><li><p>第七章 贝叶斯分类器</p></li><li><p>第八章 集成学习</p></li><li><p>第九章 聚类</p></li><li><p>第十章 降维与度量学习</p></li><li><p>第十一章 特征选择与稀疏学习</p></li><li><p>第十二章 计算学习理论</p></li><li><p>第十三章 半监督学习</p></li><li><p>第十四章 概率图模型</p></li><li><p>第十五章 规则学习</p></li><li><p>第十六章 强化学习</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Book </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_ICLR2018_Monotonic Chunkwise Attention</title>
      <link href="2021/02/21/Paper036/"/>
      <url>2021/02/21/Paper036/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>软注意力序列间模型已成功地应用于各种各样的问题，但其解码过程需要大量的时间和空间开销，不适用于实时序列转换。为了解决这些问题，我们提出了单调的组块注意力（MOCHA），它自适应地将输入序列分割成小块，在这些小块上计算软注意。我们证明了使用MOCHA的模型可以在标准反向传播的情况下有效地训练，同时允许在测试时进行在线和线性时间解码。当应用于在线语音识别时，我们获得了最先进的结果，并与使用离线软注意力机制的模型性能进行了比较。在文档总结的实验中，在没有进行单调对齐的情况下，与基于单调注意力的基线模型相比，该模型表现出显著的改进性能。</p><p><strong>参考</strong><br>[1] <em>Chiu C C, Raffel C. Monotonic chunkwise attention[J]. arXiv preprint arXiv:1712.05382, 2017.</em><a href="https://arxiv.org/abs/1712.05382">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> Attention </tag>
            
            <tag> Sequence-to-Sequence </tag>
            
            <tag> MoChA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_20160804_A Neural Transducer</title>
      <link href="2021/02/21/Paper035/"/>
      <url>2021/02/21/Paper035/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>序列到序列模型已经在各种任务上取得了令人印象深刻的结果。然而，它们不适用于需要随着更多数据到来而进行增量预测的任务，或者具有长输入序列和输出序列的任务。这是因为它们生成以整个输入序列为条件的输出序列。在本文中，我们提出了一种神经转换器，它可以在更多的输入到来时进行增量预测，而不需要重新进行整个输入序列计算。与序列到序列模型不同，神经转换器根据部分观察到的输入序列和部分生成的序列来计算下一步分布。在每个时间步，转换器可以决定发射零到多个输出符号。数据可以使用编码器进行处理，并作为转换器的输入。在每个时间步发射一个符号的离散决策使得用传统的反向传播很难学习。然而，可以通过使用动态规划算法来训练转换器以产生离散决策。我们的实验表明，神经转换器在需要数据输入时产生输出预测的环境下工作良好。我们还发现，即使在没有使用注意力机制的情况下，神经转换器在长序列中也表现得很好。</p><p><strong>参考</strong><br>[1] <em>Jaitly N, Sussillo D, Le Q V, et al. A neural transducer[J]. arXiv preprint arXiv:1511.04868, 2015.</em><a href="https://arxiv.org/abs/1511.04868">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> RNN </tag>
            
            <tag> RNN-T </tag>
            
            <tag> Sequence-to-Sequence </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_Interspeech2017_Recurrent Neural Aligner：An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping</title>
      <link href="2021/02/21/Paper034/"/>
      <url>2021/02/21/Paper034/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>介绍了一种可用于序列到序列映射任务的编解码器循环神经网络模型——循环神经对齐器（RNA）。与连接主义时间分类（CTC）模型类似，RNA定义了目标标签序列上的概率分布，包括与输入中每个时间步长相对应的空白标签。通过在所有可能的空白标签位置上边际化来计算标签序列的概率。与CTC不同的是，RNA不对标签预测做条件独立假设，它在预测t时刻的标签时，将t-1时刻的预测标签作为递归模型的附加输入，我们将该模型应用于端到端语音识别。由于解码器不使用注意机制，因此RNA能够进行流识别。该模型对转录的声学数据进行训练以预测字素，并且不使用外部语言和发音模型来解码。我们使用近似动态规划方法来优化负对数似然率，并使用基于采样的序列判别训练技术来微调模型以最小化期望词错误率。我们证明了该模型在不使用外部语言模型和进行波束搜索解码的情况下达到了具有竞争力的精度。</p><p><strong>参考</strong><br>[1] <em>Sak H, Shannon M, Rao K, et al. Recurrent Neural Aligner: An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping[C]//Interspeech. 2017, 8: 1298-1302.</em><a href="https://www.isca-speech.org/archive/Interspeech_2017/abstracts/1705.html">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> RNN </tag>
            
            <tag> Sequence-to-Sequence </tag>
            
            <tag> RNA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多伦多大学_ICML2012_Sequence Transduction with Recurrent Neural Networks</title>
      <link href="2021/02/21/Paper033/"/>
      <url>2021/02/21/Paper033/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>许多机器学习任务可以表示为输入序列到输出序列的转换：语音识别、机器翻译、蛋白质二级结构预测和文本到语音等。序列转换的关键挑战之一是学会以一种不受序列失真（如收缩、拉伸和平移）影响的方式来表示输入和输出序列。循环神经网络（RNN）是一种具有强大的序列学习结构，已被证明能够学习这样的表示。然而，在传统上，RNN需要输入和输出序列之间的预先定义的对齐来进行转换。这是一个严重的限制，因为对齐是许多序列转换问题中最困难的部分。事实上，甚至是确定输出序列的长度也常常是具有挑战性的。本文介绍了一种完全基于RNN的端到端概率序列转换系统，该系统原则上能够将任何输入序列转换成任何有限的、离散的输出序列。本文给出了在TIMIT语音语料库上进行音素识别的实验结果。</p><p><strong>参考</strong><br>[1] <em>Graves A. Sequence transduction with recurrent neural networks[J]. arXiv preprint arXiv:1211.3711, 2012.</em><a href="https://arxiv.org/abs/1211.3711">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> RNN </tag>
            
            <tag> RNN-T </tag>
            
            <tag> Sequence-to-Sequence </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多伦多大学_ICML2006_Connectionist temporal classification：labelling unsegmented sequence data with recurrent neural networks</title>
      <link href="2021/02/19/Paper032/"/>
      <url>2021/02/19/Paper032/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>许多现实世界中的序列学习任务需要从有噪声的、未切分的输入数据中预测标签序列。例如，在语音识别中，声音信号被转录成词或子词单元。循环神经网络（RNN）是功能强大的序列学习器，似乎非常适合这样的任务。然而，由于它们需要预先切分的训练数据，以及将其输出转换为标签序列的后处理，因此它们的适用性到目前为止一直受到限制。本文提出了一种训练RNN直接标注未切分序列的新方法，从而解决了这两个问题。在TIMIT语音语料库上的实验表明，该方法优于基线HMM和混合HMM-RNN。</p><p><strong>参考</strong><br>[1] <em>Graves A, Fernández S, Gomez F, et al. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks[C]//Proceedings of the 23rd international conference on Machine learning. 2006: 369-376.</em><a href="http://axon.cs.byu.edu/~martinez/classes/778/Papers/p369-graves.pdf">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> CTC </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CMU_ICASSP2016_Listen, Attend and Spell</title>
      <link href="2021/02/19/Paper031/"/>
      <url>2021/02/19/Paper031/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们介绍了Listen, Attend and Spell（LAS），这是一种学习将语音转录成字符的神经网络。与传统的DNN-HMM模型不同，该模型联合学习语音识别器的所有组件。该系统有两个组件：监听器和拼写器。监听器是接受滤波器组频谱作为输入的金字塔循环网络编码器。拼写器是基于注意力的循环网络解码器，以字符作为输出。该网络输出字符序列，而不在字符之间做出任何独立假设。这是LAS相对于以往端到端CTC模型的关键改进。在谷歌语音搜索任务的一个子集上，LAS在没有词典或语言模型的情况下实现了14.1%的词错误率（WER），在前32个波束上进行语言模型重新评分的情况下实现了10.3%的词错误率。相比之下，最先进的CLDNN-HMM模型达到了8.0%的WER。</p><p><strong>参考</strong><br>[1] <em>Chan W, Jaitly N, Le Q V, et al. Listen, attend and spell[J]. arXiv preprint arXiv:1508.01211, 2015.</em><a href="https://arxiv.org/abs/1508.01211">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> BLSTM </tag>
            
            <tag> Attention </tag>
            
            <tag> Sequence-to-Sequence </tag>
            
            <tag> LAS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>华盛顿州立大学_ICASSP2018_Attention-Based Models for Text-Dependent Speaker Verification</title>
      <link href="2021/02/14/Paper030/"/>
      <url>2021/02/14/Paper030/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>最近，基于注意力的模型在语音识别、机器翻译和图像字幕等一系列任务中表现出很好的性能，这是因为它们能够总结贯穿整个输入序列长度的相关信息。本文分析了注意机制在端到端文本相关说话人识别系统中对序列摘要问题的应用。我们探索了注意力层的不同拓扑结构及其变体，并比较了不同的池化方法在注意力权重上的差异。最终实验结果表明，与非注意力LSTM基线模型相比，基于注意力的模型可以使说话人认证系统的等错误率(EER)提高14%。</p><p><strong>参考</strong><br>[1] <em>rahman Chowdhury F A R, Wang Q, Moreno I L, et al. Attention-based models for text-dependent speaker verification[C]//2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018: 5359-5363.</em><a href="https://arxiv.org/abs/1710.10470">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> End-to-End </tag>
            
            <tag> 说话人认证 </tag>
            
            <tag> Attention </tag>
            
            <tag> LSTM </tag>
            
            <tag> TE2E </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DFKI_ICASSP2016_End-to-End Text-Dependent Speaker Verification</title>
      <link href="2021/02/14/Paper029/"/>
      <url>2021/02/14/Paper029/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>本文提出了一种数据驱动的、集成的说话人认证方法，该方法将一条测试语句和几条参考话语直接映射到单个分数进行认证，并使用与测试时相同的评估协议和度量来联合优化系统的组件。这样的方法将产生简单而高效的系统，几乎不需要特定领域的知识和做什么模型假设。我们通过将问题描述为一个单一的神经网络体系结构来实现这一想法，包括只在几条语句上估计说话人模型，并在我们内部的“OK Google”基准上进行评估，以进行依赖于文本的说话人验证。对于像我们这样需要高度精确、易于维护且占用空间小的系统的大数据应用来说，所提出的方法似乎非常有效。</p><p><strong>参考</strong><br>[1] <em>Heigold G, Moreno I, Bengio S, et al. End-to-end text-dependent speaker verification[C]//2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016: 5115-5119.</em><a href="https://arxiv.org/abs/1509.08062v1">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> End-to-End </tag>
            
            <tag> 说话人认证 </tag>
            
            <tag> LSTM </tag>
            
            <tag> TE2E </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习与人类语言处理课程2020_李宏毅_国立台湾大学</title>
      <link href="2021/02/08/Tutorial002/"/>
      <url>2021/02/08/Tutorial002/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_DLHLP20.html">深度学习与人类语言处理课程2020（官网：视频+课件+作业）</a></p></blockquote><blockquote><p><a href="https://www.bilibili.com/video/BV1nt4y1U7Zb?p=1">深度学习与人类语言处理课程2020（完整版）</a></p></blockquote><blockquote><p><a href="https://www.bilibili.com/video/BV1q5411V7tT?from=search&seid=15419541146503431608">深度学习与人类语言处理课程2020之语音识别</a></p></blockquote><ul><li><p>深度学习与人类语言处理课程介绍</p></li><li><p>语音识别课程视频内容</p><ul><li>ASR Summary</li><li>Listen,Attention and Spell(LAS)</li><li>Connectionist Temporal Classification(CTC)</li><li>RNN Transducer(RNN-T)</li><li>Neural Transducer</li><li>Monotonic Chunkwise Attention(MoChA)</li><li>Hidden Markov Model(HMM)</li><li>Alignment of HMM/CTC/RNN-T</li><li>Score Computation of HMM/CTC/RNN-T</li><li>Decoding of HMM/CTC/RNN-T</li><li>Language Model(N-Gram/NN/RNN)</li><li>Shallow/Deep Fusion/Cold Fusion</li></ul></li><li><p>语音转换课程视频内容</p><ul><li>Content/Speaker Encoder-Decoder</li><li>CycleGAN</li><li>StarGAN</li><li>Blow</li></ul></li><li><p>语音分离课程视频内容</p><ul><li>Deep Clustering</li><li>Permutation Invariant Training(PIT)</li><li>Time-domain Audio Separation Network(TasNet)</li></ul></li><li><p>语音合成课程视频内容</p><ul><li>Concatenative Approach</li><li>Parametic Approach</li><li>DeepVoice 1/3</li><li>Tacotron 1/2</li><li>FastSpeech/DurIAN</li><li>Dual Learning</li><li>Speaker Embedding/Global Style Token(GST)</li></ul></li><li><p>语音验证(说话人认证)课程视频内容</p><ul><li>Speaker Embedding</li><li>i-vector</li><li>d-vector</li><li>x-vector</li><li>End-to-End</li></ul></li><li><p>自然语言处理课程视频内容</p><ul><li>Category</li><li>Part-of-Speech(POS) Tagging</li><li>Word Segmentation</li><li>Parsing</li><li>Coreference Resolution</li><li>Summarization</li><li>Machine Translation</li><li>Grammar Error Correction</li><li>Sentiment Classification</li><li>Stance Detection</li><li>Veracity Prediction</li><li>Natural Language Inference(NLI)</li><li>Search Engine</li><li>Question Answering(QA)</li><li>Dialogue</li><li>Natural Language Generation(NLG)</li><li>Natural Language Understanding(NLU)</li><li>Knowledge Extraction</li><li>Name Entity Recognition(NER)</li><li>Relation Extraction    </li><li>BERT</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音识别 </tag>
            
            <tag> 语音转换 </tag>
            
            <tag> 说话人认证 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 语音分离 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_Interspeech2020_Conformer：Convolution-augmented Transformer for Speech Recognition</title>
      <link href="2021/02/02/Paper028/"/>
      <url>2021/02/02/Paper028/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>摘要：近年来，基于Transformer和卷积神经网络（CNN）的模型在自动语音识别（ASR）中取得了优于循环神经网络（RNNs）的良好效果。Transformer模型擅长捕获基于内容的全局交互，而CNN有效地利用局部特征。在这项工作中，我们通过研究如何将卷积神经网络和Transformer结合起来，以一种有效参数的方式对音频序列的局部和全局依赖性进行建模，从而达到两者的最佳效果。为此，我们提出了一种用于语音识别的卷积增强Transformer，称为Conformer。Conformer的性能明显优于之前基于Transformer和CNN的模型，实现了最先进的精确度。在广泛使用的LibriSpeech基准测试上，其中在Test和TestOther数据集上，我们的模型在没有使用语言模型的情况下获得了2.1%和4.3%的WER，在使用外部语言模型的情况下获得了1.9%和3.9%的WER。我们还观察到仅用10M参数的一个小模型的竞争性能为2.7%和6.3%。</p><p><strong>参考</strong><br>[1] <em>Gulati A, Qin J, Chiu C C, et al. Conformer: Convolution-augmented transformer for speech recognition[J]. arXiv preprint arXiv:2005.08100, 2020.</em><a href="https://arxiv.org/abs/2005.08100">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> Attention </tag>
            
            <tag> CNN </tag>
            
            <tag> Transformer </tag>
            
            <tag> Conformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SLT2021-儿童语音识别挑战赛研讨会</title>
      <link href="2021/02/01/Tutorial001/"/>
      <url>2021/02/01/Tutorial001/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="https://www.data-baker.com/csrc_challenge.html">SLT2021-儿童语音识别挑战赛官网</a></p></blockquote><p align="center">  <img src= "/img/loading.gif" data-lazy-src="/img/tutorials/tutorial_001-02.png" width = 520 height = 240/></p><blockquote><p><a href="https://www.bilibili.com/video/BV15N411o7JL">SLT2021-儿童语音识别挑战赛研讨会</a></p></blockquote><ul><li>研讨会视频内容<ul><li>赛道1分享-冠军队伍（上海交通大学智能语音实验室）</li><li>赛道2分享-冠军队伍（小米&amp;微软小冰&amp;Seasalt AI）</li><li>面向端侧部署的流式端到端语音识别技术-谢磊（西北工业大学）</li><li>基于直通梯度的端到端语音识别神经架构搜索-欧智坚（清华大学）</li><li>语音技术及其在美团的应用-黄辰（美团）</li><li>贝克找房的语音技术应用与研究-汤志远（贝克找房）</li><li>数据助力技术，技术助力数据-吴本谷（标贝科技）</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_ICASSP2018_Generalized End-to-End Loss for Speaker Verification</title>
      <link href="2021/01/27/Paper027/"/>
      <url>2021/01/27/Paper027/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>本文提出了一种新型的损失函数，称为广义端到端（GE2E）损失函数，使得说话人认证模型的训练比以前基于元组的端到端（TE2E）损失函数更加有效。与TE2E不同，GE2E损失函数以强调实例的方式更新网络，这些实例在训练过程的每个步骤中都难以验证。此外，GE2E损失函数不需要选择示例的初始阶段。利用这些特性，我们的新损失函数模型使说话人验证EER降低了10%以上，同时使训练时间减少了60%。从而学习出一个更好的模型。我们还介绍了MultiReader技术，它允许我们进行域自适应训练一个更精确的模型——支持多个关键字（即OK Google和Hey Google）以及多个方言。</p><p><strong>参考</strong><br>[1] <em>Wan L, Wang Q, Papir A, et al. Generalized end-to-end loss for speaker verification[C]//2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018: 4879-4883.</em><a href="https://arxiv.org/abs/1710.10467">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> End-to-End </tag>
            
            <tag> 说话人认证 </tag>
            
            <tag> LSTM </tag>
            
            <tag> GE2E </tag>
            
            <tag> TE2E </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>下班后要不要回工作消息——从八小时工作制说起</title>
      <link href="2021/01/25/Essay001/"/>
      <url>2021/01/25/Essay001/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>“八小时工作制是一个进步”，今天看了奇葩说，对刘擎教授这句话印象很深刻。</p><p>刚巧今早上看到豆瓣一篇抵制拼多多的帖子，核心观点即是拼多多招聘要求员工9-9-6甚至11-11-6，更可怕的是，作者在后续面试其他公司的时候，对于工作时间提出异议，HR称，最起码比你面试的拼多多强。</p><p>八小时工作制应该是一个基本的要求，而不是让一个又一个的互联网巨头一点点突破劳动法限制，甚至成为一个冠冕堂皇的优待理由。</p><p>前几年，当马云公开宣称9-9-6时，大概还是骂声居多，而现在，9-9-6已成社会常态。</p><p>结合前阵子拼多多、饿了么员工猝死事件，虽然我们提倡拼搏，但人不是工具，社会进步应当给不同层次的人以生存空间而非适者生存，我们的社会进化了那么多年，不是为了让社会法则发展为社会进化论。</p><p>就我们个体而言，如果对于不公的社会事件，每个人都觉得与己无关而闭口不言，那么最后总有一个与你有关的对你不利的决策或事件，到时大家都已学会闭口不言。</p><p>又说到郑爽事件，为何社会对于代孕反对的声音如此激烈，根本原因在于代孕是将人体当作工具，如果子宫可以被标价、胎儿可以被买卖，那么这个范围迟早会被扩大，肾脏、器官，乃至生命。</p><p>个人的身体自己可以做主吗，在一定程度上说，你的身体不属于你，你不能自行对其处置。就代孕以及弃养事件而言，从根本上反映出的还是社会对于人的物化反感以及对己对于弱者的处境的同情，乃至害怕，社会上毕竟弱者是大多数。</p><p>与9-9-6所一致，现代社会节奏越来越快，但人不应仅仅被当作追求利益的工具，社会生活方便带来的是人个人时间的碎片化、压缩化，何时何地都能够被找到，真的是一件好事吗？</p><p>从个人朴素的价值观而言，每个人有每个人不同的人生追求，有人热爱工作有人仅仅将工作作为挣钱工具。</p><p>一部分人将工作上的成功视为自己的毕生追求，自有不成功便成仁一说，对于那类人而言，世俗意义上的成功能够让其获得极大的成就感和满足感，普通人的生活会让他感到极大的痛苦。在这种目标导向下，将人生燃烧给自己的事业是个人选择，疯狂拼搏之后的成功或失败只是选择之后带来的一个结果而已。对于此类人而言，工作就是生活，八小时当然不能简单适用。</p><p>但也有其他一部分人，喜欢简简单单的生活，工作中的一个小成就，生活中的一点小确幸，足够感到幸福开心。或有人将工作视为人生的一个路径，赚钱也好，打发时间也罢，只是给自己找点事做。</p><p>工作只是生活的一部分，与下厨、读书、徒步、蹦迪一样，应当是由人自行安排的人生事件之一，相对只是工作占据的时间较多而已，而对于占据人生已经绝大多数时间的工作，如果我们并不想却还毫无底线的任其扩大范围和界限，那么生活何谓生活，家人、自己位于何处，不得而知。</p><p>所谓下班后工作消息要不要回，其实，作为辩题，其讨论之根本就在于每个人想法不一致，如上，对于第一类人而言，生活就是工作，就不存在下班一说了，可能他人生中的所有事情在一定阶段范畴内都要为工作让位。对于其他人而言，工作既然有上下班一说，势必从本心上讲是不希望自己下班后的其他安排被工作打乱的，哪怕只是一个小小的消息。</p><p>说这么多，下班后领导给我发消息，心里说：“呸，别打扰我生活”。实际上还不得立刻：“好的收到”。所谓社畜，不外如是。</p><p>那这篇乱七八糟没有核心思想的文字，就作为马东所说“屈服于生活的苟且，但依然怀抱诗和远方”的见证吧。</p>]]></content>
      
      
      <categories>
          
          <category> Essay </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生活随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>出门问问_ICASSP2019_Adversarial Examples for Improving End-to-end Attention-based Small-Footprint Keyword Spotting</title>
      <link href="2021/01/24/Paper026/"/>
      <url>2021/01/24/Paper026/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>在这篇文章中，我们探索使用对抗性样例来改进一个基于神经网络的关键词检测（KWS）系统。特别地，在我们的系统中，使用了一种有效的、小占用空间的基于注意力的神经网络模型。对抗性样例被模型定义为错误分类的样例，但它与原始的正确分类的样例相比只有轻微的偏差。在KWS任务中，将误唤醒或误拒绝的查询视为某种对抗性的样例是很自然的。在我们的工作中，在给定一个训练有素的基于注意力的KWS模型的情况下，我们首先使用快速梯度符号方法（FGSM）生成对抗性样例，发现这些样例会显著降低KWS的性能。使用这些对抗性样例作为扩充数据，对KWS模型进行再训练，最终在从智能说话人收集的数据集上以每小时1.0次的误报率（FAR）取得了45.6%的相对拒绝率和误拒率（FRR）。</p><p><strong>参考</strong><br>[1] <em>Wang X, Sun S, Shan C, et al. Adversarial examples for improving end-to-end attention-based small-footprint keyword spotting[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019: 6366-6370.</em><a href="http://ssli.ee.washington.edu/~mhwang/mobvoi/2019/icassp-wang.pdf">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音唤醒 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>出门问问_ICASSP2019_Knowledge Distillation for Recurrent Neural Network Language Modeling with Trust Regularization</title>
      <link href="2021/01/24/Paper025/"/>
      <url>2021/01/24/Paper025/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>循环神经网络（RNNs）因其优于传统的基于N元语法的模型而在语言建模中占据主导地位。在许多应用中，通常使用大型循环神经网络语言模型（RNNLM）或几个RNNLM的集成。这些模型占用的内存很大且需要大量的计算。在这篇文章中，我们研究了应用知识蒸馏来降低RNNLMs模型大小的效果。此外，我们还提出了一种信任正则化方法来改进RNNLM的知识提炼训练。利用信任正则化的知识精馏，我们将参数大小减少到以前发表的最佳模型的三分之一，同时保持了对Penn Treebank数据的最先进困惑度结果。在语音识别N-bestrescoring任务中，我们将RNNLM模型的规模减少到基线系统的18.5%，而在华尔街日报数据集上的错词率（WER）性能没有下降。</p><p><strong>参考</strong><br>[1] <em>Shi Y, Hwang M Y, Lei X, et al. Knowledge distillation for recurrent neural network language modeling with trust regularization[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019: 7230-7234.</em><a href="https://arxiv.org/abs/1904.04163">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语言模型 </tag>
            
            <tag> RNNLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>出门问问_ICASSP2019_End-To-End Speech Recognition Using A High Rank LSTM-CTC Based Model</title>
      <link href="2021/01/24/Paper024/"/>
      <url>2021/01/24/Paper024/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>基于 LSTM-CTC 的端到端语音模型， 由于其简单的训练过程以及高效的解码过程，最近在语音识别领域得到广泛地应用。在传统的 LSTM-CTC 模型中，瓶颈投影矩阵将从LSTM获得的隐藏特征向量映射到Softmax输出层。在本文中，我们提出用高阶投影层来代替投影矩阵。高阶投影层的输出是通过不同的投影矩阵和非线性激活函数从隐藏特征向量投影的向量加权组合。高阶投影层能够提高LSTM-CTC模型的表达能力。实验结果表明，在华尔街日报（WSJ）语料库和LibriSpeech数据集上，该方法比基准CTC系统的相对单词错误率（WER）降低了4%-6%。在不使用额外数据或数据增强的情况下，该模型的性能优于其他已发表的基于CTC的端到端（E2E）模型。</p><p><strong>参考</strong><br>[1] <em>Shi Y, Hwang M Y, Lei X. End-to-end speech recognition using a high rank lstm-ctc based model[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019: 7080-7084.</em><a href="https://arxiv.org/abs/1903.05261">[pdf]</a></p><p><strong>源码</strong><br>[1] lstm_ctc<a href="https://github.com/mobvoi/lstm_ctc">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> CTC </tag>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>出门问问_20201210_Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition</title>
      <link href="2021/01/23/Paper023/"/>
      <url>2021/01/23/Paper023/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>在本文中，我们提出了一种新型的两遍方法来将流式和非流式端到端（E2E）语音识别统一在一个模型中。该模型采用混合CTC和注意力架构，对编码器中的一致性层进行了修改。我们提出了一种基于动态组块的注意策略来允许输入任意合适的上下文长度。在推理时，CTC解码器以流式方式生成n个最佳假设。只需改变块大小，就可以很容易地控制推理延迟。然后，注意力解码器对CTC假设进行重新评分以得到最终结果。这种高效的重新评分过程只会导致非常小的句子级延迟。在开放170小时的AISHELL-1数据集上的实验表明，该方法能够简单有效地统一流式模型和非流式模型。在AISHELL-1测试集上，与标准非流式Tansformer相比，我们的统一模型在非流式ASR上的相对字符错误率（CER）降低了5.60%。同样的模型在流式ASR系统中获得了5.42%的CER和640ms的延迟。</p><p><strong>参考</strong><br>[1] <em>Zhang B, Wu D, Yao Z, et al. Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition[J]. arXiv preprint arXiv:2012.05481, 2020.</em><a href="https://arxiv.org/abs/2012.05481">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> CTC </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>百度_Interspeech2018_Cold Fusion：Training Seq2Seq Models Together with Language Models</title>
      <link href="2021/01/19/Paper022/"/>
      <url>2021/01/19/Paper022/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>基于注意力的序列到序列（Seq2Seq）模型在机器翻译、图像字幕和语音识别等生成自然语言句子的任务中表现出色。通过利用未标记的数据（通常以语言模型的形式），进一步提高了性能。在这项工作中，我们提出了冷融合方法，该方法在训练过程中利用预先训练好的语言模型，并在语音识别任务中证明了该方法的有效性。结果表明，采用冷融合的Seq2Seq模型能够更好地利用语言信息，1）具有更快的收敛速度和更好的泛化能力；2）在不到10%的标记训练数据的情况下，几乎完全迁移到新的领域。</p><p><strong>参考</strong><br>[1] <em>Sriram A, Jun H, Satheesh S, et al. Cold fusion: Training seq2seq models together with language models[J]. arXiv preprint arXiv:1708.06426, 2017.</em><a href="https://arxiv.org/abs/1708.06426">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> Attention </tag>
            
            <tag> Sequence-to-Sequence </tag>
            
            <tag> Deep Fusion </tag>
            
            <tag> Cold Fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>百度_20171105_Robust Speech Recognition Using Generative Adversarial Networks</title>
      <link href="2021/01/19/Paper021/"/>
      <url>2021/01/19/Paper021/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>本文描述了一个通用的、可扩展的、端到端的框架，该框架使用生成对抗网络（GAN）目标来实现鲁棒性语音识别。通过学习将有噪声的音频映射到与干净音频相同的嵌入空间，使用该方法训练的编码器具有改善后的不变性。与以往的方法不同，新的框架不依赖于信号处理中经常需要的领域专业知识或简化假设，而是以数据驱动的方式直接激励鲁棒性。实验结果表明，该方法在不需要专门的前端或预处理的情况下，改善了普通的序列到序列模型的模拟远场语音识别。</p><p><strong>参考</strong><br>[1] <em>Sriram A, Jun H, Gaur Y, et al. Robust speech recognition using generative adversarial networks[C]//2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018: 5639-5643.</em><a href="https://arxiv.org/abs/1711.01567">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> Attention </tag>
            
            <tag> 语音增强 </tag>
            
            <tag> SEGAN </tag>
            
            <tag> WGAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>百度_ASRU2017_Exploring Neural Transducers for End-to-End Speech Recognition</title>
      <link href="2021/01/19/Paper020/"/>
      <url>2021/01/19/Paper020/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>在这项工作中，我们对端到端语音识别的CTC模型、RNN-Transducer模型和基于注意力的Seq2Seq模型进行了实证比较。我们证明，在没有任何语言模型的情况下，Seq2Seq和RNN-Transducer模型在流行的Hub5‘00基准上都优于报道的最好的有语言模型的CTC模型。在我们内部多样化的数据集上，这些趋势仍在继续——RNN-Transducer模型在beam search后使用语言模型进行重新评分，表现优于我们最好的CTC模型。这些结果简化了语音识别流水线，因此解码现在可以纯粹表示为神经网络操作。我们还研究了编码器体系结构的选择如何影响这三种模型的性能——当所有编码层都是仅前向的，以及当编码器积极地对输入表示进行下采样时。</p><p><strong>参考</strong><br>[1] <em>Battenberg E, Chen J, Child R, et al. Exploring neural transducers for end-to-end speech recognition[C]//2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017: 206-213.</em><a href="https://arxiv.org/abs/1707.07413">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> CTC </tag>
            
            <tag> Attention </tag>
            
            <tag> RNN-T </tag>
            
            <tag> Sequence-to-Sequence </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>百度_ICML2015_Deep Speech 2：End-to-End Speech Recognition in English and Mandarin</title>
      <link href="2021/01/18/Paper019/"/>
      <url>2021/01/18/Paper019/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们证明了一种端到端的深度学习方法可以用于识别英语或普通话——两种截然不同的语言。由于该方法用神经网络取代了整个手工设计的组件管道，端到端学习使我们能够处理各种各样的语音，包括嘈杂环境、口音和不同的语言。我们方法的关键是我们的HPC（高性能计算）技术的应用，以致于比我们以前的系统提速7倍。由于这种效率，以前需要几周时间的实验，而现在只需几天就能完成。这使我们能够更快地迭代以验证更优秀的架构和算法。因此，在一些情况下，当以标准数据集为基准时，我们的系统与人类工作者的转录结果一样好。最后，通过在数据中心使用GPUs进行批量调度，我们证明了我们的系统可以以低廉的成本在线部署，并在大规模服务用户时提供低延迟。</p><p><strong>参考</strong><br>[1] <em>Amodei D, Ananthanarayanan S, Anubhai R, et al. Deep speech 2: End-to-end speech recognition in english and mandarin[C]//International conference on machine learning. 2016: 173-182.</em><a href="https://arxiv.org/abs/1512.02595">[pdf]</a></p><p><strong>源码</strong><br>[1] DeepSpeech-PaddlePaddle<a href="https://github.com/PaddlePaddle/DeepSpeech">[GitHub]</a><br>[2] DeepSpeech-TensorFlow<a href="https://github.com/mozilla/DeepSpeech">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 声学模型 </tag>
            
            <tag> 解码器 </tag>
            
            <tag> CTC </tag>
            
            <tag> 语言模型 </tag>
            
            <tag> RNN </tag>
            
            <tag> CNN </tag>
            
            <tag> GRU </tag>
            
            <tag> LSTM </tag>
            
            <tag> KenLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>康奈尔大学_UAI2018_Averaging Weights Leads to Wider Optima and Better Generalization</title>
      <link href="2021/01/16/Paper018/"/>
      <url>2021/01/16/Paper018/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>深度神经网络通常通过优化具有SGD变量的损失函数，结合衰减的学习率来训练，直到收敛。我们发现，在使用周期性或固定学习率情况下，沿着SGD轨迹的多个点的简单平均比传统的训练具有更好的泛化能力。我们还发现，这种随机加权平均(SWA)方法得到了比SGD方法更平坦的解，并且用一个单一模型逼近了最近的快速几何集成(FGE)方法。我们把SWA用在CIFAR-10、CIFAR-100和ImageNet中一系列最先进的残差网络、金字塔网、DenseNets和Shake-Shake网络上，其测试精度比传统的SGD训练获得了显著改进。简而言之，SWA非常容易实现，提高了通用性，并且几乎没有计算开销。</p><p><strong>参考</strong><br>[1] <em>Izmailov P, Podoprikhin D, Garipov T, et al. Averaging weights leads to wider optima and better generalization[J]. arXiv preprint arXiv:1803.05407, 2018.</em><a href="https://arxiv.org/abs/1803.05407">[pdf]</a></p><p><strong>源码</strong><br>[1] swa<a href="https://github.com/timgaripov/swa">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随机加权平均 </tag>
            
            <tag> 周期性学习率 </tag>
            
            <tag> 随机梯度下降 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>昆士兰科技大学_20201225_SWA Object Detection</title>
      <link href="2021/01/13/Paper017/"/>
      <url>2021/01/13/Paper017/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>你想为目标检测器改进1.0 的平均精度（AP），而不需要任何推理成本和对检测器进行任何更改吗？让我们告诉你这样一个方案吧。该方案非常简单：使用周期性学习率训练您的检测器以获得额外的12个epoch，然后将这12个产生的模型取平均作为你的最终检测模型。这种有效的方法是受到随机加权平均算法的启发，该算法是在<a href="https://arxiv.org/abs/1803.05407">此论文</a>中被提出，目的是提高深度神经网络的泛化能力。我们发现它在目标检测方面也非常有效。在这份技术报告中，我们系统地研究了随机加权平均（SWA）在目标检测和实例分割中的应用效果。通过大量的实验，我们发现一种使用SWA进行目标检测的良好策略，并且在具有挑战性的COCO基准测试中，我们在各种流行的检测器上一致地实现了约1.0的平均精度改进。我们希望这项工作能够使更多的目标检测研究人员了解这项技术，并帮助他们训练出更好的目标检测器。</p><p><strong>参考</strong><br>[1] <em>Zhang H, Wang Y, Dayoub F, et al. SWA Object Detection[J]. arXiv preprint arXiv:2012.12645, 2020.</em><a href="https://arxiv.org/abs/2012.12645">[pdf]</a></p><p><strong>源码</strong><br>[1] swa_object_detection<a href="https://github.com/hyz-xmaster/swa_object_detection">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> 随机加权平均 </tag>
            
            <tag> 周期性学习率 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>百度_20141219_Deep Speech：Scaling up end-to-end speech recognition</title>
      <link href="2021/01/11/Paper016/"/>
      <url>2021/01/11/Paper016/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们提出一个使用端到端深度学习开发的最先进的语音识别系统。我们的架构比传统的语音系统要简单得多，传统的语音系统依赖于费力的工程处理流程；当在嘈杂的环境中使用时，这些传统的系统也往往表现不佳。相比之下，我们的系统不需要手工设计的组件来模拟背景噪声、混响或扬声器变化，而是直接学习一个对这些影响具有鲁棒性的函数。相比之下，我们的系统不需要手工设计的组件来模拟背景噪声、混响或说话人变化，而是直接学习一个对这些影响具有鲁棒性的函数。我们不需要音素词典，甚至不需要“音素”的概念。该方法的关键是一个优化良好的RNN训练系统，使用多个gpu，以及一套新型的数据合成技术，使我们能够有效地获取大量不同的数据来进行训练。我们的系统称为DeepSpeech，在广泛研究的Switchboard Hub5’00数据集上优于以前发表的结果，在整个测试集实现了16.0%的错误率。与广泛使用的、最先进的商业语音系统相比，DeepSpeech还能更好地处理充满挑战的嘈杂环境。</p><p><strong>参考</strong><br>[1] <em>Hannun A, Case C, Casper J, et al. Deep speech: Scaling up end-to-end speech recognition[J]. arXiv preprint arXiv:1412.5567, 2014.</em><a href="https://arxiv.org/abs/1412.5567">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 声学模型 </tag>
            
            <tag> 解码器 </tag>
            
            <tag> CTC </tag>
            
            <tag> 语言模型 </tag>
            
            <tag> RNN </tag>
            
            <tag> N-Gram </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MERL_2017_Hybrid CTC/Attention Architecture for End-to-End Speech Recognition</title>
      <link href="2021/01/10/Paper015/"/>
      <url>2021/01/10/Paper015/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>基于隐马尔可夫模型（HMM）和深度神经网络（DNN）的传统自动语音识别（ASR）是一个非常复杂的系统，由声学模型、词典和语言模型等模块组成。该系统还需要语言学信息，如发音词典、词语切分和语音上下文相关树。另一方面，端到端自动语音识别已经成为一种流行的替代方法，通过用单一的深度网络结构表示复杂的模块，并用数据驱动的学习方法代替语言学信息的使用，这大大简化了传统自动语音识别系统的建模过程。ASR的端到端架构主要有两种：一种是基于Attention的方法，利用注意力机制来实现声学帧和识别符号之间的对齐；另一种是基于联结主义时间分类（CTC）的方法，利用马尔可夫假设，通过动态规划有效地解决序列问题。本文提出混合CTC和Attention的端到端ASR，该系统有效地利用了这两种结构在训练和解码方面的优势。在训练过程中，我们采用多目标学习框架来提高鲁棒性和实现快速收敛。在解码过程中，我们结合基于Attention和CTC的分数，采用一次性波束搜索算法进行联合解码，进一步消除不规则对齐。通过对英语（WSJ和CHiME-4）任务的实验，证明了所提出的多目标学习方法在基于CTC和Attention的编解码基线上的有效性。此外，将该方法应用于两个大规模ASR基准测试（自然日语和汉语普通话），基于多目标学习和无需语言学信息的联合解码的优势性能与传统DNN和HMM的ASR系统旗鼓相当。</p><p><strong>参考</strong><br>[1] <em>Watanabe S, Hori T, Kim S, et al. Hybrid CTC/attention architecture for end-to-end speech recognition[J]. IEEE Journal of Selected Topics in Signal Processing, 2017, 11(8): 1240-1253.</em><a href="https://www.merl.com/publications/docs/TR2017-190.pdf">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 声学模型 </tag>
            
            <tag> 解码器 </tag>
            
            <tag> CTC </tag>
            
            <tag> Attention </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速使用阿里云语音识别和合成API服务</title>
      <link href="2021/01/09/Blog004/"/>
      <url>2021/01/09/Blog004/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="下载与解压SDK"><a href="#下载与解压SDK" class="headerlink" title="下载与解压SDK"></a>下载与解压SDK</h1><p>下载<br><a href="http://download.taobaocdn.com/freedom/C3143/compress/alibabacloud-nls-python-sdk.zip?spm=a2c4g.11186623.2.18.5b631e0dFV0YRE&file=alibabacloud-nls-python-sdk.zip">Python版阿里云语音合成SDK</a><br>解压</p><pre>unzip alibabacloud-nls-python-sdk.zip </pre><h1 id="安装SDK"><a href="#安装SDK" class="headerlink" title="安装SDK"></a>安装SDK</h1><p>安装Python管理工具</p><pre>pip install setuptools</pre><p>打包</p><pre>cd alibabacloud-nls-python-sdk   python setup.py bdist_egg  </pre><p>安装</p><pre>python setup.py install  </pre><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><p>将申请阿里云语音合成API的appkey和token填入到对应第99和第100行的位置</p><pre>python speech_synthesizer_demo.py</pre><p>将申请阿里云语音识别API的appkey和token填入到对应第104和第105行的位置</p><pre>python speech_recognizer_demo.py</pre><p>将申请阿里云语音识别API的appkey和token填入到对应第114和第115行的位置</p><pre>python speech_transcriber_demo.py</pre>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音识别 </tag>
            
            <tag> 阿里云服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MERL_ICASSP2017_Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning</title>
      <link href="2021/01/09/Paper014/"/>
      <url>2021/01/09/Paper014/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>近年来，端到端语音识别技术受到了越来越多的关注，它可以将语音直接转录成文本，而无需任何预先定义的对齐方式。一种方法是基于注意力机制的编码器和解码器框架，该框架通过纯数据驱动的方法一步到位地学习可变长度输入和输出序列之间的映射关系。注意力机制模型已被证明比另一种端到端的方法——联结主义时间分类（CTC）更能提高性能，主要是因为它明确地使用了目标特征的历史信息，而没有任何条件独立性的假设。然而，我们观察到，在噪声环境下注意力机制的表现很差，并且在输入长序列的初始训练阶段很难去学习。在这种情况下，由于缺乏CTC所使用的从左到右的约束，而注意力模型过于灵活，无法预测正确的对齐。本文提出了一种新型的端到端语音识别方法，通过在多任务学习框架下使用联合CTC和注意力机制模型来提高鲁棒性和实现快速收敛，从而缓解了对齐问题。在WSJ和CHiME-4任务上的实验表明，该方法比基于CTC和基于注意力机制的编解码器基线都有优势，字符错误率（CER）相对提高了5.4-14.6%。</p><p><strong>参考</strong><br>[1] <em>Kim S, Hori T, Watanabe S. Joint CTC-attention based end-to-end speech recognition using multi-task learning[C]//2017 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2017: 4835-4839.</em><a href="https://arxiv.org/abs/1609.06773">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 声学模型 </tag>
            
            <tag> 解码器 </tag>
            
            <tag> CTC </tag>
            
            <tag> Attention </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>约翰斯·霍普金斯大学_Interspeech2018_ESPnet：End-to-End Speech Processing Toolkit</title>
      <link href="2021/01/07/Paper013/"/>
      <url>2021/01/07/Paper013/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>本文介绍一种新的端到端语音处理开源平台ESPnet。ESPnet主要致力于端到端自动语音识别（ASR），并采用了广泛使用的动态神经网络工具包Chainer和PyTorch作为主要的深度学习引擎。ESPnet还按照Kaldi ASR工具包风格进行数据处理、特征提取方法和存储格式，并提供了用于语音识别和其他语音处理实验的完整安装教程。本文介绍了该软件平台的主要架构、一些重要功能（这些功能将ESPnet与其他开源ASR工具包区分开）以及带有主要ASR基准的实验结果。</p><p><strong>参考</strong><br>[1] <em>Watanabe S, Hori T, Karita S, et al. Espnet: End-to-end speech processing toolkit[J]. arXiv preprint arXiv:1804.00015, 2018.</em><a href="https://arxiv.org/abs/1804.00015">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> HMM </tag>
            
            <tag> Kaldi </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 声学模型 </tag>
            
            <tag> DNN </tag>
            
            <tag> TDNN </tag>
            
            <tag> BLSTM </tag>
            
            <tag> LF-MMI </tag>
            
            <tag> Chain </tag>
            
            <tag> 解码器 </tag>
            
            <tag> CTC </tag>
            
            <tag> Attention </tag>
            
            <tag> 语言模型 </tag>
            
            <tag> RNNLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>东京大学_AAAI2021_Towards Fully Automated Manga Translation</title>
      <link href="2021/01/05/Paper012/"/>
      <url>2021/01/05/Paper012/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们解决了日本漫画的机器翻译问题。漫画翻译在机器翻译中涉及两个重要的问题：上下文感知和多模式翻译。由于在漫画中文本和图像以非结构化的方式混合在一起，因此从图像中获取上下文对于漫画翻译至关重要。但是，如何从图像中提取上下文并整合到机器模型中仍然是一个悬而未决的问题。此外，目前尚没有用于训练和评估这类模型的语料库和基准。在本文中，我们做出了以下四个贡献，为漫画翻译研究奠定了基础。首先，我们提出了多模式上下文感知翻译框架。我们是第一个整合从漫画图像获得上下文信息的团队。这使我们能够翻译那些不用上下文信息（例如，其他对话气泡中的文本，说话者的性别等）而无法翻译的对话气泡中的文本。其次，为了训练模型，我们提出了从成对的原始漫画及其翻译中自动构建语料库的方法，通过该方法可以构建大型并行语料库而无需任何人工标记。第三，我们创建了一个新的基准来评估漫画翻译。最后，在我们提出的方法之上，我们设计了第一个用于全自动漫画翻译的完整系统。</p><p><strong>参考</strong><br>[1] <em>Hinami R, Ishiwatari S, Yasuda K, et al. Towards Fully Automated Manga Translation[J]. arXiv preprint arXiv:2012.14271, 2020.</em><a href="https://arxiv.org/abs/2012.14271">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器翻译 </tag>
            
            <tag> 漫画翻译 </tag>
            
            <tag> 文字检测 </tag>
            
            <tag> 文字识别 </tag>
            
            <tag> 文字嵌入 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>查尔斯大学_Interspeech2020_One Model, Many Languages：Meta-learning for Multilingual Text-to-Speech</title>
      <link href="2020/12/28/Paper011/"/>
      <url>2020/12/28/Paper011/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们介绍一种多语种语音合成的方法，该方法利用上下文参数生成的元学习概念，使用比以往方法更多的语种和更少的训练数据生成听起来更自然的多语种语音。我们的模型是基于完全卷积输入文本编码器的Tacotron 2，该编码器的权重由一个单独的参数生成器网络得到。为了优化语音克隆，该模型使用了一个带有梯度反转层的对抗性说话人分类器，该分类器从编码器中移除特定于说话人的信息。为了评估，我们安排了两组实验来比较我们的模型和使用不同的跨语言参数共享的基线：（1）在低数据量训练时的稳定性和性能，（2）语码转换合成的发音准确度和语音质量。在训练中，我们使用了CSS10数据集和基于五种语言的常见语音记录的新型小数据集。我们的模型被证明能有效地跨语言共享信息，并且通过主观评价测试，它产生比基线更自然和准确的语码转换语音。</p><p><strong>参考</strong><br>[1] <em>Nekvinda T, Dušek O. One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech[J]. arXiv preprint arXiv:2008.00768, 2020.</em><a href="https://arxiv.org/abs/2008.00768">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Tacotron2 </tag>
            
            <tag> 语音转换 </tag>
            
            <tag> 多语言/跨语言 </tag>
            
            <tag> WaveRNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>台湾大学_Interspeech2019_One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization</title>
      <link href="2020/12/28/Paper010/"/>
      <url>2020/12/28/Paper010/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>近年来，无需平行数据的语音转换（VC）已成功地应用于多目标场景，即训练单个模型将输入语音转换为多个不同的说话人。然而，这种模型存在着只能将语音转换为训练数据中说话人的局限性，这就限制了语言转换的适用场景。本文提出了一种新型的One-shot语音转换方法，该方法只需源说话人和目标说话人的一个示例话语就可以实现语音转换，而且在训练过程中甚至不需要出现源说话人和目标说话人。这是通过实例规范化（IN）实现说话人和内容表征分离。客观和主观评价表明，该模型能够生成与目标说话人相似的语音。除了性能测量之外，我们还证明了该模型能够在没有任何监督的情况下学习有意义的说话人表征。</p><p><strong>参考</strong><br>[1] <em>Chou J, Yeh C, Lee H. One-shot voice conversion by separating speaker and content representations with instance normalization[J]. arXiv preprint arXiv:1904.05742, 2019.</em><a href="https://arxiv.org/abs/1904.05742">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> Tacotron </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Griffin-Lim </tag>
            
            <tag> 语音转换 </tag>
            
            <tag> 声纹编码器 </tag>
            
            <tag> 内容编码器 </tag>
            
            <tag> Instance Normalization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>香港中文大学_2016_Phonetic posteriorgrams for many-to-one voice conversion without parallel data training</title>
      <link href="2020/12/28/Paper009/"/>
      <url>2020/12/28/Paper009/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>本文提出了一种基于非平行训练数据的新型语音转换方法。该想法是通过与说话者无关的自动语音识别（SI-ASR）系统获得的语音后验图（PPGs）来建立说话人之间关系。假设这些PPGs能够在说话人规范化空间中表示语音的清晰度，并与说话人独立地对应。所提出的方法首先获得目标语音的PPGs。使用基于循环神经网络的深度双向长短时间记忆（DBLSTM）结构来学习目标语音的声学特征之间和PPGs的关系。要转换任意的一段源语音，需要从相同的SI-ASR获得其PPGs并将它们输入到训练好的DBLSTM中以生成转换语音。我们的方法有两个主要优点：（1）需要非平行训练数据；（2）训练好的模型可以将任意一个源说话人的语音转换成目标说话人的语音（即多对一转换）。实验表明我们的方法性能与最先进的系统在语音质量和说话人相似度方面相同或优之。</p><p><strong>参考</strong><br>[1] <em>Sun L, Li K, Wang H, et al. Phonetic posteriorgrams for many-to-one voice conversion without parallel data training[C]//2016 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2016: 1-6.</em><a href="http://www1.se.cuhk.edu.hk/~hccl/publications/pub/2016_paper_297.pdf">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> 声码器 </tag>
            
            <tag> 语音转换 </tag>
            
            <tag> 语音后验图 </tag>
            
            <tag> MCEP </tag>
            
            <tag> DTW </tag>
            
            <tag> DBLSTM </tag>
            
            <tag> Straight </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_20180323_Style Tokens：Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis</title>
      <link href="2020/12/28/Paper008/"/>
      <url>2020/12/28/Paper008/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>在这项工作中，我们提出了“全局风格标记”（GSTs），这是一个在最先进的端到端语音合成系统Tacotron中联合训练的嵌入库。嵌入训练没有明确的标签，但能够学习大范围的声学表征进行建模。GSTs产生了一系列丰富的显著结果。它们生成的可解释的软“标签”可用于以新型的方式控制合成，例如改变速度和独立于文本内容的说话风格。它们还可以用于风格转换，在整个长格式文本语料库中复制单个音频片段的说话风格。当对有噪声的、未标记的数据进行训练时，GSTs学习分解噪声和说话人身份信息，为实现高度可扩展但稳健的语音合成提供了一条途径。</p><p><strong>参考</strong><br>[1] <em>Wang Y, Stanton D, Zhang Y, et al. Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis[J]. arXiv preprint arXiv:1803.09017, 2018.</em><a href="https://arxiv.org/abs/1803.09017">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> Tacotron </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Griffin-Lim </tag>
            
            <tag> WaveNet </tag>
            
            <tag> 语音转换 </tag>
            
            <tag> 说话人风格迁移 </tag>
            
            <tag> 声纹编码器 </tag>
            
            <tag> 韵律编码器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_ICML2018_Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron</title>
      <link href="2020/12/28/Paper007/"/>
      <url>2020/12/28/Paper007/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们提出一种学习潜在韵律嵌入空间的Tacotron扩展语音合成架构，其中所需要的韵律从一个参考声学表征中产生。我们发现，在这个学习的嵌入空间上训练Tacotron，即使在参考说话人与合成说话人不同的情况下，合成音频也能以精细的时间细节匹配到参考信号的韵律。此外，我们还发现，参考韵律嵌入可以用来合成不同于参考语句的文本。我们定义了几个评价韵律转换的定量和主观指标，并在一个韵律转换任务中报告了来自单个说话人和44个说话人Tacotron模型的相同音频样本的结果。</p><p><strong>参考</strong><br>[1] <em>Skerry-Ryan R J, Battenberg E, Xiao Y, et al. Towards end-to-end prosody transfer for expressive speech synthesis with tacotron[J]. arXiv preprint arXiv:1803.09047, 2018.</em><a href="https://arxiv.org/abs/1803.09047">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> Tacotron </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Griffin-Lim </tag>
            
            <tag> WaveNet </tag>
            
            <tag> 语音转换 </tag>
            
            <tag> 说话人风格迁移 </tag>
            
            <tag> 韵律编码器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>斯坦福大学_2018_Storytime - End to end neural networks for audiobooks</title>
      <link href="2020/12/27/Paper006/"/>
      <url>2020/12/27/Paper006/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>有声读物是文本到语音合成的黄金标准。由于它们持续几个小时，所以它们韵律中的小特质很快就会使听众不知所措。Storytime的灵感源于我们可以使用神经网络将口语文本的流畅度提高到一个新的水平。更具体地说，我们探索了Google最近的学术出版物Tacotron的实现。它依靠编码器、解码器和注意力机制来模拟接近人类的声谱图。 截至该项目开始时，还没有公开的尝试复现其结果。 我们在探索他们的论文应用规格和建模决策的详细报告中取得了长足的进步，希望这可以推动最新技术发展，从而获得在实践中使用的类似模型。 最终，我们取得了模型概念的成功论证。</p><p><strong>参考</strong><br>[1] <em>Freeman P, Villegas E, Kamalu J. Storytime-End to end neural networks for audiobooks[J].</em><a href="http://static.tongtianta.site/paper_pdf/5ec16378-bf36-11e9-9da8-00163e08bb86.pdf">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 合成器 </tag>
            
            <tag> Tacotron </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Griffin-Lim </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_NeurIPS2018_Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis</title>
      <link href="2020/12/26/Paper005/"/>
      <url>2020/12/26/Paper005/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们描述了一种基于神经网络的文本到语音合成系统，该系统能够生成许多不同说话人音色的声音，包括在模型训练中未出现说话人的声音。我们的系统由三个独立训练的模块组成：（1）说话人编码器网络，使用来自成千上万说话人且没有抄本的独立带噪语音数据集对说话人识别任务进行训练，以从目标说话人的仅几秒参考语音中生成固定维度的嵌入矢量；（2）一个基于Tacotron2的序列到序列合成网络，该网络根据说话者的嵌入情况从文本生成梅尔频谱图；（3）一种基于WaveNet的自回归声码器，可将mel频谱图转换为一系列时域波形​​样本。我们证明了所提出的模型能够通过独立训练的说话人编码器把学到的说话人可变知识迁移到新任务（多说话人语音合成）中，并且能够合成在训练过程中未出现说话人的自然声音。我们量化了在大量多样的说话人数据集上训练说话人编码器的重要性，以获得最佳的泛化性能。最后表明，随机采样的说话人嵌入方法可用于合成不同于模型训练所使用说话人的新颖说话人音色的语音，这说明该模型已学到了高质量的说话人表征。</p><p><strong>参考</strong><br>[1] <em>Jia Y, Zhang Y, Weiss R, et al. Transfer learning from speaker verification to multispeaker text-to-speech synthesis[C]//Advances in neural information processing systems. 2018: 4480-4490.</em><a href="https://arxiv.org/abs/1806.04558">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Tacotron2 </tag>
            
            <tag> WaveNet </tag>
            
            <tag> 语音转换 </tag>
            
            <tag> 说话人风格迁移 </tag>
            
            <tag> 说话人认证 </tag>
            
            <tag> 声纹编码器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学_20200622_FastSpeech 2：Fast and High-Quality End-to-End Text to Speech</title>
      <link href="2020/12/26/Paper004/"/>
      <url>2020/12/26/Paper004/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>非自回归文本语音转换（TTS）模型（例如FastSpeech）能够比以往的自回归模型以可比的质量明显更快地合成语音。 FastSpeech模型的训练依赖于持续时间预测（以提供更多信息作为输入）和知识蒸馏（以简化输出中的数据分布）的自回归教师模型，这可以缓解一对多映射问题（即多个语音变化对应于TTS中的相同文本）。但是，FastSpeech有几个缺点：（1）师生蒸馏管道复杂且耗时；（2）从老师模型中提取的持续时间不够准确，并且从老师模型中提取的目标梅尔谱图由于数据简化导致信息损失，这两者都限制了语音质量。在本文中，我们提出FastSpeech2，它解决了FastSpeech中的问题，并通过以下方法可以更好地解决TTS中的一对多映射问题：（1）直接训练具有原始频谱的模型，而不是老师模型的简化输出；2）引入更多语音变化信息（例如音调、能量和更准确的持续时间）作为条件输入。具体来说，我们从语音波形中提取持续时间、音调和能量，并将它们直接用作训练中的条件输入，并在推理中使用预测值。我们进一步设计了FastSpeech2s，这是首次尝试直接从文本并行直接生成语音波形，从而享有完全端到端推理的优势。实验结果表明：（1）FastSpeech2的训练速度是FastSpeech的3倍，而FastSpeech2s的推理速度更快。（2）FastSpeech2和2s的语音质量优于FastSpeech，FastSpeech2甚至可以超越自回归模型。</p><p><strong>参考</strong><br>[1] <em>Ren Y, Hu C, Qin T, et al. FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech[J]. arXiv preprint arXiv:2006.04558, 2020.</em><a href="https://arxiv.org/abs/2006.04558">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> 声码器 </tag>
            
            <tag> WaveGlow </tag>
            
            <tag> FastSpeech2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学_NeurIP2019_FastSpeech：Fast, Robust and Controllable Text to Speech</title>
      <link href="2020/12/26/Paper003/"/>
      <url>2020/12/26/Paper003/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>基于神经网络的端到端文本到语音（TTS）大大提高了合成语音的质量。突出的方法（例如Tacotron2）通常首先从文本生成梅尔频谱图，然后使用诸如WaveNet的声码器从梅尔频谱图合成语音。与传统的拼接和统计参数方法相比，基于神经网络的端到端模型的推理速度较慢，并且合成语音通常不稳定（即某些单词被跳过或重复）且缺乏可控性（语音速度或韵律控制）。在这项工作中，我们提出了一种基于Transformer的新型前馈网络，可为TTS并行生成梅尔频谱图。具体来说，我们从基于编码器-解码器的教师模型中提取注意力对齐，以进行音素持续时间预测，长度调节器将其用于扩展源音素序列，以匹配目标梅尔谱图序列的长度，进而生成并行梅尔谱图。在LJSpeech数据集上的实验表明，我们的并行模型在语音质量方面与自回归模型旗鼓相当，几乎消除了在特别困难的情况下单词跳过和重复的问题，并且可以平滑地调整语音速度。最重要的是，与自回归Transformer TTS相比，我们的模型将梅尔频谱图生成速度提高了270倍，将端到端语音合成速度提高了38倍。因此，我们将我们的模型称为FastSpeech。</p><p><strong>参考</strong><br>[1] <em>Ren Y, Ruan Y, Tan X, et al. Fastspeech: Fast, robust and controllable text to speech[J]. Advances in Neural Information Processing Systems, 2019, 32: 3171-3180.</em><a href="https://arxiv.org/abs/1905.09263">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> 声码器 </tag>
            
            <tag> FastSpeech </tag>
            
            <tag> WaveGlow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_ICASSP2018_Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions</title>
      <link href="2020/12/26/Paper002/"/>
      <url>2020/12/26/Paper002/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>本文介绍了Tacotron2，这是一种直接从文本进行语音合成的神经网络结构。该系统由一个递归的序列到序列特征预测网络组成，该网络将字符嵌入映射到梅尔谱图，然后是一个修改的WaveNet模型，该模型充当声码器，负责把梅尔谱图合成时域波形。我们的模型获得平均意见得分（MOS）为4.53，这与MOS为4.58的专业录音相当。为了验证我们的设计选择，我们提出了对系统关键组件的消融（控制变量）研究，并评估了使用梅尔谱图作为WaveNet的输入（而不是语言特征，持续时间和F0特征）的影响。我们进一步证明，使用紧凑的声学中间表征可以显著简化WaveNet架构。</p><p><strong>参考</strong><br>[1] <em>Shen J, Pang R, Weiss R J, et al. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions[C]//2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018: 4779-4783.</em><a href="https://arxiv.org/abs/1712.05884">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 合成器 </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Tacotron2 </tag>
            
            <tag> WaveNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_Interspeech2017_Tacotron：Towards End-to-End Speech Synthesis</title>
      <link href="2020/12/23/Paper001/"/>
      <url>2020/12/23/Paper001/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>文本到语音合成系统通常包含多个阶段，例如前端文本分析，声学模型和音频合成模块。构建这些组件通常需要广泛的领域专业知识，并且可能包含生硬的设计选择。在本文中，我们介绍了Tacotron，这是一种从文字直接合成语音的端到端生成文本到语音模型。给定文本及其音频，可以使用随机初始化从头开始完全训练模型。我们提出了几种关键技术，以使序列到序列框架能够很好地完成这一具有挑战性的任务。Tacotron在美国英语上获得主观五等级MOS为3.82，就自然度而言，其表现优于参数生成系统。此外，由于Tacotron在帧级生成语音，它比样本级自回归方法快得多。</p><p><strong>参考</strong><br>[1] <em>Wang Y, Skerry-Ryan R J, Stanton D, et al. Tacotron: Towards end-to-end speech synthesis[J]. arXiv preprint arXiv:1703.10135, 2017.</em><a href="https://arxiv.org/abs/1703.10135">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 合成器 </tag>
            
            <tag> Tacotron </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Griffin-Lim </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速修改Ubuntu镜像源以加速下载与安装</title>
      <link href="2020/12/20/Blog003/"/>
      <url>2020/12/20/Blog003/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-修改apt-apt-get镜像源"><a href="#1-修改apt-apt-get镜像源" class="headerlink" title="(1)修改apt/apt-get镜像源"></a>(1)修改apt/apt-get镜像源</h1><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><pre>sudo gedit /etc/apt/sources.list</pre><h2 id="注释掉旧镜像源，添加相应的内容"><a href="#注释掉旧镜像源，添加相应的内容" class="headerlink" title="注释掉旧镜像源，添加相应的内容"></a>注释掉旧镜像源，添加相应的内容</h2><p>（以清华镜像源为例）</p><pre># 清华源deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse</pre><p>也可在软件和更新中选择清华源，更方便快捷。</p><h2 id="最后更新软件列表即可"><a href="#最后更新软件列表即可" class="headerlink" title="最后更新软件列表即可"></a>最后更新软件列表即可</h2><pre>sudo apt-get update</pre><hr><h1 id="2-修改pip镜像源"><a href="#2-修改pip镜像源" class="headerlink" title="(2)修改pip镜像源"></a>(2)修改pip镜像源</h1><h2 id="修改配置文件-1"><a href="#修改配置文件-1" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><pre>gedit  ~/.pip/pip.conf</pre><h2 id="添加相应的内容即可"><a href="#添加相应的内容即可" class="headerlink" title="添加相应的内容即可"></a>添加相应的内容即可</h2><pre>[global]timeout = 6000index-url = https://pypi.tuna.tsinghua.edu.cn/simpletrusted-host = pypi.tuna.tsinghua.edu.cn</pre><hr><h1 id="3-修改conda镜像源"><a href="#3-修改conda镜像源" class="headerlink" title="(3)修改conda镜像源"></a>(3)修改conda镜像源</h1><h2 id="修改配置文件-2"><a href="#修改配置文件-2" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><pre>gedit ~/.condarc</pre><h2 id="添加相应的内容即可-1"><a href="#添加相应的内容即可-1" class="headerlink" title="添加相应的内容即可"></a>添加相应的内容即可</h2><pre>channels:  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ssl_verify: true</pre>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络技术 </tag>
            
            <tag> Linux </tag>
            
            <tag> Ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速搭建与使用网络热点服务</title>
      <link href="2020/12/05/Blog002/"/>
      <url>2020/12/05/Blog002/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><pre>sudo apt-get install  hostapd dnsmasqgit clone https://github.com/binglel/create_apcd create_apsudo make install</pre><h1 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h1><ul><li><p>No passphrase (open network)</p><pre>create_ap wlp3s0 enp2s0 hulahula</pre></li><li><p>WPA + WPA2 passphrase</p><pre>create_ap wlp3s0 enp2s0 hulahula 13001150505</pre></li></ul><h1 id="监控客户端"><a href="#监控客户端" class="headerlink" title="监控客户端"></a>监控客户端</h1><ul><li><p>首先查看正在运行的网络热点服务及其PID</p><pre>create_ap --list-running</pre></li><li><p>然后通过PID查看连接当前网络热点服务的客户端（以7170为例）</p><pre>create_ap --list-clients 7170</pre></li></ul><p>注意，若在服务启动过程中，有如下报错：</p><pre>Failed to initialize lock error</pre><p>则解决方法如下：</p><pre>sudo rm /tmp/create_ap.all.lock</pre><p>最后重新启动服务即可。</p>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络技术 </tag>
            
            <tag> Wi-Fi </tag>
            
            <tag> 网络热点服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>装修漏水致房屋受损，业主获赔3千却付了近两万的鉴定费</title>
      <link href="2020/11/16/Tale001/"/>
      <url>2020/11/16/Tale001/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>一场因装修引发的邻里纠纷中，杨女士获赔3千多元，却付出了近2万元的鉴定费。</p><p>因楼上装修导致房屋两次漏水，双方就赔偿事宜协商未果，业主杨女士将装修邻居马先生和装修公司诉至法院，要求赔礼道歉、赔偿财产和精神损失共计5.4万元。日前，海淀法院审结了此案。法院判决装修公司赔偿杨女士损失3064.62元。</p><h1 id="案情简介"><a href="#案情简介" class="headerlink" title="案情简介"></a>案情简介</h1><p>杨女士诉称，楼上住户马先生装修房屋，因装修公司施工不当导致家中天花板两次漏水。后双方协商赔偿事宜但没有达成一致，故诉至法院，要求马先生和装修公司赔礼道歉，并赔偿财产和精神损失共计5.4万元。</p><p>被告马先生辩称，渗水后其积极与杨女士协商，愿意给杨女士无偿修复好，也同意合理的赔偿，但杨女士要求的赔偿金额过高。</p><p>被告装修公司辩称，因装修时地面找平造成渗水，但并未造成其他损失，公司愿意承担所有责任，愿意免费修复并积极赔偿，但是杨女士主张的数额过高且不合理，无法同意其诉讼请求。</p><h1 id="法院判决"><a href="#法院判决" class="headerlink" title="法院判决"></a>法院判决</h1><p>法院经审理认为，在事发后以及本案审理过程中，装修公司均表示同意给杨女士家免费修复漏水受损部位，并赔偿一定数额的损失。因鉴定费用较高，反而可能会扩大双方的损失，法官建议杨女士选择有利于双方的调解方式。</p><p>但杨女士不同意装修公司进行修复，仍坚持申请对自家房屋因渗水造成的损失进行修复方案鉴定及造价评估。</p><p>在评估过程中，杨女士另外提出要求鉴定机构对受损部位之外的墙面、地面等多处的修复费用单方造价进行评估，主要理由是天花板粉刷后与墙壁会有色差，施工中会对四壁及周边有污损。</p><p>经鉴定，鉴定机构出具鉴定意见书认定受损部位造价仅为3064.62元，杨女士自行增加鉴定的部分造价5474.36元。杨女士共支付鉴定费18000元。</p><p>对于杨女士因渗水造成的损失，应由装修公司承担相应责任；但因双方在庭审中已经对受损部位进行了确认，因此杨女士在鉴定过程中提出的受损部位以外的修复费用，缺乏相应的事实和法律依据，法院不予支持；装修公司为具有相应资质的专业装修机构，已经表示同意免费维修并赔偿一定数额的损失，但杨女士仍然不同意装修公司进行维修，故本案鉴定费用是杨女士为证明其主张的举证支出，应由其自行承担。</p><p>最终，法院判决装修公司赔偿杨女士因渗水而造成损失3064.62元，鉴定费18000元由杨女士自行承担。</p><p>宣判后，双方均未提出上诉。</p><h1 id="法官提示"><a href="#法官提示" class="headerlink" title="法官提示"></a>法官提示</h1><p>漏水发生后，会严重影响楼下业主的生活，甚至会造成业主财产损失，楼下业主的焦虑心情可以理解。但是当我们遇上此类事情时，一定要理性对待，否则反而可能会像上述案例一样造成不必要的损失。</p><p>首先，装修公司因施工不当造成他人损失，一定要提供妥善的解决方案，避免进一步激化矛盾；</p><p>其次，作为受损方，在他人已经释出诚意时，能够协商解决的，尽量通过双方协商或者在居委会、法院等第三方的主持下，妥善解决问题。如果盲目提出高价索赔，不仅问题得不到解决，反而可能进一步将损失扩大；</p><p>最后，一般来说，漏水案件会涉及漏水成因、修复方案、修复方案造价等鉴定项目，特别是前两个鉴定，鉴定费均在万元以上。一些当事人可能因为不清楚诉讼及鉴定的成本而一味寻求诉讼的方式解决问题，法院作为中立的第三方，在遇到此类案件时会从专业的角度以及过往此类案件的审理结果等方面向当事人进行充分释明。如果当事人不能确定自己的损失，还可以通过多种渠道，如询问业内其他装修公司等，让双方都对自己的损失有一个合理的预期，更容易达成一致意见进行调解。这样不仅可以避免不必要的诉累，也可以避免其他社会资源的浪费。</p>]]></content>
      
      
      <categories>
          
          <category> Tale </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 法律故事 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速搭建与使用SOCKS5代理服务</title>
      <link href="2020/11/15/Blog001/"/>
      <url>2020/11/15/Blog001/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h1><pre>git clone https://github.com/binglel/socks5_pythoncd socks5_pythonpython socks5.py -hpython socks5.py start --port=1080 --auth=bulabula:31415926</pre><h1 id="停止"><a href="#停止" class="headerlink" title="停止"></a>停止</h1><pre>python socks5.py stop</pre>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> SOCK5 </tag>
            
            <tag> 网络技术 </tag>
            
            <tag> 代理服务 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
