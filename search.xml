<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>语音相关顶会论文集</title>
      <link href="2021/06/15/Blog007/"/>
      <url>2021/06/15/Blog007/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="Interspeech历年论文集"><a href="#Interspeech历年论文集" class="headerlink" title="Interspeech历年论文集"></a><a href="https://www.isca-speech.org/iscaweb/index.php/archive/online-archive">Interspeech历年论文集</a></h1><p><a href="https://www.isca-speech.org/archive/Interspeech_2020/">Interspeech2020</a> / <a href="https://www.isca-speech.org/archive/Interspeech_2019/">Interspeech2019</a> / <a href="https://www.isca-speech.org/archive/Interspeech_2018/">Interspeech2018</a> / <a href="https://www.isca-speech.org/archive/Interspeech_2017/">Interspeech2017</a> / <a href="https://www.isca-speech.org/archive/Interspeech_2016/">Interspeech2016</a> / <a href="https://www.isca-speech.org/archive/interspeech_2015/">Interspeech2015</a> / <a href="https://www.isca-speech.org/archive/interspeech_2014/">Interspeech2014</a> / <a href="https://www.isca-speech.org/archive/interspeech_2013/">Interspeech2013</a> / <a href="https://www.isca-speech.org/archive/interspeech_2012/">Interspeech2012</a> / <a href="https://www.isca-speech.org/archive/interspeech_2011/">Interspeech2011</a> / <a href="https://www.isca-speech.org/archive/interspeech_2010/">Interspeech2010</a> / <a href="https://www.isca-speech.org/archive/interspeech_2009/">Interspeech2009</a> / <a href="https://www.isca-speech.org/archive/interspeech_2008/">Interspeech2008</a> / <a href="https://www.isca-speech.org/archive/interspeech_2007/">Interspeech2007</a> / <a href="https://www.isca-speech.org/archive/interspeech_2006/">Interspeech2006</a> / <a href="https://www.isca-speech.org/archive/interspeech_2005/">Interspeech2005</a> / <a href="https://www.isca-speech.org/archive/interspeech_2004/">Interspeech2004</a> / <a href="https://www.isca-speech.org/archive/eurospeech_2003/">Interspeech2003</a> / <a href="https://www.isca-speech.org/archive/icslp_2002/">Interspeech2002</a> / <a href="https://www.isca-speech.org/archive/eurospeech_2001/">Interspeech2001</a> / <a href="https://www.isca-speech.org/archive/icslp_2000/">Interspeech2000</a>  ……</p><h1 id="ICASSP历年论文集"><a href="#ICASSP历年论文集" class="headerlink" title="ICASSP历年论文集"></a><a href="https://dblp.uni-trier.de/db/conf/icassp/index.html">ICASSP历年论文集</a></h1><p><a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2020.html">ICASSP2020</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2019.html">ICASSP2019</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2018.html">ICASSP2018</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2017.html">ICASSP2017</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2016.html">ICASSP2016</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2015.html">ICASSP2015</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2014.html">ICASSP2014</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2013.html">ICASSP2013</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2012.html">ICASSP2012</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2011.html">ICASSP2011</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2010.html">ICASSP2010</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2009.html">ICASSP2009</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2008.html">ICASSP2008</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2007.html">ICASSP2007</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2006.html">ICASSP2006</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2005.html">ICASSP2005</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2004.html">ICASSP2004</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2003.html">ICASSP2003</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2002.html">ICASSP2002</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2001.html">ICASSP2001</a> / <a href="https://dblp.uni-trier.de/db/conf/icassp/icassp2000.html">ICASSP2000</a>  ……</p><h1 id="ASRU历年论文集"><a href="#ASRU历年论文集" class="headerlink" title="ASRU历年论文集"></a>ASRU历年论文集</h1><p><a href="https://dblp.uni-trier.de/db/conf/asru/asru2019.html">ASRU2019</a> / <a href="https://dblp.uni-trier.de/db/conf/asru/asru2017.html">ASRU2017</a> / <a href="https://dblp.uni-trier.de/db/conf/asru/asru2015">ASRU2015</a> / <a href="https://dblp.uni-trier.de/db/conf/asru/asru2013.html">ASRU2013</a> / <a href="https://dblp.uni-trier.de/db/conf/asru/asru2011.html">ASRU2011</a> / <a href="https://dblp.uni-trier.de/db/conf/asru/asru2009.html">ASRU2009</a> / <a href="https://dblp.uni-trier.de/db/conf/asru/asru2007.html">ASRU2007</a></p><h1 id="ACL历年论文集"><a href="#ACL历年论文集" class="headerlink" title="ACL历年论文集"></a><a href="https://www.aclweb.org/anthology/">ACL历年论文集</a></h1><h1 id="CCL历年论文集"><a href="#CCL历年论文集" class="headerlink" title="CCL历年论文集"></a><a href="http://cips-cl.org/anthology">CCL历年论文集</a></h1><p><a href="http://cips-cl.org/static/anthology/CCL-2020/CCL2020.html">CCL2020</a> / <a href="http://cips-cl.org/static/anthology/CCL-2019/CCL2019.html">CCL2019</a> / <a href="http://cips-cl.org/static/anthology/CCL-2018/CCL2018.html">CCL2018</a> / <a href="http://cips-cl.org/static/anthology/CCL-2017/CCL2017.html">CCL2017</a> / <a href="http://cips-cl.org/static/anthology/CCL-2016/CCL2016.html">CCL2016</a> / <a href="http://cips-cl.org/static/anthology/CCL-2015/CCL2015.html">CCL2015</a> / <a href="http://cips-cl.org/static/anthology/CCL-2014/CCL-2014.html">CCL2014</a> / <a href="http://cips-cl.org/static/anthology/CCL-2013/CCL-2013.html">CCL2013</a> / <a href="http://cips-cl.org/static/anthology/CCL-2011/CCL-2011.html">CCL2011</a> / <a href="http://cips-cl.org/static/anthology/CCL-2009/CCL-2009.html">CCL2009</a> / <a href="http://cips-cl.org/static/anthology/CCL-2007/CCL-2007.html">CCL2007</a> / <a href="http://cips-cl.org/static/anthology/CCL-2005/CCL-2005.html">CCL2005</a> / <a href="http://cips-cl.org/static/anthology/CCL-2003/CCL-2003.html">CCL2003</a> / <a href="http://cips-cl.org/static/anthology/CCL-2001/CCL-2001.html">CCL2001</a> / <a href="http://cips-cl.org/static/anthology/CCL-1999/CCL-1999.html">CCL1999</a> / <a href="http://cips-cl.org/static/anthology/CCL-1997/CCL-1997.html">CCL1997</a> / <a href="http://cips-cl.org/static/anthology/CCL-1995/CCL-1995.html">CCL1995</a> / <a href="http://cips-cl.org/static/anthology/CCL-1993/CCL-1993.html">CCL1993</a></p>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 会议论文集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>华中科技大学_AAAI2020_Real-time Scene Text Detection with Differentiable Binarization</title>
      <link href="2021/06/15/Paper050/"/>
      <url>2021/06/15/Paper050/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>近年来，基于分割的方法在场景文本检测中非常流行，因为分割结果可以更准确地描述各种形状的场景文本，如曲线文本。然而，二值化的后处理对于基于分割的检测是必不可少的，它将分割方法产生的概率图转换为文本的边界框或区域。本文提出了一种可在分段网络中进行二值化处理的模块——微分二值化(DB)。在DB模块的优化下，分割网络可以自适应地设置二值化阈值，这不仅简化了后处理，而且提高了文本检测的性能。基于一个简单的分割网络，我们在五个基准数据集上验证了DB的性能改进，在检测精度和速度方面都一致地达到了最新的结果。特别是，对于轻量级主干，DB的性能改进非常显著，因此我们可以在检测精度和效率之间寻找理想的折衷。具体地说，以ResNet-18为主干，我们的检测器在MSRA-TD500数据集上实现了82.8的F-measure，运行速度为62FPS。</p><p><strong>参考</strong><br>[1] <em>Liao M, Wan Z, Yao C, et al. Real-time scene text detection with differentiable binarization[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(07): 11474-11481.</em><a href="https://arxiv.org/abs/1911.08947">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>DB</em><a href="https://github.com/MhLiao/DB">[GitHub]</a><br>[2] <em>DBNet_PyTorch</em><a href="https://github.com/WenmuZhou/DBNet.pytorch">[GitHub]</a><br>[3] <em>DBNet_TensorFLow</em><a href="https://github.com/zonasw/DBNet">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>清华大学_20191120_CAT：CRF-based ASR Toolkit</title>
      <link href="2021/06/02/Paper049/"/>
      <url>2021/06/02/Paper049/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>本文介绍了一个新型的开源自动语音识别工具包——CAT（CRF-based ASR Toolkit）。CAT的一个关键特点是在条件随机场（CRF）的框架下进行区分训练，特别是使用了受连接主义时态分类（CTC）启发的状态拓扑。CAT包含CTC-CRF的完整实现，并为基于CRF的端到端语音识别提供完整的工作流程。在Switchboard和Aishell等英文和中文基准测试上的评估结果表明，CAT在现有的端到端模型中以较少的参数获得了最先进的结果，并且与混合DNN-HMM模型相比具有竞争力。在灵活性方面，我们证明了基于i-vector的说话人自适应识别和延迟控制机制可以在CAT中方便而有效地进行探索。我们希望CAT，特别是基于CRF的框架和软件，能够引起社会的广泛关注，并能够得到进一步的探索和改进。</p><p><strong>参考</strong><br>[1] <em>An K, Xiang H, Ou Z. CAT: crf-based ASR toolkit[J]. arXiv preprint arXiv:1911.08747, 2019.</em><a href="https://arxiv.org/abs/1911.08747">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>CAT</em><a href="https://github.com/thu-spmi/cat">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>北京医疗</title>
      <link href="2021/05/31/Tale003/"/>
      <url>2021/05/31/Tale003/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://ybj.beijing.gov.cn/">北京市医疗保障局</a></p><blockquote><p><a href="http://ybj.beijing.gov.cn/2020_zwfw/2020_bmcx/202002/t20200217_1644108.html">北京市基本医疗保险A类定点医疗机构名单（32家）</a><br><a href="http://ybj.beijing.gov.cn/2020_zwfw/2020_bmcx/202002/t20200217_1644107.html">北京定点医疗机构查询</a></p></blockquote></blockquote><blockquote><p><a href="http://www.nhc.gov.cn/wjw/index.shtml">中华人民共和国国家卫生健康委员会</a></p><blockquote><p><a href="http://zgcx.nhc.gov.cn:9090/unit">全国医疗机构查询</a><br><a href="http://zgcx.nhc.gov.cn:9090/doctor">医生执业注册信息查询</a><br><a href="http://zgcx.nhc.gov.cn:9090/nurse">护士执业注册信息查询</a></p></blockquote></blockquote>]]></content>
      
      
      <categories>
          
          <category> Tale </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 医疗常识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>西班牙电话公司_NeurIPS2019_Blow：a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion</title>
      <link href="2021/05/31/Paper048/"/>
      <url>2021/05/31/Paper048/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>原始音频生成的端到端模型是一个挑战，特别是当它必须非并行处理数据时，这在许多情况下都是理想的设置。声音转换就是其中的一种情况，在这种情况下，模型必须在录音中模仿说话人。在本文中，我们提出了Blow，一种使用超网络条件处理的单尺度归一化流程，用于实现原始音频之间的多对多语音转换。使用单个说话者标识符在逐帧的基础上使用非并行数据对Blow进行端到端训练。我们发现，Blow与现有的基于流的架构、其他竞争基线相比更具优势，在客观和主观评估中都获得了同等或更好的性能。我们通过消融研究进一步评估其主要组成部分的影响，并量化一些属性，如必要的训练数据量或对目标说话人的偏好。</p><p><strong>参考</strong><br>[1] <em>Serrà J, Pascual S, Segura C. Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion[J]. arXiv preprint arXiv:1906.00794, 2019.</em><a href="https://arxiv.org/abs/1906.00794">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音转换 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>百度_20181012_Neural Voice Cloning with a Few Samples</title>
      <link href="2021/05/31/Paper047/"/>
      <url>2021/05/31/Paper047/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>语音克隆是个性化语音接口非常需要的功能。基于神经网络的语音合成已被证明可以为大量说话人生成高质量的语音。本文介绍了一种以少量音频样本作为输入的神经语音克隆系统。我们研究了两种方法：说话人自适应和说话人编码。说话人自适应是基于用几个克隆样本微调多说话人生成模型。说话人编码是基于训练单独的模型，以从克隆音频直接推断新的说话人嵌入信息，并与多说话人生成模型一起使用。在语音的自然度和与原说话人的相似性方面，即使克隆音频很少，这两种方法也都能达到很好的性能。虽然说话人自适应可以获得更好的自然度和相似性，但说话人编码方法的克隆时间或所需内存明显较少，有利于低资源部署。</p><p><strong>参考</strong><br>[1] <em>Arik S O, Chen J, Peng K, et al. Neural voice cloning with a few samples[J]. arXiv preprint arXiv:1802.06006, 2018.</em><a href="https://arxiv.org/abs/1802.06006">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>Neural-Voice-Cloning-With-Few-Samples</em><a href="https://github.com/SforAiDl/Neural-Voice-Cloning-With-Few-Samples">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音转换 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_20160223_Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
      <link href="2021/05/10/Paper046/"/>
      <url>2021/05/10/Paper046/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>非常深的卷积网络是近年来图像识别性能最大进步的核心。一个例子是Inception结构，它已经被证明以相对较低的计算成本获得了非常好的性能。最近，将残差连接与更传统的架构相结合的方法，在ILSVRC2015挑战赛中获得了最先进的性能，其性能与最新一代的Inception-v3网络相似。这就提出了这样一个问题：将Inception结构与残差连接相结合是否有好处。在这里，我们给出了明确的经验证据，带有残差连接的训练可以显著地加速Inception网络的训练。也有一些证据表明，残差Inception网络稍微优于同样没有残差连接的的Inception网络性能。我们还针对残差和非残差的Inception网络提出了几种新的流线型体系结构。这些变化显著提高了ILSVRC2012分类任务的单帧识别性能。我们进一步展示了适当的激活缩放如何使非常广泛的残差Inception网络的训练稳定。在3个残差和1个Inception-v4的集成下，我们在ImageNet分类（CLS）挑战的测试集上获得了3.08%的TOP-5误差。</p><p><strong>参考</strong><br>[1] <em>Szegedy C, Ioffe S, Vanhoucke V, et al. Inception-v4, inception-resnet and the impact of residual connections on learning[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2017, 31(1).</em><a href="https://arxiv.org/abs/1602.07261v1">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> Inception </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_CVPR2016_Rethinking the Inception Architecture for Computer Vision</title>
      <link href="2021/05/10/Paper045/"/>
      <url>2021/05/10/Paper045/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>卷积网络是用于各种任务的最先进计算机视觉解决方案的核心。自2014年以来，非常深的卷积网络开始成为主流，在各种基准中产生了实质性的收益。尽管增加的模型大小和计算成本往往会为大多数任务带来立竿见影的性能提升（只要为训练提供了足够的标签数据），但计算效率和低参数计数仍然是移动视觉和大数据场景等各种用例的考虑因素。在这里，我们探索通过适当的因子分解卷积和积极的正则化来扩大网络规模的方法，旨在尽可能有效地利用增加的计算。我们在ILSVRC2012分类挑战验证集上对我们的方法进行了基准测试，结果表明，与现有技术相比，我们的方法获得了显著的收益：使用计算成本为50亿次乘加的网络进行单帧评估时，TOP-1和TOP-5的误差分别为21.2%和5.6%，每个推理的计算成本为50亿次乘加，并且使用的参数少于2500万个。在4个模型集成和多种评估的情况下，我们在验证集上报告了3.5%的TOP-5错误(在测试集上错误了3.6%)，在验证集上报告了17.3%的TOP-1错误。</p><p><strong>参考</strong><br>[1] <em>Szegedy C, Vanhoucke V, Ioffe S, et al. Rethinking the inception architecture for computer vision[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 2818-2826.</em><a href="https://arxiv.org/abs/1512.00567">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> Inception </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_20150302_Batch Normalization：Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
      <link href="2021/05/10/Paper044/"/>
      <url>2021/05/10/Paper044/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>训练深度神经网络是复杂的，因为在训练过程中，随着前几层参数的变化，每一层的输入分布也会发生变化。这要求较小的学习率和小心翼翼的参数初始化，从而减慢了训练速度，并使训练具有饱和非线性的模型变得非常困难。我们将这种现象称为内部协变量漂移，并通过对层输入进行归一化来解决这个问题。我们的方法的优势在于将规范化作为模型体系结构的一部分，并对每个训练小批量执行规范化。批处理标准化使我们可以使用更大的学习率，同时不必太过谨慎地进行初始化。它还起到了正则化的作用，在某些情况下消除了Dropout的需要。将批归一化方法应用于最先进的图像分类模型，在减少14倍训练步骤的情况下达到了相同的精度，并且比原始模型的性能有了很大的提高。使用批量归一化网络集合，我们改进了ImageNet分类的最佳发布结果：TOP-5的验证误差达到4.9%（测试误差4.8%），超过了人类评分员的准确率。</p><p><strong>参考</strong><br>[1] <em>Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[C]//International conference on machine learning. PMLR, 2015: 448-456.</em><a href="https://arxiv.org/abs/1502.03167">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> Inception </tag>
            
            <tag> BatchNormalization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_CVPR2015_Going Deeper with Convolutions</title>
      <link href="2021/05/10/Paper043/"/>
      <url>2021/05/10/Paper043/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们提出了一种代号为”Inception“的深卷积神经网络结构，该结构在2014年ImageNet大规模视觉识别挑战赛（ILSVRC2014）中使分类和检测效果的达到最新技术水平。该结构的主要特点是提高了网络内部计算资源的利用率。这是通过精心设计实现的，该结构允许增加网络的深度和宽度，同时保持计算预算不变。为了优化质量，架构决策基于赫布原则和多尺度处理的直觉。我们在提交给ILSVRC2014年的报告中使用的一个具体实例是GoogLeNet，这是一个22层深的网络，其质量是在分类和检测的背景下进行评估的。</p><p><strong>参考</strong><br>[1] <em>Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1-9.</em><a href="https://arxiv.org/abs/1409.4842">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> Inception </tag>
            
            <tag> GoogLeNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数度智慧_Datasets_110小时8K</title>
      <link href="2021/05/10/Data026/"/>
      <url>2021/05/10/Data026/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://www.shudu-ai.com/">官网</a></p><blockquote><p><a href="http://www.shudu-ai.com/#team1">下载地址</a></p></blockquote></blockquote><p>第一批数据：中文普通话开源语音数据库录音室长200+小时，切割时长110+。音频参数为8000K，16bit，mono。经过专业语音校对人员转写标注，并通过严格质量检验，此数据库文本正确率在95%以上。分为训练集、开发集、测试集。（支持学术研究，未经允许禁止商用）<br>第二批数据：与第一批相似，录制时长500小时，切割290小时。此数据需要申请。同步可申请新的460+切割数据，录制600+。</p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 中文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>微软_CVPR2016_Deep Residual Learning for Image Recognition</title>
      <link href="2021/05/08/Paper042/"/>
      <url>2021/05/08/Paper042/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>越深层的神经网络越难训练。我们提出了一个残差学习框架，以简化网络的训练，该网络比以前使用的网络要深得多。我们明确地将网络层重新表示为参考层输入的学习残差函数，而不是学习未参考的函数。我们提供了全面的实验证据，表明该残差网络更容易被优化，并且可以从更大深度获得精度的提升。在ImageNet数据集上，我们评估了深度高达152层的残差网络，该网络比VGG网络深8倍，但复杂度仍然较低。该残差网络的集合在ImageNet测试集上获得了3.57%的误差。这一结果在ILSVRC2015分类任务中获得第一名。我们还对CIFAR-10进行了100层和1000层的分析。表征的深度对于许多视觉识别任务至关重要。仅仅由于我们极深层的表示，我们在COCO目标检测数据集上获得了28%的相对改进。深层残差网络是我们提交给ILSVRC和COCO2015竞赛任务1的基线，我们还在ImageNet检测、ImageNet定位、COCO检测和COCO分割任务上获得了第一名。</p><p><strong>参考</strong><br>[1] <em>He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</em><a href="https://arxiv.org/abs/1512.03385">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>ResNet_Caffe</em><a href="https://github.com/KaimingHe/deep-residual-networks">[GitHub]</a><br>[2] <em>ResNet_Keras</em><a href="https://github.com/raghakot/keras-resnet">[GitHub]</a><br>[3] <em>ResNet_PyTorch</em><a href="https://github.com/facebookarchive/fb.resnet.torch">[GitHub]</a><br>[4] <em>ResNet_TensorFlow</em><a href="https://github.com/ry/tensorflow-resnet">[GitHub]</a><br>[5] <em>ResNet_MXNet</em><a href="https://github.com/tornadomeet/ResNet">[GitHub]</a><br>[6] <em>ResNet_Neon</em><a href="https://github.com/apark263/cfmz">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> ResNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>康奈尔大学_CVPR2017_Densely Connected Convolutional Networks</title>
      <link href="2021/05/07/Paper041/"/>
      <url>2021/05/07/Paper041/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>最近的研究表明，如果卷积网络在靠近输入的层和靠近输出的层之间包含更短的连接，那么卷积网络的训练可以更深入、更准确、更有效。在本文中，我们采纳了这一研究结果，并提出了密集卷积网络（DenseNet），它以前馈的方式将每一层连接到每一层。传统的L层卷积网络有L个连接——每层与其下一层之间有一个连接，而我们的网络有L(L+1)/2个直接连接。对于每一层，前面所有层的特征映射被用作输入，而它自己的特征映射被用作到所有后续层的输入。DenseNet有几个显著的优点：它们缓解了消失梯度问题，加强了特征传播，鼓励了特征重用，并大大减少了参数数量。我们在四个竞争激烈的目标识别基准任务（CIFAR-10、CIFAR-100、SVHN和ImageNet）上对我们提出的体系结构进行了评估。DenseNet在大多数方面都比最先进的技术有了很大的改进，同时需要更少的计算来实现高性能。代码和预训练模型可在<a href="https://github.com/liuzhuang13/DenseNet">此处</a>找到。</p><p><strong>参考</strong><br>[1] <em>Huang G, Liu Z, Van Der Maaten L, et al. Densely connected convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4700-4708.</em><a href="https://arxiv.org/abs/1608.06993">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>DenseNet_PyTorch</em><a href="https://github.com/liuzhuang13/DenseNet">[GitHub]</a><br>[2] <em>DenseNet_TensorFlow</em><a href="https://github.com/flyyufelix/DenseNet-Keras">[GitHub]</a><br>[3] <em>DenseNet_MXNet</em><a href="https://github.com/miraclewkf/DenseNet">[GitHub]</a><br>[4] <em>DenseNet_Caffe</em><a href="https://github.com/shicai/DenseNet-Caffe">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> DenseNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>华盛顿大学_20160509_You Only Look Once：Unified, Real-Time Object Detection</title>
      <link href="2021/04/30/Paper040/"/>
      <url>2021/04/30/Paper040/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们提出了一种新的目标检测方法YOLO。关于目标检测的以往工作是重新调整分类器的用途来执行检测。取而代之的是，我们将目标检测定义为空间上的边界框分离和相关类别概率的回归问题。单个神经网络在一次评估中直接从完整图像预测边界框和类别概率。由于整个检测管道是一个单一网络，因此可以直接根据检测性能进行端到端的优化。我们的统一架构速度极快。我们的基本YOLO模型以每秒45帧的速度实时处理图像。该网络的一个较小版本Fast YOLO，能够令人震惊地每秒处理155帧，且其mAP仍达到了其他实时检测器的两倍。与最先进的检测系统相比，YOLO会产生更多的定位错误，但在不存在的情况下，预测错误检测的可能性要小得多。最后，YOLO学习了目标非常一般的表示形式。在毕加索数据集和人物艺术数据集上，从自然图像推广到艺术品时，它比所有其他检测方法（包括DPM和R-CNN）都有很大的优势。</p><p><strong>参考</strong><br>[1] <em>Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 779-788.</em><a href="https://arxiv.org/abs/1506.02640">[pdf]</a><br>[2] <a href="https://pjreddie.com/darknet/yolo/"><em>YOLO: Real-Time Object Detection</em></a></p><p><strong>源码</strong><br>[1] <em>darknet</em><a href="https://github.com/pjreddie/darknet">[GitHub]</a><br>[2] <em>YOLO_TensorFlow</em><a href="https://github.com/hizhangp/yolo_tensorflow">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> YOLO </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开源_Datasets_CIFAR</title>
      <link href="2021/04/27/Data025/"/>
      <url>2021/04/27/Data025/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://www.cs.toronto.edu/~kriz/cifar.html">官网</a></p></blockquote><p>CIFAR由Alex Krizhevsky、Vinod Nair和Geoffrey Hinton收集，起初的数据集共分为10类，分别为飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船、卡车，所以CIFAR数据集常以CIFAR-10命名。CIFAR共包含60000张32*32的RGB图像（50000张训练+10000张测试），其中没有任何类型重叠的情况。后来CIFAR又出了一个分类更多的版本CIFAR-100，从名字也可以看出共有100类，将图片分得更细。CIFAR数据集分为Python、MATLAB、二进制bin文件包，可通过官网直接下载。</p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开源_Datasets_MNIST</title>
      <link href="2021/04/27/Data024/"/>
      <url>2021/04/27/Data024/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://yann.lecun.com/exdb/mnist/">官网</a></p></blockquote><p>MNIST数据集来自美国国家标准与技术研究所，该数据集包含60000张用于训练的样本和10000张用于测试的样本，样本是固定大小（28*28）。训练集由来自250个不同人手写的数字构成, 其中50%是高中学生，50%来自人口普查局的工作人员；测试集也是同样比例的手写数字数据。</p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语音识别开源工具_Kaldi</title>
      <link href="2021/04/26/Open004/"/>
      <url>2021/04/26/Open004/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://www.kaldi-asr.org/">官网</a></p></blockquote><blockquote><p>kaldi<a href="https://github.com/kaldi-asr/kaldi">[Github]</a></p></blockquote><h1 id="部署过程"><a href="#部署过程" class="headerlink" title="部署过程"></a>部署过程</h1><h2 id="安装依赖工具"><a href="#安装依赖工具" class="headerlink" title="安装依赖工具"></a>安装依赖工具</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install autoconf automake gcc libtool subversion libatlas-base-dev gfortran g++ zlib1g-dev sox </span><br></pre></td></tr></table></figure><h2 id="获取开源代码"><a href="#获取开源代码" class="headerlink" title="获取开源代码"></a>获取开源代码</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/kaldi-asr/kaldi</span><br></pre></td></tr></table></figure><h2 id="创建虚拟空间"><a href="#创建虚拟空间" class="headerlink" title="创建虚拟空间"></a>创建虚拟空间</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export PATH=&quot;~/anaconda3/bin:$PATH&quot;</span><br><span class="line">PATH=~/cuda-10.0/bin:$PATH</span><br><span class="line">export PATH</span><br><span class="line">export LD_LIBRARY_PATH=~/cuda-10.0/lib64:/lib</span><br><span class="line"></span><br><span class="line">conda create -n py2_kaldi python=2.7</span><br><span class="line">source activate py2_kaldi</span><br></pre></td></tr></table></figure><p>若选用Python3，则需要在kaldi/tools/python/路径下创建一个空文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch kaldi/tools/python/python/.use_default_python</span><br></pre></td></tr></table></figure><h2 id="检测依赖环境"><a href="#检测依赖环境" class="headerlink" title="检测依赖环境"></a>检测依赖环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd kaldi/tools</span><br><span class="line">extras/check_dependencies.sh</span><br></pre></td></tr></table></figure><p>根据提示安装缺失的工具即可，可能需要Intel MKL函数库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">extras/install_mkl.sh</span><br></pre></td></tr></table></figure><h2 id="编译安装工具"><a href="#编译安装工具" class="headerlink" title="编译安装工具"></a>编译安装工具</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make -j12</span><br></pre></td></tr></table></figure><p>此过程会花费更长的时间，建议多核并行编译；此过程安装的工具有cub、openfst、sctk、sph2pipe；最后补充安装语言模型工具irstlm</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">extras/install_irstlm.sh</span><br></pre></td></tr></table></figure><h2 id="编译安装模块"><a href="#编译安装模块" class="headerlink" title="编译安装模块"></a>编译安装模块</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd ../src</span><br><span class="line">./configure --cudatk-dir=/home/asr/cuda-10.0 --shared</span><br><span class="line">make depend -j12</span><br><span class="line">make -j12</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Open </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> Kaldi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《中华人民共和国民法典》</title>
      <link href="2021/04/15/Tale002/"/>
      <url>2021/04/15/Tale002/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://www.npc.gov.cn/">中国人大网</a></p><blockquote><p><a href="http://www.npc.gov.cn/npc/c30834/202006/75ba6483b8344591abd07917e1d25cc8.shtml">中华人民共和国民法典电子预览版</a></p></blockquote></blockquote><p>《中华人民共和国民法典》于2020年5月28日在十三届全国人大三次会议审议通过，这是新中国成立以来第一部以“法典”命名的法律，是一部固根本、稳预期、利长远的基础性法律，是新时代我国社会主义法治建设的重大成果。民法典的颁布实施对于完善中国特色社会主义法治体系、推进国家治理体系和治理能力现代化，切实维护好广大人民的根本利益，促进社会公平正义具有重要意义。</p>]]></content>
      
      
      <categories>
          
          <category> Tale </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 法律常识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开源_Datasets_汉字与拼音</title>
      <link href="2021/04/09/Data023/"/>
      <url>2021/04/09/Data023/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>[1] <em>chinese-xinhua</em><a href="https://github.com/pwxcoo/chinese-xinhua">[GitHub]</a><br>中华新华字典数据库和 API 。收录包括 14032 条歇后语，16142 个汉字，264434 个词语，31648 个成语。<br>注意：字典存在重复的字</p><p>[2] <em>phrase-pinyin-data</em><a href="https://github.com/mozillazg/phrase-pinyin-data">[GitHub]</a><br>词语拼音数据</p><p>[3] <em>pinyin-data</em><a href="https://github.com/mozillazg/pinyin-data">[GitHub]</a><br>汉字拼音数据</p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 汉字与拼音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>出门问问_20210202_WeNet：Production First and Production Ready End-to-End Speech Recognition Toolkit</title>
      <link href="2021/03/31/Paper039/"/>
      <url>2021/03/31/Paper039/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>在本文中，我们提出了一个新开源、产品优先和产品就绪的端到端（E2E）语音识别工具包——WeNet。该工具包的主要目的是缩小E2E语音识别模型的研究和产品之间的差距。WeNet提供了一种在多种现实场景中发布ASR应用程序的有效方式，这是与其他开源E2E语音识别工具包的主要区别和优势。本文从模型体系结构、框架设计和性能度量三个方面对WeNet进行了介绍。我们使用WeNet在AISHELL-1上进行的实验，不仅在统一的流式和非流式的两遍（U2）E2E模型上给出了有前途的字符错误率（CER），而且显示出合理的RTF和延迟，这两个方面都有利于产品应用。</p><p><strong>参考</strong><br>[1] <em>Zhang B, Wu D, Yang C, et al. WeNet: Production First and Production Ready End-to-End Speech Recognition Toolkit[J]. arXiv preprint arXiv:2102.01547, 2021.</em><a href="https://arxiv.org/abs/2102.01547">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>wenet</em><a href="https://github.com/mobvoi/wenet">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 端到端 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用操作系统镜像下载指南</title>
      <link href="2021/03/30/Blog006/"/>
      <url>2021/03/30/Blog006/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h1><p><a href="https://www.microsoft.com/zh-cn/software-download/windows10">MediaCreationTool</a> / <a href="https://msdn.itellyou.cn/">MSDN</a> / <a href="https://next.itellyou.cn/">NEXT</a></p><h1 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h1><p><a href="https://ubuntu.com/download/desktop">官网</a> / <a href="https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/">清华源</a> / <a href="http://mirrors.aliyun.com/ubuntu-releases/">阿里云源</a> / <a href="http://mirrors.163.com/ubuntu-releases/">网易源</a> / <a href="https://mirrors.ustc.edu.cn/ubuntu-releases/">中科大源</a> / <a href="http://mirrors.sohu.com/ubuntu-releases/">搜狐源</a> / <a href="http://mirror.hust.edu.cn/ubuntu-releases/">华中科大源</a> / <a href="http://mirror.bit.edu.cn/ubuntu-releases/">北理源</a> / <a href="https://mirror.lzu.edu.cn/ubuntu-releases/">兰大源</a> / <a href="http://mirrors.neusoft.edu.cn/ubuntu-releases/">大连东软信息学院源</a></p><h1 id="Mint"><a href="#Mint" class="headerlink" title="Mint"></a>Mint</h1><p><a href="https://www.linuxmint.com/download_all.php">官网</a> / <a href="https://mirrors.tuna.tsinghua.edu.cn/linuxmint-cd/stable/">清华源</a></p><h1 id="CentOS"><a href="#CentOS" class="headerlink" title="CentOS"></a>CentOS</h1><p><a href="https://www.centos.org/download/">官网</a> / <a href="https://mirrors.tuna.tsinghua.edu.cn/centos-vault/">清华源</a></p>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ubuntu </tag>
            
            <tag> 操作系统 </tag>
            
            <tag> Windows </tag>
            
            <tag> Mint </tag>
            
            <tag> CentOS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语音标注工具_Praat</title>
      <link href="2021/03/23/Open003/"/>
      <url>2021/03/23/Open003/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="https://www.fon.hum.uva.nl/praat/">官网</a></p></blockquote><blockquote><p>Praat<a href="https://github.com/praat/praat">[Github]</a></p></blockquote><p>Praat的主要功能是对自然语言的语音信号进行采集、分析和标注，并执行包括变换和滤波等在内的多种处理任务。作为分析结果的文字报表和语图，不但可以输出到个人计算机的磁盘文件中和终端的显示器上，更能够输出为精致的矢量图或位图，供写作和印刷学术论文与专著使用。<br>此外，Praat还可用于合成语音或声音、统计分析语言学数据、辅助语音教学测试，等等。随着新版本的发布，Praat的功能和用途仍在不断扩展，但实际上多数用户只需要用到一小部分功能。Praat支持Windows、Linux和Mac端安装与使用。</p>]]></content>
      
      
      <categories>
          
          <category> Open </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音标注 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语音标注工具_ELAN</title>
      <link href="2021/03/23/Open002/"/>
      <url>2021/03/23/Open002/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="https://archive.mpi.nl/tla/elan">官网</a></p></blockquote><p>ELAN是一个对视频和音频数据的标识进行创建、编辑、可视化和搜索的标注工具，旨在为标识提供声音技术以及对多媒体剪辑进行开发利用。ELAN支持Windows、Linux和Mac端安装与使用。虽然ELAN专门为语言、手语、姿势提供分析，但是每个人都可以用它来处理多媒体数据，如视频和音频，以便对其进行标识、分析和建档。用户使用ELAN可以向音频或视频记录添加无限数量的文本注释。注释可以是在媒体中观察到的任何特征的句子、单词、翻译、描述等。层可以分层互连。批注可以与媒体时间对齐，也可以引用其他现有批注。注释的内容由Unicode文本组成，并且注释文档以XML格式（EAF）存储。</p>]]></content>
      
      
      <categories>
          
          <category> Open </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音标注 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习白板推导_ZhouJie</title>
      <link href="2021/03/23/Tutorial009/"/>
      <url>2021/03/23/Tutorial009/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p>Machine-Learning-Session<a href="https://github.com/shuhuai007/Machine-Learning-Session">[GitHub]</a></p></blockquote><blockquote><p><a href="https://space.bilibili.com/97068901">视频</a></p></blockquote><blockquote><p><a href="https://blog.csdn.net/qq_41485273/article/details/111563979">笔记</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数学之美_吴军</title>
      <link href="2021/03/19/Book003/"/>
      <url>2021/03/19/Book003/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p align="left">  <img src= "/img/loading.gif" data-lazy-src="/img/books/book_003-02.jpg" width = 240 height = 320/></p><ul><li><p>第一章 文字和语言 vs 数字和信息</p></li><li><p>第二章 自然语言处理——从规则到统计</p></li><li><p>第三章 统计语言模型</p></li><li><p>第四章 谈谈中文分词</p></li><li><p>第五章 隐含马尔可夫模</p></li><li><p>第六章 信息的度量和作用</p></li><li><p>第七章 贾里尼克和现代语言处理</p></li><li><p>第八章 简单之美——布尔代数和搜索引擎的索引</p></li><li><p>第九章 图论和网络爬虫</p></li><li><p>第十章 PageRank——Google的民主表决式网页排名技术</p></li><li><p>第十一章 如何确定网页和查询的相关性</p></li><li><p>第十二章 地图和本地搜索的最基本技术——有限状态机和动态规划</p></li><li><p>第十三章 Google AK-47的设计者——阿米特·辛格博士</p></li><li><p>第十四章 余弦定理和新闻的分类</p></li><li><p>第十五章 矩阵运算和文本处理中的两个分类问题</p></li><li><p>第十六章 信息指纹及其应用</p></li><li><p>第十七章 由电视剧《暗算》所想到的——谈谈密码学的数学原理</p></li><li><p>第十八章 闪光的不一定是金子——谈谈搜索引擎反作弊问题</p></li><li><p>第十九章 谈谈数学模型的重要性</p></li><li><p>第二十章 不要把鸡蛋放到一个篮子里——谈谈最大熵模型</p></li><li><p>第二十一章 拼音输入法的数学原理</p></li><li><p>第二十二章 自然语言处理的教父马库斯和他的优秀弟子们</p></li><li><p>第二十三章 布隆过滤器</p></li><li><p>第二十四章 马尔可夫链的扩展——贝叶斯网络</p></li><li><p>第二十五章 条件随机场和句法分析</p></li><li><p>第二十六章 维特比和他的维特比算法</p></li><li><p>第二十七章 再谈文本自动分类问题——期望最大化算法</p></li><li><p>第二十八章 逻辑回归和搜索广告</p></li><li><p>第二十九章 各个击破算法和Google云计算的基础</p></li></ul><p><strong>参考</strong><br>[1] <em>数学之美_吴军</em><a href="http://note.youdao.com/noteshare?id=357a8d6fc06d285ba06148d698bf0575&sub=CE4784055FCE4BD19A258F050B68D6E5">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Book </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计学习方法_李航</title>
      <link href="2021/03/19/Book004/"/>
      <url>2021/03/19/Book004/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p align="left">  <img src= "/img/loading.gif" data-lazy-src="/img/books/book_004-02.jpg" width = 240 height = 320/></p><ul><li><p>第一章 统计学习方法概论</p></li><li><p>第二章 感知机</p></li><li><p>第三章 k近邻法</p></li><li><p>第四章 朴素贝叶斯法</p></li><li><p>第五章 决策树</p></li><li><p>第六章 逻辑斯谛回归与最大熵模型</p></li><li><p>第七章 支持向量机</p></li><li><p>第八章 提升方法</p></li><li><p>第九章 EM算法及其推广</p></li><li><p>第十章 隐马尔可夫模型</p></li><li><p>第十一章 条件随机场</p></li><li><p>第十二章 统计学习方法总结</p></li></ul><p><strong>参考</strong><br>[1] <em>统计学习方法(非扫描版)_李航</em><a href="http://note.youdao.com/noteshare?id=e534664d3f86db8df63db877315e9fca&sub=B0E1D5D153764B1DBEA301189A28EE85">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>lihang_book_algorithm</em><a href="https://github.com/WenDesi/lihang_book_algorithm">[GitHub]</a> / <a href="https://blog.csdn.net/wds2006sdo/article/details/51923546">[CSDN]</a></p>]]></content>
      
      
      <categories>
          
          <category> Book </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IRISA-CNRS_Datasets_DEMAND</title>
      <link href="2021/03/17/Data022/"/>
      <url>2021/03/17/Data022/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><ul><li>下载地址<ul><li><a href="https://zenodo.org/record/1227121#.YFHH7HUzbaR">DEMAND</a><br>A collection of multi-channel recordings of acoustic noise in diverse environments</li></ul></li></ul><p>该数据集是在不同环境下采集的多通道声音噪声。</p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 噪声数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>乔治梅森大学_Datasets_The Speech Accent Archive</title>
      <link href="2021/03/17/Data021/"/>
      <url>2021/03/17/Data021/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="https://linguistics.gmu.edu/">LINGUISTICS@GMU</a></p></blockquote><ul><li>下载地址<ul><li><a href="http://accent.gmu.edu/index.php">speech accent archive</a></li></ul></li></ul><p>语音口音档案包括各种语言背景的大量语音样本，内容是以英语为母语和非母语的人阅读相同的一段文字，该档案供那些想要比较和分析不同英语使用者口音的人使用。</p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 英文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爱丁堡大学_Datasets_CSTR VCTK Corpus</title>
      <link href="2021/03/17/Data020/"/>
      <url>2021/03/17/Data020/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="https://era.ed.ac.uk/handle/1842/905">The Centre for Speech Technology Research (CSTR@Edin.)</a></p><blockquote><p><a href="https://datashare.ed.ac.uk/">Edinburgh DataShare</a></p></blockquote></blockquote><ul><li>下载地址<ul><li><a href="https://datashare.ed.ac.uk/handle/10283/3443">CSTR VCTK Corpus</a></li></ul></li></ul><p>该语料库包括110位不同口音的英语说话人，每位说话人大约400条语音，句子内容选自报纸等。</p><p><strong>参考</strong><br>[1] <em>Oord A ,  Dieleman S ,  Zen H , et al. WaveNet: A Generative Model for Raw Audio[J].  2016.</em><a href="http://arxiv.org/abs/1609.03499">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>wavenet_PyTorch</em><a href="https://github.com/r9y9/wavenet_vocoder">[GitHub]</a><br>[2] <em>wavenet_TensorFlow</em><a href="https://github.com/ibab/tensorflow-wavenet">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 英文语音数据集 </tag>
            
            <tag> 说话人风格迁移 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LibriVox_Datasets_The LJ Speech Dataset</title>
      <link href="2021/03/14/Data019/"/>
      <url>2021/03/14/Data019/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><ul><li>下载地址<ul><li><a href="https://keithito.com/LJ-Speech-Dataset/">LJ-Speech-Dataset</a></li></ul></li></ul><p>这是一个公共领域的语音数据集，包含13100句单人的短音频片段，这些语音片段来自7部非小说类书籍。 为每个剪辑提供了转录。 剪辑的长度从1到10秒不等，总长度约为24小时。</p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 英文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_Datasets_LibriTTS</title>
      <link href="2021/03/14/Data018/"/>
      <url>2021/03/14/Data018/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><ul><li>下载地址<ul><li><a href="http://www.openslr.org/60/">0penSLR</a> / <a href="https://openslr.magicdatatech.com/60/">OpenSLR-China</a><br>Large-scale corpus of English speech derived from the original materials of the LibriSpeech corpus</li></ul></li></ul><p><strong>参考</strong><br>[1] <em>Zen H, Dang V, Clark R, et al. LibriTTS: A corpus derived from LibriSpeech for text-to-speech[J]. arXiv preprint arXiv:1904.02882, 2019.</em><a href="https://arxiv.org/abs/1904.02882">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 英文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>约翰斯·霍普金斯大学_Datasets_LibriSpeech</title>
      <link href="2021/03/14/Data017/"/>
      <url>2021/03/14/Data017/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="https://www.clsp.jhu.edu/">The Center for Language and Speech Processing (CLSP@JHU)</a></p></blockquote><ul><li>下载地址<ul><li><a href="http://www.openslr.org/12/">0penSLR</a> / <a href="https://openslr.magicdatatech.com/12/">OpenSLR-China</a><br>Large-scale (1000 hours) corpus of read English speech</li><li><a href="http://www.openslr.org/31/">0penSLR</a> / <a href="https://openslr.magicdatatech.com/31/">OpenSLR-China</a><br>Subset of LibriSpeech corpus for purpose of regression testing</li></ul></li></ul><p>Librispeech是由kaldi开发者整理并发布的免费英语朗读数据，其数据总量是<code>960小时</code>，内容来自有声电子书项目LibriVox。</p><p><strong>数据内容</strong><br>train-clean-100：251个说话人<br>train-clean-360：921个说话人<br>train-clean-500：1166个说话人<br>……</p><p><strong>参考</strong><br>[1] <em>Panayotov V, Chen G, Povey D, et al. Librispeech: an asr corpus based on public domain audio books[C]//2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2015: 5206-5210.</em><a href="http://www.danielpovey.com/files/2015_icassp_librispeech.pdf">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>kaldi</em><a href="https://github.com/kaldi-asr/kaldi">[GitHub]</a><br>[2] <em>wenet</em><a href="https://github.com/mobvoi/wenet">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> Kaldi </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 英文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>清华大学_Datasets_CN-Celeb</title>
      <link href="2021/03/14/Data016/"/>
      <url>2021/03/14/Data016/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://cslt.riit.tsinghua.edu.cn/">清华大学语音和语言技术中心（CSLT@THU）</a></p><blockquote><p><a href="http://cslt.riit.tsinghua.edu.cn/mediawiki/index.php/Public_data">CSLT Open_Data List</a></p></blockquote></blockquote><ul><li>下载地址<ul><li><a href="http://www.openslr.org/82/">0penSLR</a> / <a href="https://openslr.magicdatatech.com/82/">OpenSLR-China</a><br>A Free Chinese Speaker Recognition Corpus Released by CSLT@Tsinghua University</li></ul></li></ul><p>该数据集是一个非约束条件下的大规模中文说话人识别数据集，包含环境、通道与情感的变化。这是与目前大多数开源说话人识别数据集(约束条件、很小的噪声和通道变化)的最大区别。该数据集包含1000个说话人，共计约13万个句子，总时长274小时，涵盖了11种真实场景下的不同类型。</p><p><strong>参考</strong><br>[1] <em>Fan Y, Kang J W, Li L T, et al. CN-CELEB: a challenging Chinese speaker recognition dataset[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 7604-7608.</em><a href="https://arxiv.org/abs/1911.01799">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>kaldi</em><a href="https://github.com/kaldi-asr/kaldi">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 说话人认证 </tag>
            
            <tag> Kaldi </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 中文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>冲浪科技_Datasets_Free ST American English Corpus</title>
      <link href="2021/03/14/Data015/"/>
      <url>2021/03/14/Data015/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://surfing.ai/">冲浪科技官网</a></p><blockquote><p><a href="http://surfing.ai/index.php?c=category&id=19">开源数据库</a></p></blockquote></blockquote><ul><li>下载地址<ul><li><a href="http://www.openslr.org/45/">OpenSLR</a> / <a href="https://openslr.magicdatatech.com/45/">OpenSLR-China</a><br>Free ST American English Corpus</li></ul></li></ul><p>ST-AEDS-20180100_1是由冲浪科技公司发布的英文语音数据集，使用手机在室内静音环境下录制，该数据集包含<code>10个说话者</code>，每位说话者大约<code>350句</code>。</p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 英文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>冲浪科技_Datasets_Free ST Chinese Mandarin Corpus</title>
      <link href="2021/03/14/Data014/"/>
      <url>2021/03/14/Data014/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://surfing.ai/">冲浪科技官网</a></p><blockquote><p><a href="http://surfing.ai/index.php?c=category&id=19">开源数据库</a></p></blockquote></blockquote><ul><li>下载地址<ul><li><a href="http://www.openslr.org/38/">OpenSLR</a> / <a href="https://openslr.magicdatatech.com/38/">OpenSLR-China</a><br>Free ST Chinese Mandarin Corpus</li></ul></li></ul><p>ST-CMDS-20170001_1是由冲浪科技公司发布的中文语音数据集，使用手机在室内静音环境下录制，该数据集包含<code>102600条</code>，<code>大约100余小时</code>的语音数据。数据内容以平时的网上语音聊天和智能语音控制语句为主，<code>855个不同说话者</code>，同时有男声和女声，适合多种场景下使用。</p><p><strong>源码</strong><br>[1] <em>ASRT_SpeechRecognition</em><a href="https://github.com/nl8590687/ASRT_SpeechRecognition">[GitHub]</a><br>[2] <em>VoiceprintRecognition-Tensorflow</em><a href="https://github.com/yeyupiaoling/VoiceprintRecognition-Tensorflow">[Github]</a><br>[3] <em>kaldi</em><a href="https://github.com/kaldi-asr/kaldi">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 中文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>上海元语_Datasets_Primewords Chinese Corpus Set 1</title>
      <link href="2021/03/14/Data013/"/>
      <url>2021/03/14/Data013/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="https://www.primewords.cn/">上海元语官网</a></p><blockquote><p><a href="https://www.primewords.cn/#data">更多数据</a></p></blockquote></blockquote><ul><li>下载地址<ul><li><a href="http://www.openslr.org/47/">OpenSLR</a> / <a href="https://openslr.magicdatatech.com/47/">OpenSLR-China</a><br>Primewords Chinese Corpus Set 1</li></ul></li></ul><p>Primewords Chinese Corpus Set 1包含<code>100小时</code>的中文语音数据，由上海元语信息技术有限公司发布。语料库由<code>296名</code>汉语母语人用智能手机录制。转录准确率超过98％，置信水平为95％，学术用途免费。抄本和语音之间的映射以JSON格式给出。</p><p><strong>源码</strong><br>[1] <em>kaldi</em><a href="https://github.com/kaldi-asr/kaldi">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 中文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爱数智慧_Datasets_MAGICDATA Mandarin Chinese Read Speech Corpus</title>
      <link href="2021/03/13/Data012/"/>
      <url>2021/03/13/Data012/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="https://www.magicdatatech.cn/">爱数智慧官网</a></p><blockquote><p><a href="https://www.magicdatatech.cn/open-datasets">开源数据集</a></p></blockquote></blockquote><ul><li>下载地址<ul><li><a href="http://www.openslr.org/68/">OpenSLR</a> / <a href="https://openslr.magicdatatech.com/68/">OpenSLR-China</a><br>MAGICDATA Mandarin Chinese Read Speech Corpus</li></ul></li></ul><p>MagicData（爱数智慧）技术有限公司的语料库，语料库包含<code>755小时</code>的语音数据，其主要是移动终端的录音数据。邀请来自中国不同重点区域的<code>1080名</code>说话者参与录制。句子转录准确率高于98％。录音在安静的室内环境中进行。数据库分为训练集、验证集和测试集。诸如语音数据编码和说话者信息的细节信息被保存在元数据文件中。录音文本领域多样化，包括互动问答、音乐搜索、SNS信息、家庭指挥和控制等，还提供了分段的成绩单。该语料库旨在支持语音识别、机器翻译、说话人识别和其他语音相关领域的研究人员。</p><p><strong>源码</strong><br>[1] <em>kaldi</em><a href="https://github.com/kaldi-asr/kaldi">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 中文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>标贝科技_Datasets_BZNSYP</title>
      <link href="2021/03/13/Data011/"/>
      <url>2021/03/13/Data011/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="https://www.data-baker.com/#/index">标贝科技官网</a></p><blockquote><p><a href="https://www.data-baker.com/#/data/index/source">开放数据</a></p></blockquote></blockquote><ul><li>下载地址<ul><li><a href="https://online-of-baklong.oss-cn-huhehaote.aliyuncs.com/story_resource/BZNSYP.rar?Expires=1615629471&OSSAccessKeyId=LTAI3GkKBSJFDJsp&Signature=f0/nWKVtLuda/XTbY1QhL03JJZI=">Aliyun</a><br>Chinese Standard Mandarin Speech Copus（10000 Sentences）</li></ul></li></ul><p>中文标准女声音库，采集对象的音色风格知性阳光、亲切自然，专业标准普通话女声，听感乐观积极。录制环境为专业录音室和录音软件，录音环境和设备自始至终保持不变，录音环境的信噪比不低于35dB;单声道录音，用48KHz 16比特采样频率、PCM WAV格式。录音语料涵盖各类新闻、小说、科技、娱乐、对话等领域，语料设计综合语料样本量，力求在有限的语料数据量内，对音节音子、类型、音调、音连以及韵律等尽可能全面的覆盖。根据合成语音标注标准对音库进行文本音字校对、韵律层级标注、语音文件边界切分标注。</p><p><strong>数据参数</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">数据内容：中文标准女声语音库数据</span></span><br><span class="line"><span class="string">录音语料：综合语料样本量；音节音子的数量、类型、音调、音连以及韵律等进行覆盖。</span></span><br><span class="line"><span class="string">有效时长：约12小时</span></span><br><span class="line"><span class="string">平均字数：16字</span></span><br><span class="line"><span class="string">语言类型：标准普通话</span></span><br><span class="line"><span class="string">发</span> <span class="string">音</span> <span class="string">人：女；20-30岁；声音积极知性</span></span><br><span class="line"><span class="string">录音环境：声音采集环境为专业录音棚环境：1）录音棚符合专业音库录制标准；2）录音环境和设备自始至终保持不变；3）录音环境的信噪比不低于35dB。</span></span><br><span class="line"><span class="string">录制工具：专业录音设备及录音软件</span></span><br><span class="line"><span class="string">采样格式：无压缩PCM</span> <span class="string">WAV格式，采样率为48KHz、16bit</span></span><br><span class="line"><span class="string">标注内容：音字校对、韵律标注、中文声韵母边界切分</span></span><br><span class="line"><span class="string">标注格式：文本标注为.txt格式文档；音节音素边界切分文件为.interval格式</span></span><br><span class="line"><span class="string">质量标准：1.</span> <span class="string">语音文件为48k</span> <span class="string">16bit</span> <span class="string">wav格式，音色、音量、语速一致，无漂零无截幅；2.标注文件字准率不低于99.8%；3.音素边界错误大于10ms的比例小于1%；音节边界准确率大于98%.</span></span><br><span class="line"><span class="string">存储方式：FTP存储</span></span><br><span class="line"><span class="string">文件格式：音频文件：WAV</span> <span class="string">文本标注文件：TXT</span> <span class="string">边界标注文件：INTERVAL</span></span><br><span class="line"><span class="string">版权所属者：标贝(北京)科技有限公司</span></span><br></pre></td></tr></table></figure><p><strong>源码</strong><br>[1] <em>espnet</em><a href="https://github.com/espnet/espnet">[GitHub]</a><br>[2] <em>ParallelWaveGAN</em><a href="https://github.com/kan-bayashi/ParallelWaveGAN">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 中文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据堂_Datasets_AIDataTang_1505zh</title>
      <link href="2021/03/13/Data010/"/>
      <url>2021/03/13/Data010/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="https://www.datatang.com/">数据堂官网</a></p><blockquote><p><a href="https://www.datatang.com/opensource">AI数据开源计划</a><br><a href="https://www.datatang.com/Opendatasets">AI数据助研计划</a></p></blockquote></blockquote><ul><li>下载地址<ul><li>aidatatang_1505zh（<strong>需申请下载</strong>）</li></ul></li></ul><p>aidatatang_1505zh中文普通话语音数据集时长1505小时，是数据堂中文普通话语音数据库中的一部分。采集区域覆盖全国34个省级行政区域，参与录音人数达6408人，录音内容超30万条口语化句子。经过专业语音校对人员转写标注，并通过严格质量检验，句标注准确率达98%以上，是行业内句准确率的最高标准。</p><p><strong>源码</strong><br>[1] <em>aidatatang_1505zh</em><a href="https://github.com/xiayongtao/aidatatang_1505zh">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 中文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据堂_Datasets_AIDataTang_200zh</title>
      <link href="2021/03/13/Data009/"/>
      <url>2021/03/13/Data009/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="https://www.datatang.com/">数据堂官网</a></p><blockquote><p><a href="https://www.datatang.com/opensource">AI数据开源计划</a><br><a href="https://www.datatang.com/Opendatasets">AI数据助研计划</a></p></blockquote></blockquote><ul><li>下载地址<ul><li><a href="http://www.openslr.org/62/">OpenSLR</a> / <a href="https://openslr.magicdatatech.com/62/">OpenSLR-China</a><br>aidatatang_200zh数据集是aidatatang_1505zh数据集中的一部分</li></ul></li></ul><p>aidatatang_200zh是由北京数据科技有限公司（数据堂）提供的开放式中文普通话电话语音库。语料库长达200小时，由Android系统手机（16kHz，16位）和iOS系统手机（16kHz，16位）记录。邀请来自中国不同重点区域的600名演讲者参加录音，录音是在安静的室内环境或环境中进行，其中包含不影响语音识别的背景噪音。参与者的性别和年龄均匀分布。语料库的语言材料是设计为音素均衡的口语句子。每个句子的手动转录准确率大于98％。</p><p><strong>源码</strong><br>[1] <em>kaldi</em><a href="https://github.com/kaldi-asr/kaldi">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 中文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>希尔贝壳_Datasets_DMASH</title>
      <link href="2021/03/13/Data008/"/>
      <url>2021/03/13/Data008/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://www.aishelltech.com/">希尔贝壳官网</a></p><blockquote><p><a href="http://www.aishelltech.com/DMASH_Dataset">DMASH</a></p></blockquote></blockquote><ul><li>下载地址<ul><li>DMASH（<strong>需申请下载</strong>）</li><li><a href="http://aishell-dmash.oss-cn-hangzhou.aliyuncs.com/train_dev.rar">Aliyun-Train/Dev</a> + <a href="http://aishell-dmash.oss-cn-hangzhou.aliyuncs.com/evaluation.rar">Aliyun-Test</a><br>The FFSVC20 challenge dataset is part of the DMASH dataset</li></ul></li></ul><p>DMASH为中文普通话麦克风阵列家居场景语音数据库，在两个不同房间的真实智能家居场景中录制的。录音设备包括一个近距离通话麦克风和七组设备，分别位于房间的七个不同位置。每组录音设备包括一部iPhone、一部安卓手机、一部iPad、一个麦克风，以及一个半径为5厘米的圆形麦克风阵列。该数据集包含500名说话者的50000小时语音数据，每个说话者录制三次，每次间隔7-15天。AISHELL-DMASH数据集由专业语音校对人员转写标注，并通过严格质量检验，字词的准确率达到98%，可用于声纹识别、语音识别、唤醒词识别等方面的研究。</p><p><strong>参考</strong><br>[1] <em>Qin X, Li M, Bu H, et al. The INTERSPEECH 2020 Far-Field Speaker Verification Challenge[J]. arXiv preprint arXiv:2005.08046, 2020.</em><a href="https://arxiv.org/abs/2005.08046">[pdf]</a><br>[2] <em>Qin X, Li M, Bu H, et al. The ffsvc 2020 evaluation plan[J]. arXiv preprint arXiv:2002.00387, 2020.</em><a href="https://arxiv.org/abs/2002.00387">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 中文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>希尔贝壳_Datasets_HI-MIA</title>
      <link href="2021/03/12/Data007/"/>
      <url>2021/03/12/Data007/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://www.aishelltech.com/">希尔贝壳官网</a></p><blockquote><p><a href="http://www.aishelltech.com/wakeup_data">HI-MIA</a></p></blockquote></blockquote><ul><li>下载地址<ul><li><a href="http://www.openslr.org/85/">OpenSLR</a> / <a href="https://openslr.magicdatatech.com/85/">OpenSLR-China</a><br>A far-field text-dependent speaker verification database for AISHELL Speaker Verification Challenge 2019</li><li><a href="https://mab.to/fH6iY8C5Ank17">Netdisk-Train/Dev</a> + <a href="https://mab.to/TBvujkh8W5uPN">Netdisk-Test</a> + <a href="https://mab.to/PuqMHfmHJ68yX">Netdisk-Test-Update</a><br>The data is used in AISHELL Speaker Verification Challenge 2019</li><li>AISHELL-WakeUp-1（<strong>需申请下载</strong>）</li><li><a href="https://mab.to/4iLb6p5siFp31">Netdisk-2019B-EVAL</a><br>Speech and Speaker Recognition Evaluation</li></ul></li></ul><p>HI-MIA，是一个智能家居场景下的固有唤醒词数据库。该数据库共包含<code>340个说话人</code>，每个说话人语料包含了近场麦克风拾音和远场麦克风阵列的多通道拾音。它可用于声纹识别、语音唤醒识别等研究。HI-MIA是从<a href="http://www.aishelltech.com/wakeup_data"><strong>AISHELL-WakeUp-1</strong></a>数据集（254人）和<a href="http://www.aishelltech.com/aishell_2019B_eval"><strong>AISHELL-2019B-EVAL</strong></a>数据集（86人）两个子数据库提取出来的，分别覆盖近讲高保真麦克风、1/3/5米圆型阵列的数据。<br>其中，AISHELL-WakeUp-1语音数据库共唤醒词语音<code>3936003条</code>，<code>1561.12小时</code>。录音语言，中文和英文；录音地区，中国。录音文本为“你好，米雅”、“hi, mia”唤醒词。邀请254名发言人参与录制。录制过程在真实家居环境中，设置7个录音位，使用6个圆形16路PDM麦克风阵列录音板做远讲拾音（16kHz，16bit）、1个高保真麦克风做近讲拾音（44.1kHz，16bit）。此数据库经过专业语音校对人员转写标注，并通过严格质量检验，字正确率100%。可用于声纹识别、语音唤醒识别等研究使用。</p><p><strong>参考</strong><br>[1] <em>Qin X, Bu H, Li M. Hi-mia: A far-field text-dependent speaker verification database and the baselines[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 7609-7613.</em><a href="https://arxiv.org/abs/1912.01231">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>kaldi</em><a href="https://github.com/kaldi-asr/kaldi">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音唤醒 </tag>
            
            <tag> 语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>希尔贝壳_Datasets_AISHELL-3</title>
      <link href="2021/03/07/Data006/"/>
      <url>2021/03/07/Data006/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://www.aishelltech.com/">希尔贝壳官网</a></p><blockquote><p><a href="http://www.aishelltech.com/aishell_3">AISHELL-3</a></p></blockquote></blockquote><ul><li>下载地址<ul><li><a href="http://www.openslr.org/93/">OpenSLR</a> / <a href="https://openslr.magicdatatech.com/93/">OpenSLR-China</a></li><li><a href="https://mab.to/nqR84htNPt662">Netdisk</a></li></ul></li></ul><p>希尔贝壳中文普通话语音数据库AISHELL-3的语音时长为85小时88035句，可做为多说话人合成系统。录制过程在安静室内环境中， 使用高保真麦克风（44.1kHz，16bit）。218名来自中国不同口音区域的发言人参与录制。专业语音校对人员进行拼音和韵律标注，并通过严格质量检验，此数据库音字确率在98%以上。</p><p><strong>参考</strong><br>[1] <em>Shi Y, Bu H, Xu X, et al. AISHELL-3: A Multi-speaker Mandarin TTS Corpus and the Baselines[J]. arXiv preprint arXiv:2010.11567, 2020.</em><a href="https://arxiv.org/abs/2010.11567">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>aishell-3-baseline-fc</em><a href="https://github.com/sos1sos2Sixteen/aishell-3-baseline-fc">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 中文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>希尔贝壳_Datasets_AISHELL-2</title>
      <link href="2021/03/07/Data005/"/>
      <url>2021/03/07/Data005/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://www.aishelltech.com/">希尔贝壳官网</a></p><blockquote><p><a href="http://www.aishelltech.com/aishell_2">AISHELL-2</a></p></blockquote></blockquote><ul><li>下载地址<ul><li>AISHELL-3（<strong>需申请下载</strong>）</li></ul></li></ul><p>希尔贝壳中文普通话语音数据库AISHELL-2的语音时长为<code>1000小时</code>，其中718小时来自AISHELL-ASR0009-[ZH-CN]，282小时来自AISHELL-ASR0010-[ZH-CN]。录音文本涉及唤醒词、语音控制词、智能家居、无人驾驶、工业生产等12个领域。录制过程在安静室内环境中， 同时使用3种不同设备： 高保真麦克风（44.1kHz，16bit）；Android系统手机（16kHz，16bit）；iOS系统手机（16kHz，16bit）。AISHELL-2采用iOS系统手机录制的语音数据。<code>1991名</code>来自中国不同口音区域的发言人参与录制。经过专业语音校对人员转写标注，并通过严格质量检验，此数据库文本正确率在96%以上。</p><p><strong>参考</strong><br>[1] <em>Du J, Na X, Liu X, et al. Aishell-2: Transforming mandarin asr research into industrial scale[J]. arXiv preprint arXiv:1808.10583, 2018.</em><a href="https://arxiv.org/abs/1808.10583">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>kaldi</em><a href="https://github.com/kaldi-asr/kaldi">[GitHub]</a><br>[2] <em>wenet</em><a href="https://github.com/mobvoi/wenet">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> Kaldi </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 中文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>希尔贝壳_Datasets_AISHELL-1</title>
      <link href="2021/03/07/Data004/"/>
      <url>2021/03/07/Data004/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://www.aishelltech.com/">希尔贝壳官网</a></p><blockquote><p><a href="http://www.aishelltech.com/kysjcp">AISHELL-1</a></p></blockquote></blockquote><ul><li>下载地址<ul><li><a href="http://www.openslr.org/33/">OpenSLR</a> / <a href="https://openslr.magicdatatech.com/33/">OpenSLR-China</a></li><li><a href="https://mab.to/ojn60R5b4rTor">Netdisk</a></li></ul></li></ul><p>希尔贝壳中文普通话开源语音数据库AISHELL-ASR0009-OS1录音时长<code>178小时</code>，是希尔贝壳中文普通话语音数据库AISHELL-ASR0009的一部分。AISHELL-ASR0009录音文本涉及智能家居、无人驾驶、工业生产等11个领域。录制过程在安静室内环境中， 同时使用3种不同设备： 高保真麦克风（44.1kHz，16-bit）；Android系统手机（16kHz，16-bit）；iOS系统手机（16kHz，16-bit）。高保真麦克风录制的音频降采样为16kHz，用于制作AISHELL-ASR0009-OS1。<code>400名</code>来自中国不同口音区域的发言人参与录制。经过专业语音校对人员转写标注，并通过严格质量检验，此数据库文本正确率在95%以上。分为训练集、开发集、测试集。</p><p><strong>参考</strong><br>[1] <em>Bu H, Du J, Na X, et al. Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline[C]//2017 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA). IEEE, 2017: 1-5.</em><a href="https://arxiv.org/abs/1709.05522">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>kaldi</em><a href="https://github.com/kaldi-asr/kaldi">[GitHub]</a><br>[2] <em>wenet</em><a href="https://github.com/mobvoi/wenet">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> Kaldi </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 中文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CMU_Datasets_AN4</title>
      <link href="2021/03/07/Data003/"/>
      <url>2021/03/07/Data003/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://www.speech.cs.cmu.edu/">Speech at CMU</a></p><blockquote><p><a href="http://www.speech.cs.cmu.edu/databases/">The CMU Audio Databases</a></p></blockquote></blockquote><ul><li>下载地址<ul><li><a href="http://www.speech.cs.cmu.edu/databases/an4/">官网</a><br>This database, also known as AN4 and as the Alphanumeric database, was recorded internally at CMU circa 1991.</li></ul></li></ul><p>AN4是由美国卡内基梅隆大学与1991年采集的，其内容是个人信息录入，总共包含约50分钟的语音。</p><p><strong>源码</strong><br>[1] <em>kaldi</em><a href="https://github.com/kaldi-asr/kaldi">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> Kaldi </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 英文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DARPA_Datasets_TIMIT</title>
      <link href="2021/03/07/Data002/"/>
      <url>2021/03/07/Data002/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><ul><li>下载地址<ul><li><a href="https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3">Academic Torrents</a><br>The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus</li><li><a href="https://www.kesci.com/mw/dataset/5e7da79c246a590036b71cbb/content">和鲸社区</a><br>DARPA TIMIT语音连续语音语料库训练和测试数据</li></ul></li></ul><p>TIMIT是由国防高级研究计划局-信息科学与技术办公室（DARPA-ISTO）赞助的多个网站共同努力的结果。文本语料库设计是麻省理工学院（MIT）、斯坦福研究所（SRI）和德州仪器（TI）的共同努力。该语音在TI录制，在MIT转录，并由美国国家标准技术研究院（NIST）进行维护、验证和准备用于CD-ROM制作。TIMIT数据集的语音采样频率为16kHz，一共包含6300个句子，由来自美国八个主要方言地区的630个人每人说出给定的10个句子，所有的句子都在音素级别(phone level)上进行了手动分割、标记。70%的说话人是男性，大多数说话者是成年白人。<br><strong>Data = TrainData + TestData</strong></p><p><strong>参考</strong><br>[1] <em>Disc N S, Garofolo J S, Lamel L F, et al. Acoustic-Phonetic Continuous Speech Corpus[J].</em><a href="http://perso.limsi.fr/lamel/TIMIT_NISTIR4930.pdf">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>kaldi</em><a href="https://github.com/kaldi-asr/kaldi">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> Kaldi </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 英文语音数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>清华大学_Datasets_THCHS30</title>
      <link href="2021/03/06/Data001/"/>
      <url>2021/03/06/Data001/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://cslt.riit.tsinghua.edu.cn/">清华大学语音和语言技术中心（CSLT@THU）</a></p><blockquote><p><a href="http://cslt.riit.tsinghua.edu.cn/mediawiki/index.php/Public_data">CSLT Open_Data List</a></p></blockquote></blockquote><ul><li><a href="http://166.111.134.19:7777/data/thchs30/README.html">下载地址</a><ul><li><a href="http://www.openslr.org/18/">0penSLR</a> / <a href="https://openslr.magicdatatech.com/18/">OpenSLR-China</a><br>SLR18 is used by the Kaldi: A Free Chinese Speech Corpus Released by CSLT@Tsinghua University</li><li><a href="http://166.111.134.19:7777/data/thchs30/standalone.html">Standalone</a><br>This version is used by the HTK, Sphinx etc.</li></ul></li></ul><p>THCHS30是一个很经典的中文语音数据集，包含了<code>1万余条</code>语音文件，大约<code>40小时</code>的中文语音数据，内容以文章、诗句为主。它是由清华大学语音与语言技术中心（CSLT）出版的开放式中文语音数据库。原创录音于2002年由朱晓燕教授在清华大学计算机科学系智能与系统重点实验室监督下进行，原名为“TCMSD”，代表“清华连续普通话语音数据库”。13年后的出版由王东博士发起，并得到了朱晓燕教授的支持。他们希望为语音识别领域的新入门的研究人员提供玩具级别的数据库，因此，数据库对学术用户完全免费。</p><p><strong>Data(13388条) = TrainData(10000条) + DevData(893条) + TestData(2495条)</strong></p><p><strong>女声ID</strong>（49）</p><pre>A2、A4、A6、A7、A11、A12、A13、A14、A19、A22、A23、A32、A34、A36;B2、B4、B6、B7、B11、B12、B15、B22、B31、B32、B33;C2、C4、C6、C7、C12、C13、C14、C17、C18、C19、C20、C21、C22、C23、C31;D4、D6、D7、D11、D12、D13、D21、D31、D32.</pre><p><strong>男声ID</strong>（10）</p><pre>A5、A8、A9、A33、A35、B8、B21、B34、C8、D8.</pre><p><strong>参考</strong><br>[1] <em>Wang D, Zhang X. Thchs-30: A free chinese speech corpus[J]. arXiv preprint arXiv:1512.01882, 2015.</em><a href="https://arxiv.org/abs/1512.01882">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>kaldi</em><a href="https://github.com/kaldi-asr/kaldi">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音识别 </tag>
            
            <tag> 说话人认证 </tag>
            
            <tag> Kaldi </tag>
            
            <tag> 语音数据集 </tag>
            
            <tag> 中文语音数据集 </tag>
            
            <tag> HTK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_Interspeech2019_SpecAugment：A Simple Data Augmentation Method for Automatic Speech Recognition</title>
      <link href="2021/03/04/Paper038/"/>
      <url>2021/03/04/Paper038/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>本文提出了一种简单的语音识别数据增强方法，称之为SpecAugment。SpecAugment直接应用在神经网络的特征输入(即滤波器组系数)。增强策略包括扭曲特征、掩蔽频域块和掩蔽时间步长块。我们将SpecAugment应用于端到端语音识别任务的LAS网络。我们在LibriSpeech 960小时和Swichboard 300小时任务上实现了最先进的性能，超越了所有以前的工作。在LibriSpeech test-other测试集上，在不使用语言模型的情况下的WER为6.8%，与语言模型浅融合的WER为5.8%。这与之前WER为7.5%的最先进混合系统相提并论。对于Switchboard数据集，我们在Hub5‘00测试集的Switchboard/CallHome部分，在不使用语言模型的情况下实现了7.2%/14.6%的性能，在浅层融合的情况下实现了6.8%/14.1%的性能，而之前最先进的混合系统的WER为8.3%/17.3%。</p><p><strong>参考</strong><br>[1] <em>Park D S, Chan W, Zhang Y, et al. Specaugment: A simple data augmentation method for automatic speech recognition[J]. arXiv preprint arXiv:1904.08779, 2019.</em><a href="https://arxiv.org/abs/1904.08779">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>SpecAugment</em><a href="https://github.com/DemisEom/SpecAugment">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音增强 </tag>
            
            <tag> LAS </tag>
            
            <tag> SpecAugment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>加泰罗尼亚理工大学_Interspeech2017_SEGAN：Speech Enhancement Generative Adversarial Network</title>
      <link href="2021/03/02/Paper037/"/>
      <url>2021/03/02/Paper037/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>当前的语音增强技术是在频谱域上操作或开发一些更高级别的特征。其中大多数能解决有限数量的噪声环境，并依赖于一阶统计量。为了规避这些问题，深度网络越来越多地被使用，这要归功于它们从大样本集中学习复杂函数的能力。在这项工作中，我们提出使用生成对抗网络进行语音增强。与当前的技术不同，我们在波形级别操作，端到端地训练模型，并将28个说话人和40个不同的噪声环境合并到同一模型中，以便在它们之间共享模型参数。我们使用一个独立的、未见的测试集对所提出的模型进行评估，该测试集有两个说话人和20种不同的噪声环境。增强后的样本证实了该模型的可行性，客观评价和主观评价都证实了该模型的有效性。在此基础上，我们开启了语音增强生成架构的探索，它可能会逐步吸收更多以语音为中心的设计选择，以提高其性能。</p><p><strong>参考</strong><br>[1] <em>Pascual S, Bonafonte A, Serra J. SEGAN: Speech enhancement generative adversarial network[J]. arXiv preprint arXiv:1703.09452, 2017.</em><a href="https://arxiv.org/abs/1703.09452">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>segan_TensorFlow</em><a href="https://github.com/santi-pdp/segan">[GitHub]</a><br>[2] <a href="https://binglel.gitee.io/2021/03/02/Open001/"><em>语音增强模型-SEGAN开源代码</em></a><br>[3] <em>segan_PyTorch</em><a href="https://github.com/santi-pdp/segan_pytorch">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音增强 </tag>
            
            <tag> GAN </tag>
            
            <tag> SEGAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语音增强模型-SEGAN开源代码</title>
      <link href="2021/03/02/Open001/"/>
      <url>2021/03/02/Open001/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><ul><li><a href="https://github.com/santi-pdp/segan">segan_Tensorflow</a></li></ul><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><h2 id="虚拟空间"><a href="#虚拟空间" class="headerlink" title="虚拟空间"></a>虚拟空间</h2><p>CUDA v8.0 + CUDNN v5.1</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n py2_tf0.12_gpu python=2.7</span><br><span class="line">source activate py2_tf0.12_gpu</span><br></pre></td></tr></table></figure><h2 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/santi-pdp/segan</span><br><span class="line">cd segan</span><br><span class="line">sudo apt install sox</span><br><span class="line">pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><h1 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod 754 prepare_data.sh</span><br><span class="line">./prepare_data.sh</span><br></pre></td></tr></table></figure><h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h2><pre>~/segan/data ~/seganINFLATING CLEAN TRAINSET ZIP...CONVERTING CLEAN WAVS TO 16K...~/segan/data/clean_trainset_wav ~/segan/data ~/segan~/segan/data ~/seganINFLATING NOISY TRAINSET ZIP...CONVERTING NOISY WAVS TO 16K...~/segan/data/noisy_trainset_wav ~/segan/data ~/segan~/segan/data ~/segan~/seganPREPARING TRAINING DATA...I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locallyI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locallyI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locallyI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locallyI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally--------------------------------------------------Processing wav file 11572/11572 data/clean_trainset_wav_16k/p233_033.wav          **************************************************Total processing and writing time: 35.0719149113 s</pre><p>首先完成数据下载和解压，然后通过sox工具完成采样频率转换（to 16kHz），接着读取成对的干净语音和带躁语音，通过一个重叠50%的滑动窗口（2^14）生成标准单位（大约1秒）的2^14（16384）个采样点，最后将所有生成的数据存储为一个TFRecord格式的文件，目的是在训练时可高效读取。</p><h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod 754 train_segan.sh</span><br><span class="line">./train_segan.sh</span><br></pre></td></tr></table></figure><p>在NVIDIA GeForce RTX 3080单卡（显存10G+内存16G）上使用默认参数(batch_size=100)进行训练，大约用时10h。<br>而在NVIDIA GeForce GTX 1080单卡（显存8G+内存16G）上使用调整参数(batch_size=24)进行训练，大约用时49h。</p><h2 id="模型结构可视化"><a href="#模型结构可视化" class="headerlink" title="模型结构可视化"></a>模型结构可视化</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=&quot;segan_allbiased_preemph/train&quot;</span><br></pre></td></tr></table></figure><p>浏览地址<br><a href="http://127.0.1.1:6006/">http://127.0.1.1:6006</a></p><p>更多信息可参阅官网及源码！</p>]]></content>
      
      
      <categories>
          
          <category> Open </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 开源代码 </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> 语音增强 </tag>
            
            <tag> GAN </tag>
            
            <tag> SEGAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>跟踪语音领域最新技术和最新结果</title>
      <link href="2021/02/28/Blog005/"/>
      <url>2021/02/28/Blog005/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="https://github.com/syhw/wer_are_we">wer_are_we</a></p></blockquote><p><strong>语音识别任务的数据集列表</strong></p><ul><li><p>LibriSpeech</p></li><li><p>WSJ</p></li><li><p>Hub5’00 Evaluation(Switchboard / CallHome)</p></li><li><p>Rich Transcriptions</p></li><li><p>Fisher</p></li><li><p>TED-LIUM</p></li><li><p>CHiME6(multiarray noisy speech)</p></li><li><p>CHiME(noisy speech)</p></li><li><p>TIMIT</p></li></ul><blockquote><p><a href="https://github.com/zzw922cn/awesome-speech-recognition-speech-synthesis-papers">awesome-speech-recognition-speech-synthesis-papers</a></p></blockquote><blockquote><p><a href="https://github.com/thu-spmi/ASR-Benchmarks">ASR-Benchmarks</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音识别 </tag>
            
            <tag> 语音转换 </tag>
            
            <tag> 音乐合成 </tag>
            
            <tag> 说话人认证 </tag>
            
            <tag> 语言模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于Kaldi的语音识别_Dan·Povey_约翰斯·霍普金斯大学</title>
      <link href="2021/02/27/Tutorial008/"/>
      <url>2021/02/27/Tutorial008/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://www.danielpovey.com/">Dan Povey</a></p></blockquote><blockquote><p><a href="https://www.clsp.jhu.edu/">The Center for Language and Speech Processing(CLSP@JHU)</a></p></blockquote><blockquote><p><a href="http://www.danielpovey.com/kaldi-lectures.html">基于Kaldi的语音识别（官网：课件）</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> Kaldi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语音识别：从入门到精通_谢磊_西北工业大学</title>
      <link href="2021/02/27/Tutorial007/"/>
      <url>2021/02/27/Tutorial007/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://lxie.npu-aslp.org/">谢磊</a> / <a href="https://teacher.nwpu.edu.cn/xielei">谢磊</a></p></blockquote><blockquote><p><a href="http://www.npu-aslp.org/">西北工业大学音频、语音与语言处理研究组(ASLP@NPU)</a></p></blockquote><blockquote><p><a href="https://www.shenlanxueyuan.com/course/311">语音识别：从入门到精通</a></p></blockquote><ul><li><p>语音识别综述-谢磊</p></li><li><p>语音信号处理及特征提取-孙思宁</p></li><li><p>GMM以及EM算法-孙思宁</p></li><li><p>隐马尔可夫模型(HMM)-许开拓</p></li><li><p>基于GMM-HMM的语音识别系统-张彬彬</p></li><li><p>基于DNN-HMM的语音识别系统-张彬彬</p></li><li><p>语言模型-吕航</p></li><li><p>基于WFST的解码器-吕航</p></li><li><p>区分性训练和LF-MMI-张彬彬</p></li><li><p>端到端语音识别-许开拓</p></li><li><p>课程总结-谢磊</p></li><li><p>kaldi学习经验-吕航</p></li></ul><p><strong>源码</strong><br>[1] <em>ASR_Course</em><a href="https://github.com/nwpuaslp/ASR_Course">[GitHub]</a><br>[2] <em>speech</em><a href="https://github.com/xbsdsongnan/speech">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文本数据管理与分析_邱锡鹏_复旦大学</title>
      <link href="2021/02/26/Tutorial006/"/>
      <url>2021/02/26/Tutorial006/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="https://xpqiu.github.io/index.html">邱锡鹏</a></p></blockquote><blockquote><p><a href="https://textprocessing.github.io/">文本数据管理与分析（官网：课件）</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络与深度学习_邱锡鹏_复旦大学</title>
      <link href="2021/02/26/Tutorial005/"/>
      <url>2021/02/26/Tutorial005/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="https://xpqiu.github.io/index.html">邱锡鹏</a></p></blockquote><blockquote><p><a href="https://nndl.github.io/">神经网络与深度学习（官网：课件+电子书）</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
            <tag> RNN </tag>
            
            <tag> CNN </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>信号与系统_李琳山_国立台湾大学</title>
      <link href="2021/02/24/Tutorial004/"/>
      <url>2021/02/24/Tutorial004/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/103S207">信号与系统开放版（官网：视频+课件）</a></p></blockquote><blockquote><p><a href="http://speech.ee.ntu.edu.tw/SS2020Spring/">信号与系统2020（官网：视频+课件）</a></p></blockquote><blockquote><p><a href="https://www.bilibili.com/video/BV1fz411B7e2?from=search&seid=14818241412218339029">信号与系统2020（完整版）</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kaldi语音识别实战_陈果果&amp;都家宇&amp;那兴宇&amp;张俊博</title>
      <link href="2021/02/23/Book006/"/>
      <url>2021/02/23/Book006/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p align="left">  <img src= "/img/loading.gif" data-lazy-src="/img/books/book_006-02.jpg" width = 240 height = 320/></p><ul><li><p>第一章 语音识别技术基础</p></li><li><p>第二章 Kaldi概要介绍</p></li><li><p>第三章 数据整理</p></li><li><p>第四章 经典声学建模技术</p></li><li><p>第五章 构图和解码</p></li><li><p>第六章 深度学习声学建模技术</p></li><li><p>第七章 关键词搜索与语音唤醒</p></li><li><p>第八章 说话人识别</p></li><li><p>第九章 语音识别应用实践</p></li></ul><p><strong>源码</strong><br>[1] <em>kaldi</em><a href="https://github.com/kaldi-asr/kaldi">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Book </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 说话人认证 </tag>
            
            <tag> 语音唤醒 </tag>
            
            <tag> Kaldi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>解析深度学习：语音识别实践_俞栋&amp;邓力（俞凯&amp;钱彦旻等译）</title>
      <link href="2021/02/23/Book005/"/>
      <url>2021/02/23/Book005/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p align="left">  <img src= "/img/loading.gif" data-lazy-src="/img/books/book_005-02.jpg" width = 240 height = 320/></p><ul><li><p>第一章 简介</p></li><li><p>第二章 混合高斯模型</p></li><li><p>第三章 隐马尔可夫模型及其变体</p></li><li><p>第四章 深度神经网络</p></li><li><p>第五章 高级模型初始化技术</p></li><li><p>第六章 深度神经网络-隐马尔可夫模型混合系统</p></li><li><p>第七章 训练和解码的加速</p></li><li><p>第八章 深度神经网络序列鉴别性训练</p></li><li><p>第九章 深度神经网络中的特征表示学习</p></li><li><p>第十章 深度神经网络和混合高斯模型的融合</p></li><li><p>第十一章 深度神经网络的自适应技术</p></li><li><p>第十二章 深度神经网络中的表征共享和迁移</p></li><li><p>第十三章 循环神经网络及相关模型</p></li><li><p>第十四章 计算型网络</p></li><li><p>第十五章 总结及未来研究方向</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Book </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> GMM </tag>
            
            <tag> HMM </tag>
            
            <tag> GMM-HMM </tag>
            
            <tag> DNN-HMM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数位语音处理概论_李琳山_国立台湾大学</title>
      <link href="2021/02/23/Tutorial003/"/>
      <url>2021/02/23/Tutorial003/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/104S204">数位语音处理概论开放版（官网：视频+课件）</a></p></blockquote><blockquote><p><a href="http://speech.ee.ntu.edu.tw/DSP2019Spring/">数位语音处理概论2019（官网：视频+课件）</a></p></blockquote><blockquote><p><a href="https://www.bilibili.com/video/BV1Gt411V7Pq?from=search&seid=9842121860892116799">数位语音处理概论2019（完整版）</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 说话人认证 </tag>
            
            <tag> 语言模型 </tag>
            
            <tag> 信号处理 </tag>
            
            <tag> 声学模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>现代语音信号处理_胡航</title>
      <link href="2021/02/22/Book002/"/>
      <url>2021/02/22/Book002/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p align="left">  <img src= "/img/loading.gif" data-lazy-src="/img/books/book_002-02.jpg" width = 240 height = 320/></p><ul><li><p>第一章 绪论</p></li><li><p>第二章 语音信号处理的基础知识</p></li><li><p>第三章 时域分析</p></li><li><p>第四章 短时傅里叶分析</p></li><li><p>第五章 倒谱分析与同态滤波</p></li><li><p>第六章 线性预测分析</p></li><li><p>第七章 语音信号的非线性分析</p></li><li><p>第八章 语音特征参数估计</p></li><li><p>第九章 矢量量化</p></li><li><p>第十章 隐马尔可夫模型</p></li><li><p>第十一章 语音编码</p></li><li><p>第十二章 语音合成</p></li><li><p>第十三章 语音识别</p></li><li><p>第十四章 说话人识别</p></li><li><p>第十五章 智能信息处理技术在语音信号处理中的应用</p></li><li><p>第十六章 语音增强</p></li><li><p>第十七章 基于麦克风阵列的语音信号处理</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Book </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习_周志华</title>
      <link href="2021/02/22/Book001/"/>
      <url>2021/02/22/Book001/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p align="left">  <img src= "/img/loading.gif" data-lazy-src="/img/books/book_001-02.jpg" width = 240 height = 320/></p><ul><li><p>第一章 绪论</p></li><li><p>第二章 模型评估与选择</p></li><li><p>第三章 线性模型</p></li><li><p>第四章 决策树</p></li><li><p>第五章 神经网络</p></li><li><p>第六章 支持向量机</p></li><li><p>第七章 贝叶斯分类器</p></li><li><p>第八章 集成学习</p></li><li><p>第九章 聚类</p></li><li><p>第十章 降维与度量学习</p></li><li><p>第十一章 特征选择与稀疏学习</p></li><li><p>第十二章 计算学习理论</p></li><li><p>第十三章 半监督学习</p></li><li><p>第十四章 概率图模型</p></li><li><p>第十五章 规则学习</p></li><li><p>第十六章 强化学习</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Book </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_ICLR2018_Monotonic Chunkwise Attention</title>
      <link href="2021/02/21/Paper036/"/>
      <url>2021/02/21/Paper036/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>软注意力序列间模型已成功地应用于各种各样的问题，但其解码过程需要大量的时间和空间开销，不适用于实时序列转换。为了解决这些问题，我们提出了单调的组块注意力（MOCHA），它自适应地将输入序列分割成小块，在这些小块上计算软注意。我们证明了使用MOCHA的模型可以在标准反向传播的情况下有效地训练，同时允许在测试时进行在线和线性时间解码。当应用于在线语音识别时，我们获得了最先进的结果，并与使用离线软注意力机制的模型性能进行了比较。在文档总结的实验中，在没有进行单调对齐的情况下，与基于单调注意力的基线模型相比，该模型表现出显著的改进性能。</p><p><strong>参考</strong><br>[1] <em>Chiu C C, Raffel C. Monotonic chunkwise attention[J]. arXiv preprint arXiv:1712.05382, 2017.</em><a href="https://arxiv.org/abs/1712.05382">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> Attention </tag>
            
            <tag> Sequence-to-Sequence </tag>
            
            <tag> MoChA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_20160804_A Neural Transducer</title>
      <link href="2021/02/21/Paper035/"/>
      <url>2021/02/21/Paper035/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>序列到序列模型已经在各种任务上取得了令人印象深刻的结果。然而，它们不适用于需要随着更多数据到来而进行增量预测的任务，或者具有长输入序列和输出序列的任务。这是因为它们生成以整个输入序列为条件的输出序列。在本文中，我们提出了一种神经转换器，它可以在更多的输入到来时进行增量预测，而不需要重新进行整个输入序列计算。与序列到序列模型不同，神经转换器根据部分观察到的输入序列和部分生成的序列来计算下一步分布。在每个时间步，转换器可以决定发射零到多个输出符号。数据可以使用编码器进行处理，并作为转换器的输入。在每个时间步发射一个符号的离散决策使得用传统的反向传播很难学习。然而，可以通过使用动态规划算法来训练转换器以产生离散决策。我们的实验表明，神经转换器在需要数据输入时产生输出预测的环境下工作良好。我们还发现，即使在没有使用注意力机制的情况下，神经转换器在长序列中也表现得很好。</p><p><strong>参考</strong><br>[1] <em>Jaitly N, Sussillo D, Le Q V, et al. A neural transducer[J]. arXiv preprint arXiv:1511.04868, 2015.</em><a href="https://arxiv.org/abs/1511.04868">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> RNN </tag>
            
            <tag> RNN-T </tag>
            
            <tag> Sequence-to-Sequence </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_Interspeech2017_Recurrent Neural Aligner：An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping</title>
      <link href="2021/02/21/Paper034/"/>
      <url>2021/02/21/Paper034/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>介绍了一种可用于序列到序列映射任务的编解码器循环神经网络模型——循环神经对齐器（RNA）。与连接主义时间分类（CTC）模型类似，RNA定义了目标标签序列上的概率分布，包括与输入中每个时间步长相对应的空白标签。通过在所有可能的空白标签位置上边际化来计算标签序列的概率。与CTC不同的是，RNA不对标签预测做条件独立假设，它在预测t时刻的标签时，将t-1时刻的预测标签作为递归模型的附加输入，我们将该模型应用于端到端语音识别。由于解码器不使用注意机制，因此RNA能够进行流识别。该模型对转录的声学数据进行训练以预测字素，并且不使用外部语言和发音模型来解码。我们使用近似动态规划方法来优化负对数似然率，并使用基于采样的序列判别训练技术来微调模型以最小化期望词错误率。我们证明了该模型在不使用外部语言模型和进行波束搜索解码的情况下达到了具有竞争力的精度。</p><p><strong>参考</strong><br>[1] <em>Sak H, Shannon M, Rao K, et al. Recurrent Neural Aligner: An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping[C]//Interspeech. 2017, 8: 1298-1302.</em><a href="https://www.isca-speech.org/archive/Interspeech_2017/abstracts/1705.html">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> RNN </tag>
            
            <tag> Sequence-to-Sequence </tag>
            
            <tag> RNA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多伦多大学_ICML2012_Sequence Transduction with Recurrent Neural Networks</title>
      <link href="2021/02/21/Paper033/"/>
      <url>2021/02/21/Paper033/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>许多机器学习任务可以表示为输入序列到输出序列的转换：语音识别、机器翻译、蛋白质二级结构预测和文本到语音等。序列转换的关键挑战之一是学会以一种不受序列失真（如收缩、拉伸和平移）影响的方式来表示输入和输出序列。循环神经网络（RNN）是一种具有强大的序列学习结构，已被证明能够学习这样的表示。然而，在传统上，RNN需要输入和输出序列之间的预先定义的对齐来进行转换。这是一个严重的限制，因为对齐是许多序列转换问题中最困难的部分。事实上，甚至是确定输出序列的长度也常常是具有挑战性的。本文介绍了一种完全基于RNN的端到端概率序列转换系统，该系统原则上能够将任何输入序列转换成任何有限的、离散的输出序列。本文给出了在TIMIT语音语料库上进行音素识别的实验结果。</p><p><strong>参考</strong><br>[1] <em>Graves A. Sequence transduction with recurrent neural networks[J]. arXiv preprint arXiv:1211.3711, 2012.</em><a href="https://arxiv.org/abs/1211.3711">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> RNN </tag>
            
            <tag> RNN-T </tag>
            
            <tag> Sequence-to-Sequence </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多伦多大学_ICML2006_Connectionist temporal classification：labelling unsegmented sequence data with recurrent neural networks</title>
      <link href="2021/02/19/Paper032/"/>
      <url>2021/02/19/Paper032/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>许多现实世界中的序列学习任务需要从有噪声的、未切分的输入数据中预测标签序列。例如，在语音识别中，声音信号被转录成词或子词单元。循环神经网络（RNN）是功能强大的序列学习器，似乎非常适合这样的任务。然而，由于它们需要预先切分的训练数据，以及将其输出转换为标签序列的后处理，因此它们的适用性到目前为止一直受到限制。本文提出了一种训练RNN直接标注未切分序列的新方法，从而解决了这两个问题。在TIMIT语音语料库上的实验表明，该方法优于基线HMM和混合HMM-RNN。</p><p><strong>参考</strong><br>[1] <em>Graves A, Fernández S, Gomez F, et al. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks[C]//Proceedings of the 23rd international conference on Machine learning. 2006: 369-376.</em><a href="http://axon.cs.byu.edu/~martinez/classes/778/Papers/p369-graves.pdf">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> CTC </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CMU_ICASSP2016_Listen, Attend and Spell</title>
      <link href="2021/02/19/Paper031/"/>
      <url>2021/02/19/Paper031/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们介绍了Listen, Attend and Spell（LAS），这是一种学习将语音转录成字符的神经网络。与传统的DNN-HMM模型不同，该模型联合学习语音识别器的所有组件。该系统有两个组件：监听器和拼写器。监听器是接受滤波器组频谱作为输入的金字塔循环网络编码器。拼写器是基于注意力的循环网络解码器，以字符作为输出。该网络输出字符序列，而不在字符之间做出任何独立假设。这是LAS相对于以往端到端CTC模型的关键改进。在谷歌语音搜索任务的一个子集上，LAS在没有词典或语言模型的情况下实现了14.1%的词错误率（WER），在前32个波束上进行语言模型重新评分的情况下实现了10.3%的词错误率。相比之下，最先进的CLDNN-HMM模型达到了8.0%的WER。</p><p><strong>参考</strong><br>[1] <em>Chan W, Jaitly N, Le Q V, et al. Listen, attend and spell[J]. arXiv preprint arXiv:1508.01211, 2015.</em><a href="https://arxiv.org/abs/1508.01211">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> BLSTM </tag>
            
            <tag> Attention </tag>
            
            <tag> Sequence-to-Sequence </tag>
            
            <tag> LAS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>华盛顿州立大学_ICASSP2018_Attention-Based Models for Text-Dependent Speaker Verification</title>
      <link href="2021/02/14/Paper030/"/>
      <url>2021/02/14/Paper030/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>最近，基于注意力的模型在语音识别、机器翻译和图像字幕等一系列任务中表现出很好的性能，这是因为它们能够总结贯穿整个输入序列长度的相关信息。本文分析了注意机制在端到端文本相关说话人识别系统中对序列摘要问题的应用。我们探索了注意力层的不同拓扑结构及其变体，并比较了不同的池化方法在注意力权重上的差异。最终实验结果表明，与非注意力LSTM基线模型相比，基于注意力的模型可以使说话人认证系统的等错误率(EER)提高14%。</p><p><strong>参考</strong><br>[1] <em>rahman Chowdhury F A R, Wang Q, Moreno I L, et al. Attention-based models for text-dependent speaker verification[C]//2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018: 5359-5363.</em><a href="https://arxiv.org/abs/1710.10470">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 说话人认证 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> Attention </tag>
            
            <tag> LSTM </tag>
            
            <tag> TE2E </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DFKI_ICASSP2016_End-to-End Text-Dependent Speaker Verification</title>
      <link href="2021/02/14/Paper029/"/>
      <url>2021/02/14/Paper029/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>本文提出了一种数据驱动的、集成的说话人认证方法，该方法将一条测试语句和几条参考话语直接映射到单个分数进行认证，并使用与测试时相同的评估协议和度量来联合优化系统的组件。这样的方法将产生简单而高效的系统，几乎不需要特定领域的知识和做什么模型假设。我们通过将问题描述为一个单一的神经网络体系结构来实现这一想法，包括只在几条语句上估计说话人模型，并在我们内部的“OK Google”基准上进行评估，以进行依赖于文本的说话人验证。对于像我们这样需要高度精确、易于维护且占用空间小的系统的大数据应用来说，所提出的方法似乎非常有效。</p><p><strong>参考</strong><br>[1] <em>Heigold G, Moreno I, Bengio S, et al. End-to-end text-dependent speaker verification[C]//2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016: 5115-5119.</em><a href="https://arxiv.org/abs/1509.08062v1">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 说话人认证 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> LSTM </tag>
            
            <tag> TE2E </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习与人类语言处理课程_李宏毅_国立台湾大学</title>
      <link href="2021/02/08/Tutorial002/"/>
      <url>2021/02/08/Tutorial002/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_DLHLP20.html">深度学习与人类语言处理课程2020（官网：视频+课件+作业）</a></p></blockquote><blockquote><p><a href="https://www.bilibili.com/video/BV1nt4y1U7Zb?p=1">深度学习与人类语言处理课程2020（完整版）</a></p></blockquote><blockquote><p><a href="https://www.bilibili.com/video/BV1q5411V7tT?from=search&seid=15419541146503431608">深度学习与人类语言处理课程2020之语音识别</a></p></blockquote><ul><li><p>深度学习与人类语言处理课程介绍</p></li><li><p>语音识别课程视频内容</p><ul><li>ASR Summary</li><li>Listen,Attention and Spell(LAS)</li><li>Connectionist Temporal Classification(CTC)</li><li>RNN Transducer(RNN-T)</li><li>Neural Transducer</li><li>Monotonic Chunkwise Attention(MoChA)</li><li>Hidden Markov Model(HMM)</li><li>Alignment of HMM/CTC/RNN-T</li><li>Score Computation of HMM/CTC/RNN-T</li><li>Decoding of HMM/CTC/RNN-T</li><li>Language Model(N-Gram/NN/RNN)</li><li>Shallow/Deep Fusion/Cold Fusion</li></ul></li><li><p>语音转换课程视频内容</p><ul><li>Content/Speaker Encoder-Decoder</li><li>CycleGAN</li><li>StarGAN</li><li>Blow</li></ul></li><li><p>语音分离课程视频内容</p><ul><li>Deep Clustering</li><li>Permutation Invariant Training(PIT)</li><li>Time-domain Audio Separation Network(TasNet)</li></ul></li><li><p>语音合成课程视频内容</p><ul><li>Concatenative Approach</li><li>Parametic Approach</li><li>DeepVoice 1/3</li><li>Tacotron 1/2</li><li>FastSpeech/DurIAN</li><li>Dual Learning</li><li>Speaker Embedding/Global Style Token(GST)</li></ul></li><li><p>语音验证(说话人认证)课程视频内容</p><ul><li>Speaker Embedding</li><li>i-vector</li><li>d-vector</li><li>x-vector</li><li>End-to-End</li></ul></li><li><p>自然语言处理课程视频内容</p><ul><li>Category</li><li>Part-of-Speech(POS) Tagging</li><li>Word Segmentation</li><li>Parsing</li><li>Coreference Resolution</li><li>Summarization</li><li>Machine Translation</li><li>Grammar Error Correction</li><li>Sentiment Classification</li><li>Stance Detection</li><li>Veracity Prediction</li><li>Natural Language Inference(NLI)</li><li>Search Engine</li><li>Question Answering(QA)</li><li>Dialogue</li><li>Natural Language Generation(NLG)</li><li>Natural Language Understanding(NLU)</li><li>Knowledge Extraction</li><li>Name Entity Recognition(NER)</li><li>Relation Extraction    </li><li>BERT</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音识别 </tag>
            
            <tag> 语音转换 </tag>
            
            <tag> 说话人认证 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 语音分离 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_Interspeech2020_Conformer：Convolution-augmented Transformer for Speech Recognition</title>
      <link href="2021/02/02/Paper028/"/>
      <url>2021/02/02/Paper028/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>摘要：近年来，基于Transformer和卷积神经网络（CNN）的模型在自动语音识别（ASR）中取得了优于循环神经网络（RNNs）的良好效果。Transformer模型擅长捕获基于内容的全局交互，而CNN有效地利用局部特征。在这项工作中，我们通过研究如何将卷积神经网络和Transformer结合起来，以一种有效参数的方式对音频序列的局部和全局依赖性进行建模，从而达到两者的最佳效果。为此，我们提出了一种用于语音识别的卷积增强Transformer，称为Conformer。Conformer的性能明显优于之前基于Transformer和CNN的模型，实现了最先进的精确度。在广泛使用的LibriSpeech基准测试上，其中在Test和TestOther数据集上，我们的模型在没有使用语言模型的情况下获得了2.1%和4.3%的WER，在使用外部语言模型的情况下获得了1.9%和3.9%的WER。我们还观察到仅用10M参数的一个小模型的竞争性能为2.7%和6.3%。</p><p><strong>参考</strong><br>[1] <em>Gulati A, Qin J, Chiu C C, et al. Conformer: Convolution-augmented transformer for speech recognition[J]. arXiv preprint arXiv:2005.08100, 2020.</em><a href="https://arxiv.org/abs/2005.08100">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> Attention </tag>
            
            <tag> CNN </tag>
            
            <tag> Transformer </tag>
            
            <tag> Conformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SLT2021-儿童语音识别挑战赛研讨会</title>
      <link href="2021/02/01/Tutorial001/"/>
      <url>2021/02/01/Tutorial001/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p><a href="https://www.data-baker.com/csrc_challenge.html">SLT2021-儿童语音识别挑战赛官网</a></p></blockquote><p align="center">  <img src= "/img/loading.gif" data-lazy-src="/img/tutorials/tutorial_001-02.png" width = 520 height = 240/></p><blockquote><p><a href="https://www.bilibili.com/video/BV15N411o7JL">SLT2021-儿童语音识别挑战赛研讨会</a></p></blockquote><ul><li>研讨会视频内容<ul><li>赛道1分享-冠军队伍（上海交通大学智能语音实验室）</li><li>赛道2分享-冠军队伍（小米&amp;微软小冰&amp;Seasalt AI）</li><li>面向端侧部署的流式端到端语音识别技术-谢磊（西北工业大学）</li><li>基于直通梯度的端到端语音识别神经架构搜索-欧智坚（清华大学）</li><li>语音技术及其在美团的应用-黄辰（美团）</li><li>贝克找房的语音技术应用与研究-汤志远（贝克找房）</li><li>数据助力技术，技术助力数据-吴本谷（标贝科技）</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_ICASSP2018_Generalized End-to-End Loss for Speaker Verification</title>
      <link href="2021/01/27/Paper027/"/>
      <url>2021/01/27/Paper027/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>本文提出了一种新型的损失函数，称为广义端到端（GE2E）损失函数，使得说话人认证模型的训练比以前基于元组的端到端（TE2E）损失函数更加有效。与TE2E不同，GE2E损失函数以强调实例的方式更新网络，这些实例在训练过程的每个步骤中都难以验证。此外，GE2E损失函数不需要选择示例的初始阶段。利用这些特性，我们的新损失函数模型使说话人验证EER降低了10%以上，同时使训练时间减少了60%。从而学习出一个更好的模型。我们还介绍了MultiReader技术，它允许我们进行域自适应训练一个更精确的模型——支持多个关键字（即OK Google和Hey Google）以及多个方言。</p><p><strong>参考</strong><br>[1] <em>Wan L, Wang Q, Papir A, et al. Generalized end-to-end loss for speaker verification[C]//2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018: 4879-4883.</em><a href="https://arxiv.org/abs/1710.10467">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 说话人认证 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> LSTM </tag>
            
            <tag> GE2E </tag>
            
            <tag> TE2E </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>下班后要不要回工作消息——从八小时工作制说起</title>
      <link href="2021/01/25/Essay001/"/>
      <url>2021/01/25/Essay001/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>“八小时工作制是一个进步”，今天看了奇葩说，对刘擎教授这句话印象很深刻。</p><p>刚巧今早上看到豆瓣一篇抵制拼多多的帖子，核心观点即是拼多多招聘要求员工9-9-6甚至11-11-6，更可怕的是，作者在后续面试其他公司的时候，对于工作时间提出异议，HR称，最起码比你面试的拼多多强。</p><p>八小时工作制应该是一个基本的要求，而不是让一个又一个的互联网巨头一点点突破劳动法限制，甚至成为一个冠冕堂皇的优待理由。</p><p>前几年，当马云公开宣称9-9-6时，大概还是骂声居多，而现在，9-9-6已成社会常态。</p><p>结合前阵子拼多多、饿了么员工猝死事件，虽然我们提倡拼搏，但人不是工具，社会进步应当给不同层次的人以生存空间而非适者生存，我们的社会进化了那么多年，不是为了让社会法则发展为社会进化论。</p><p>就我们个体而言，如果对于不公的社会事件，每个人都觉得与己无关而闭口不言，那么最后总有一个与你有关的对你不利的决策或事件，到时大家都已学会闭口不言。</p><p>又说到郑爽事件，为何社会对于代孕反对的声音如此激烈，根本原因在于代孕是将人体当作工具，如果子宫可以被标价、胎儿可以被买卖，那么这个范围迟早会被扩大，肾脏、器官，乃至生命。</p><p>个人的身体自己可以做主吗，在一定程度上说，你的身体不属于你，你不能自行对其处置。就代孕以及弃养事件而言，从根本上反映出的还是社会对于人的物化反感以及对己对于弱者的处境的同情，乃至害怕，社会上毕竟弱者是大多数。</p><p>与9-9-6所一致，现代社会节奏越来越快，但人不应仅仅被当作追求利益的工具，社会生活方便带来的是人个人时间的碎片化、压缩化，何时何地都能够被找到，真的是一件好事吗？</p><p>从个人朴素的价值观而言，每个人有每个人不同的人生追求，有人热爱工作有人仅仅将工作作为挣钱工具。</p><p>一部分人将工作上的成功视为自己的毕生追求，自有不成功便成仁一说，对于那类人而言，世俗意义上的成功能够让其获得极大的成就感和满足感，普通人的生活会让他感到极大的痛苦。在这种目标导向下，将人生燃烧给自己的事业是个人选择，疯狂拼搏之后的成功或失败只是选择之后带来的一个结果而已。对于此类人而言，工作就是生活，八小时当然不能简单适用。</p><p>但也有其他一部分人，喜欢简简单单的生活，工作中的一个小成就，生活中的一点小确幸，足够感到幸福开心。或有人将工作视为人生的一个路径，赚钱也好，打发时间也罢，只是给自己找点事做。</p><p>工作只是生活的一部分，与下厨、读书、徒步、蹦迪一样，应当是由人自行安排的人生事件之一，相对只是工作占据的时间较多而已，而对于占据人生已经绝大多数时间的工作，如果我们并不想却还毫无底线的任其扩大范围和界限，那么生活何谓生活，家人、自己位于何处，不得而知。</p><p>所谓下班后工作消息要不要回，其实，作为辩题，其讨论之根本就在于每个人想法不一致，如上，对于第一类人而言，生活就是工作，就不存在下班一说了，可能他人生中的所有事情在一定阶段范畴内都要为工作让位。对于其他人而言，工作既然有上下班一说，势必从本心上讲是不希望自己下班后的其他安排被工作打乱的，哪怕只是一个小小的消息。</p><p>说这么多，下班后领导给我发消息，心里说：“呸，别打扰我生活”。实际上还不得立刻：“好的收到”。所谓社畜，不外如是。</p><p>那这篇乱七八糟没有核心思想的文字，就作为马东所说“屈服于生活的苟且，但依然怀抱诗和远方”的见证吧。</p>]]></content>
      
      
      <categories>
          
          <category> Essay </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生活随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>出门问问_ICASSP2019_Adversarial Examples for Improving End-to-end Attention-based Small-Footprint Keyword Spotting</title>
      <link href="2021/01/24/Paper026/"/>
      <url>2021/01/24/Paper026/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>在这篇文章中，我们探索使用对抗性样例来改进一个基于神经网络的关键词检测（KWS）系统。特别地，在我们的系统中，使用了一种有效的、小占用空间的基于注意力的神经网络模型。对抗性样例被模型定义为错误分类的样例，但它与原始的正确分类的样例相比只有轻微的偏差。在KWS任务中，将误唤醒或误拒绝的查询视为某种对抗性的样例是很自然的。在我们的工作中，在给定一个训练有素的基于注意力的KWS模型的情况下，我们首先使用快速梯度符号方法（FGSM）生成对抗性样例，发现这些样例会显著降低KWS的性能。使用这些对抗性样例作为扩充数据，对KWS模型进行再训练，最终在从智能说话人收集的数据集上以每小时1.0次的误报率（FAR）取得了45.6%的相对拒绝率和误拒率（FRR）。</p><p><strong>参考</strong><br>[1] <em>Wang X, Sun S, Shan C, et al. Adversarial examples for improving end-to-end attention-based small-footprint keyword spotting[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019: 6366-6370.</em><a href="http://ssli.ee.washington.edu/~mhwang/mobvoi/2019/icassp-wang.pdf">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音唤醒 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>出门问问_ICASSP2019_Knowledge Distillation for Recurrent Neural Network Language Modeling with Trust Regularization</title>
      <link href="2021/01/24/Paper025/"/>
      <url>2021/01/24/Paper025/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>循环神经网络（RNNs）因其优于传统的基于N元语法的模型而在语言建模中占据主导地位。在许多应用中，通常使用大型循环神经网络语言模型（RNNLM）或几个RNNLM的集成。这些模型占用的内存很大且需要大量的计算。在这篇文章中，我们研究了应用知识蒸馏来降低RNNLMs模型大小的效果。此外，我们还提出了一种信任正则化方法来改进RNNLM的知识提炼训练。利用信任正则化的知识精馏，我们将参数大小减少到以前发表的最佳模型的三分之一，同时保持了对Penn Treebank数据的最先进困惑度结果。在语音识别N-bestrescoring任务中，我们将RNNLM模型的规模减少到基线系统的18.5%，而在华尔街日报数据集上的错词率（WER）性能没有下降。</p><p><strong>参考</strong><br>[1] <em>Shi Y, Hwang M Y, Lei X, et al. Knowledge distillation for recurrent neural network language modeling with trust regularization[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019: 7230-7234.</em><a href="https://arxiv.org/abs/1904.04163">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语言模型 </tag>
            
            <tag> RNNLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>出门问问_ICASSP2019_End-To-End Speech Recognition Using A High Rank LSTM-CTC Based Model</title>
      <link href="2021/01/24/Paper024/"/>
      <url>2021/01/24/Paper024/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>基于 LSTM-CTC 的端到端语音模型， 由于其简单的训练过程以及高效的解码过程，最近在语音识别领域得到广泛地应用。在传统的 LSTM-CTC 模型中，瓶颈投影矩阵将从LSTM获得的隐藏特征向量映射到Softmax输出层。在本文中，我们提出用高阶投影层来代替投影矩阵。高阶投影层的输出是通过不同的投影矩阵和非线性激活函数从隐藏特征向量投影的向量加权组合。高阶投影层能够提高LSTM-CTC模型的表达能力。实验结果表明，在华尔街日报（WSJ）语料库和LibriSpeech数据集上，该方法比基准CTC系统的相对单词错误率（WER）降低了4%-6%。在不使用额外数据或数据增强的情况下，该模型的性能优于其他已发表的基于CTC的端到端（E2E）模型。</p><p><strong>参考</strong><br>[1] <em>Shi Y, Hwang M Y, Lei X. End-to-end speech recognition using a high rank lstm-ctc based model[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019: 7080-7084.</em><a href="https://arxiv.org/abs/1903.05261">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>lstm_ctc</em><a href="https://github.com/mobvoi/lstm_ctc">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> CTC </tag>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>出门问问_20201210_Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition</title>
      <link href="2021/01/23/Paper023/"/>
      <url>2021/01/23/Paper023/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>在本文中，我们提出了一种新型的两遍方法来将流式和非流式端到端（E2E）语音识别统一在一个模型中。该模型采用混合CTC和注意力架构，对编码器中的一致性层进行了修改。我们提出了一种基于动态组块的注意策略来允许输入任意合适的上下文长度。在推理时，CTC解码器以流式方式生成n个最佳假设。只需改变块大小，就可以很容易地控制推理延迟。然后，注意力解码器对CTC假设进行重新评分以得到最终结果。这种高效的重新评分过程只会导致非常小的句子级延迟。在开放170小时的AISHELL-1数据集上的实验表明，该方法能够简单有效地统一流式模型和非流式模型。在AISHELL-1测试集上，与标准非流式Tansformer相比，我们的统一模型在非流式ASR上的相对字符错误率（CER）降低了5.60%。同样的模型在流式ASR系统中获得了5.42%的CER和640ms的延迟。</p><p><strong>参考</strong><br>[1] <em>Zhang B, Wu D, Yao Z, et al. Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition[J]. arXiv preprint arXiv:2012.05481, 2020.</em><a href="https://arxiv.org/abs/2012.05481">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>wenet</em><a href="https://github.com/mobvoi/wenet">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> CTC </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>百度_Interspeech2018_Cold Fusion：Training Seq2Seq Models Together with Language Models</title>
      <link href="2021/01/19/Paper022/"/>
      <url>2021/01/19/Paper022/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>基于注意力的序列到序列（Seq2Seq）模型在机器翻译、图像字幕和语音识别等生成自然语言句子的任务中表现出色。通过利用未标记的数据（通常以语言模型的形式），进一步提高了性能。在这项工作中，我们提出了冷融合方法，该方法在训练过程中利用预先训练好的语言模型，并在语音识别任务中证明了该方法的有效性。结果表明，采用冷融合的Seq2Seq模型能够更好地利用语言信息，1）具有更快的收敛速度和更好的泛化能力；2）在不到10%的标记训练数据的情况下，几乎完全迁移到新的领域。</p><p><strong>参考</strong><br>[1] <em>Sriram A, Jun H, Satheesh S, et al. Cold fusion: Training seq2seq models together with language models[J]. arXiv preprint arXiv:1708.06426, 2017.</em><a href="https://arxiv.org/abs/1708.06426">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> Attention </tag>
            
            <tag> Sequence-to-Sequence </tag>
            
            <tag> Deep Fusion </tag>
            
            <tag> Cold Fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>百度_20171105_Robust Speech Recognition Using Generative Adversarial Networks</title>
      <link href="2021/01/19/Paper021/"/>
      <url>2021/01/19/Paper021/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>本文描述了一个通用的、可扩展的、端到端的框架，该框架使用生成对抗网络（GAN）目标来实现鲁棒性语音识别。通过学习将有噪声的音频映射到与干净音频相同的嵌入空间，使用该方法训练的编码器具有改善后的不变性。与以往的方法不同，新的框架不依赖于信号处理中经常需要的领域专业知识或简化假设，而是以数据驱动的方式直接激励鲁棒性。实验结果表明，该方法在不需要专门的前端或预处理的情况下，改善了普通的序列到序列模型的模拟远场语音识别。</p><p><strong>参考</strong><br>[1] <em>Sriram A, Jun H, Gaur Y, et al. Robust speech recognition using generative adversarial networks[C]//2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018: 5639-5643.</em><a href="https://arxiv.org/abs/1711.01567">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语音增强 </tag>
            
            <tag> SEGAN </tag>
            
            <tag> Attention </tag>
            
            <tag> WGAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>百度_ASRU2017_Exploring Neural Transducers for End-to-End Speech Recognition</title>
      <link href="2021/01/19/Paper020/"/>
      <url>2021/01/19/Paper020/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>在这项工作中，我们对端到端语音识别的CTC模型、RNN-Transducer模型和基于注意力的Seq2Seq模型进行了实证比较。我们证明，在没有任何语言模型的情况下，Seq2Seq和RNN-Transducer模型在流行的Hub5‘00基准上都优于报道的最好的有语言模型的CTC模型。在我们内部多样化的数据集上，这些趋势仍在继续——RNN-Transducer模型在beam search后使用语言模型进行重新评分，表现优于我们最好的CTC模型。这些结果简化了语音识别流水线，因此解码现在可以纯粹表示为神经网络操作。我们还研究了编码器体系结构的选择如何影响这三种模型的性能——当所有编码层都是仅前向的，以及当编码器积极地对输入表示进行下采样时。</p><p><strong>参考</strong><br>[1] <em>Battenberg E, Chen J, Child R, et al. Exploring neural transducers for end-to-end speech recognition[C]//2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017: 206-213.</em><a href="https://arxiv.org/abs/1707.07413">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> CTC </tag>
            
            <tag> Attention </tag>
            
            <tag> RNN-T </tag>
            
            <tag> Sequence-to-Sequence </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>百度_ICML2015_Deep Speech 2：End-to-End Speech Recognition in English and Mandarin</title>
      <link href="2021/01/18/Paper019/"/>
      <url>2021/01/18/Paper019/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们证明了一种端到端的深度学习方法可以用于识别英语或普通话——两种截然不同的语言。由于该方法用神经网络取代了整个手工设计的组件管道，端到端学习使我们能够处理各种各样的语音，包括嘈杂环境、口音和不同的语言。我们方法的关键是我们的HPC（高性能计算）技术的应用，以致于比我们以前的系统提速7倍。由于这种效率，以前需要几周时间的实验，而现在只需几天就能完成。这使我们能够更快地迭代以验证更优秀的架构和算法。因此，在一些情况下，当以标准数据集为基准时，我们的系统与人类工作者的转录结果一样好。最后，通过在数据中心使用GPUs进行批量调度，我们证明了我们的系统可以以低廉的成本在线部署，并在大规模服务用户时提供低延迟。</p><p><strong>参考</strong><br>[1] <em>Amodei D, Ananthanarayanan S, Anubhai R, et al. Deep speech 2: End-to-end speech recognition in english and mandarin[C]//International conference on machine learning. 2016: 173-182.</em><a href="https://arxiv.org/abs/1512.02595">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>DeepSpeech-PaddlePaddle</em><a href="https://github.com/PaddlePaddle/DeepSpeech">[GitHub]</a><br>[2] <em>DeepSpeech-TensorFlow</em><a href="https://github.com/mozilla/DeepSpeech">[GitHub]</a><br>[3] <em>DeepSpeech-PyTorch</em><a href="https://github.com/SeanNaren/deepspeech.pytorch">[GitHub]</a><br>[4] <em>DeepSpeech-Neon</em><a href="https://github.com/NervanaSystems/deepspeech">[GitHub]</a><br>[5] <em>DeepSpeech-MXNet</em><a href="https://github.com/samsungsds-rnd/deepspeech.mxnet">[GitHub]</a><br>[6] <em>DeepSpeech-Keras</em><a href="https://github.com/robmsmt/KerasDeepSpeech">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语言模型 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 声学模型 </tag>
            
            <tag> 解码器 </tag>
            
            <tag> CTC </tag>
            
            <tag> RNN </tag>
            
            <tag> CNN </tag>
            
            <tag> GRU </tag>
            
            <tag> LSTM </tag>
            
            <tag> KenLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>康奈尔大学_UAI2018_Averaging Weights Leads to Wider Optima and Better Generalization</title>
      <link href="2021/01/16/Paper018/"/>
      <url>2021/01/16/Paper018/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>深度神经网络通常通过优化具有SGD变量的损失函数，结合衰减的学习率来训练，直到收敛。我们发现，在使用周期性或固定学习率情况下，沿着SGD轨迹的多个点的简单平均比传统的训练具有更好的泛化能力。我们还发现，这种随机加权平均(SWA)方法得到了比SGD方法更平坦的解，并且用一个单一模型逼近了最近的快速几何集成(FGE)方法。我们把SWA用在CIFAR-10、CIFAR-100和ImageNet中一系列最先进的残差网络、金字塔网、DenseNets和Shake-Shake网络上，其测试精度比传统的SGD训练获得了显著改进。简而言之，SWA非常容易实现，提高了通用性，并且几乎没有计算开销。</p><p><strong>参考</strong><br>[1] <em>Izmailov P, Podoprikhin D, Garipov T, et al. Averaging weights leads to wider optima and better generalization[J]. arXiv preprint arXiv:1803.05407, 2018.</em><a href="https://arxiv.org/abs/1803.05407">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>swa</em><a href="https://github.com/timgaripov/swa">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随机加权平均 </tag>
            
            <tag> 周期性学习率 </tag>
            
            <tag> 随机梯度下降 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>昆士兰科技大学_20201225_SWA Object Detection</title>
      <link href="2021/01/13/Paper017/"/>
      <url>2021/01/13/Paper017/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>你想为目标检测器改进1.0 的平均精度（AP），而不需要任何推理成本和对检测器进行任何更改吗？让我们告诉你这样一个方案吧。该方案非常简单：使用周期性学习率训练您的检测器以获得额外的12个epoch，然后将这12个产生的模型取平均作为你的最终检测模型。这种有效的方法是受到随机加权平均算法的启发，该算法是在<a href="https://arxiv.org/abs/1803.05407">此论文</a>中被提出，目的是提高深度神经网络的泛化能力。我们发现它在目标检测方面也非常有效。在这份技术报告中，我们系统地研究了随机加权平均（SWA）在目标检测和实例分割中的应用效果。通过大量的实验，我们发现一种使用SWA进行目标检测的良好策略，并且在具有挑战性的COCO基准测试中，我们在各种流行的检测器上一致地实现了约1.0的平均精度改进。我们希望这项工作能够使更多的目标检测研究人员了解这项技术，并帮助他们训练出更好的目标检测器。</p><p><strong>参考</strong><br>[1] <em>Zhang H, Wang Y, Dayoub F, et al. SWA Object Detection[J]. arXiv preprint arXiv:2012.12645, 2020.</em><a href="https://arxiv.org/abs/2012.12645">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>swa_object_detection</em><a href="https://github.com/hyz-xmaster/swa_object_detection">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> 随机加权平均 </tag>
            
            <tag> 周期性学习率 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>百度_20141219_Deep Speech：Scaling up end-to-end speech recognition</title>
      <link href="2021/01/11/Paper016/"/>
      <url>2021/01/11/Paper016/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们提出一个使用端到端深度学习开发的最先进的语音识别系统。我们的架构比传统的语音系统要简单得多，传统的语音系统依赖于费力的工程处理流程；当在嘈杂的环境中使用时，这些传统的系统也往往表现不佳。相比之下，我们的系统不需要手工设计的组件来模拟背景噪声、混响或扬声器变化，而是直接学习一个对这些影响具有鲁棒性的函数。相比之下，我们的系统不需要手工设计的组件来模拟背景噪声、混响或说话人变化，而是直接学习一个对这些影响具有鲁棒性的函数。我们不需要音素词典，甚至不需要“音素”的概念。该方法的关键是一个优化良好的RNN训练系统，使用多个gpu，以及一套新型的数据合成技术，使我们能够有效地获取大量不同的数据来进行训练。我们的系统称为DeepSpeech，在广泛研究的Switchboard Hub5’00数据集上优于以前发表的结果，在整个测试集实现了16.0%的错误率。与广泛使用的、最先进的商业语音系统相比，DeepSpeech还能更好地处理充满挑战的嘈杂环境。</p><p><strong>参考</strong><br>[1] <em>Hannun A, Case C, Casper J, et al. Deep speech: Scaling up end-to-end speech recognition[J]. arXiv preprint arXiv:1412.5567, 2014.</em><a href="https://arxiv.org/abs/1412.5567">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语言模型 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 声学模型 </tag>
            
            <tag> 解码器 </tag>
            
            <tag> CTC </tag>
            
            <tag> RNN </tag>
            
            <tag> N-Gram </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MERL_2017_Hybrid CTC/Attention Architecture for End-to-End Speech Recognition</title>
      <link href="2021/01/10/Paper015/"/>
      <url>2021/01/10/Paper015/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>基于隐马尔可夫模型（HMM）和深度神经网络（DNN）的传统自动语音识别（ASR）是一个非常复杂的系统，由声学模型、词典和语言模型等模块组成。该系统还需要语言学信息，如发音词典、词语切分和语音上下文相关树。另一方面，端到端自动语音识别已经成为一种流行的替代方法，通过用单一的深度网络结构表示复杂的模块，并用数据驱动的学习方法代替语言学信息的使用，这大大简化了传统自动语音识别系统的建模过程。ASR的端到端架构主要有两种：一种是基于Attention的方法，利用注意力机制来实现声学帧和识别符号之间的对齐；另一种是基于联结主义时间分类（CTC）的方法，利用马尔可夫假设，通过动态规划有效地解决序列问题。本文提出混合CTC和Attention的端到端ASR，该系统有效地利用了这两种结构在训练和解码方面的优势。在训练过程中，我们采用多目标学习框架来提高鲁棒性和实现快速收敛。在解码过程中，我们结合基于Attention和CTC的分数，采用一次性波束搜索算法进行联合解码，进一步消除不规则对齐。通过对英语（WSJ和CHiME-4）任务的实验，证明了所提出的多目标学习方法在基于CTC和Attention的编解码基线上的有效性。此外，将该方法应用于两个大规模ASR基准测试（自然日语和汉语普通话），基于多目标学习和无需语言学信息的联合解码的优势性能与传统DNN和HMM的ASR系统旗鼓相当。</p><p><strong>参考</strong><br>[1] <em>Watanabe S, Hori T, Kim S, et al. Hybrid CTC/attention architecture for end-to-end speech recognition[J]. IEEE Journal of Selected Topics in Signal Processing, 2017, 11(8): 1240-1253.</em><a href="https://www.merl.com/publications/docs/TR2017-190.pdf">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 声学模型 </tag>
            
            <tag> 解码器 </tag>
            
            <tag> CTC </tag>
            
            <tag> Attention </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速使用阿里云语音识别和合成API服务</title>
      <link href="2021/01/09/Blog004/"/>
      <url>2021/01/09/Blog004/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="下载与解压SDK"><a href="#下载与解压SDK" class="headerlink" title="下载与解压SDK"></a>下载与解压SDK</h1><p>下载<br><a href="http://download.taobaocdn.com/freedom/C3143/compress/alibabacloud-nls-python-sdk.zip?spm=a2c4g.11186623.2.18.5b631e0dFV0YRE&file=alibabacloud-nls-python-sdk.zip">Python版阿里云语音合成SDK</a><br>解压</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip alibabacloud-nls-python-sdk.zip </span><br></pre></td></tr></table></figure><h1 id="安装SDK"><a href="#安装SDK" class="headerlink" title="安装SDK"></a>安装SDK</h1><p>安装Python管理工具</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install setuptools</span><br></pre></td></tr></table></figure><p>打包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd alibabacloud-nls-python-sdk   </span><br><span class="line">python setup.py bdist_egg  </span><br></pre></td></tr></table></figure><p>安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python setup.py install  </span><br></pre></td></tr></table></figure><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><p>将申请阿里云语音合成API的appkey和token填入到对应第99和第100行的位置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python speech_synthesizer_demo.py</span><br></pre></td></tr></table></figure><p>将申请阿里云语音识别API的appkey和token填入到对应第104和第105行的位置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python speech_recognizer_demo.py</span><br></pre></td></tr></table></figure><p>将申请阿里云语音识别API的appkey和token填入到对应第114和第115行的位置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python speech_transcriber_demo.py</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音识别 </tag>
            
            <tag> 阿里云服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MERL_ICASSP2017_Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning</title>
      <link href="2021/01/09/Paper014/"/>
      <url>2021/01/09/Paper014/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>近年来，端到端语音识别技术受到了越来越多的关注，它可以将语音直接转录成文本，而无需任何预先定义的对齐方式。一种方法是基于注意力机制的编码器和解码器框架，该框架通过纯数据驱动的方法一步到位地学习可变长度输入和输出序列之间的映射关系。注意力机制模型已被证明比另一种端到端的方法——联结主义时间分类（CTC）更能提高性能，主要是因为它明确地使用了目标特征的历史信息，而没有任何条件独立性的假设。然而，我们观察到，在噪声环境下注意力机制的表现很差，并且在输入长序列的初始训练阶段很难去学习。在这种情况下，由于缺乏CTC所使用的从左到右的约束，而注意力模型过于灵活，无法预测正确的对齐。本文提出了一种新型的端到端语音识别方法，通过在多任务学习框架下使用联合CTC和注意力机制模型来提高鲁棒性和实现快速收敛，从而缓解了对齐问题。在WSJ和CHiME-4任务上的实验表明，该方法比基于CTC和基于注意力机制的编解码器基线都有优势，字符错误率（CER）相对提高了5.4-14.6%。</p><p><strong>参考</strong><br>[1] <em>Kim S, Hori T, Watanabe S. Joint CTC-attention based end-to-end speech recognition using multi-task learning[C]//2017 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2017: 4835-4839.</em><a href="https://arxiv.org/abs/1609.06773">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 声学模型 </tag>
            
            <tag> 解码器 </tag>
            
            <tag> CTC </tag>
            
            <tag> Attention </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>约翰斯·霍普金斯大学_Interspeech2018_ESPnet：End-to-End Speech Processing Toolkit</title>
      <link href="2021/01/07/Paper013/"/>
      <url>2021/01/07/Paper013/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>本文介绍一种新的端到端语音处理开源平台ESPnet。ESPnet主要致力于端到端自动语音识别（ASR），并采用了广泛使用的动态神经网络工具包Chainer和PyTorch作为主要的深度学习引擎。ESPnet还按照Kaldi ASR工具包风格进行数据处理、特征提取方法和存储格式，并提供了用于语音识别和其他语音处理实验的完整安装教程。本文介绍了该软件平台的主要架构、一些重要功能（这些功能将ESPnet与其他开源ASR工具包区分开）以及带有主要ASR基准的实验结果。</p><p><strong>参考</strong><br>[1] <em>Watanabe S, Hori T, Karita S, et al. Espnet: End-to-end speech processing toolkit[J]. arXiv preprint arXiv:1804.00015, 2018.</em><a href="https://arxiv.org/abs/1804.00015">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音识别 </tag>
            
            <tag> 语言模型 </tag>
            
            <tag> HMM </tag>
            
            <tag> Kaldi </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 声学模型 </tag>
            
            <tag> DNN </tag>
            
            <tag> TDNN </tag>
            
            <tag> BLSTM </tag>
            
            <tag> LF-MMI </tag>
            
            <tag> Chain </tag>
            
            <tag> 解码器 </tag>
            
            <tag> CTC </tag>
            
            <tag> Attention </tag>
            
            <tag> RNNLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>东京大学_AAAI2021_Towards Fully Automated Manga Translation</title>
      <link href="2021/01/05/Paper012/"/>
      <url>2021/01/05/Paper012/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们解决了日本漫画的机器翻译问题。漫画翻译在机器翻译中涉及两个重要的问题：上下文感知和多模式翻译。由于在漫画中文本和图像以非结构化的方式混合在一起，因此从图像中获取上下文对于漫画翻译至关重要。但是，如何从图像中提取上下文并整合到机器模型中仍然是一个悬而未决的问题。此外，目前尚没有用于训练和评估这类模型的语料库和基准。在本文中，我们做出了以下四个贡献，为漫画翻译研究奠定了基础。首先，我们提出了多模式上下文感知翻译框架。我们是第一个整合从漫画图像获得上下文信息的团队。这使我们能够翻译那些不用上下文信息（例如，其他对话气泡中的文本，说话者的性别等）而无法翻译的对话气泡中的文本。其次，为了训练模型，我们提出了从成对的原始漫画及其翻译中自动构建语料库的方法，通过该方法可以构建大型并行语料库而无需任何人工标记。第三，我们创建了一个新的基准来评估漫画翻译。最后，在我们提出的方法之上，我们设计了第一个用于全自动漫画翻译的完整系统。</p><p><strong>参考</strong><br>[1] <em>Hinami R, Ishiwatari S, Yasuda K, et al. Towards Fully Automated Manga Translation[J]. arXiv preprint arXiv:2012.14271, 2020.</em><a href="https://arxiv.org/abs/2012.14271">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器翻译 </tag>
            
            <tag> 漫画翻译 </tag>
            
            <tag> 文字检测 </tag>
            
            <tag> 文字识别 </tag>
            
            <tag> 文字嵌入 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>查尔斯大学_Interspeech2020_One Model, Many Languages：Meta-learning for Multilingual Text-to-Speech</title>
      <link href="2020/12/28/Paper011/"/>
      <url>2020/12/28/Paper011/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们介绍一种多语种语音合成的方法，该方法利用上下文参数生成的元学习概念，使用比以往方法更多的语种和更少的训练数据生成听起来更自然的多语种语音。我们的模型是基于完全卷积输入文本编码器的Tacotron 2，该编码器的权重由一个单独的参数生成器网络得到。为了优化语音克隆，该模型使用了一个带有梯度反转层的对抗性说话人分类器，该分类器从编码器中移除特定于说话人的信息。为了评估，我们安排了两组实验来比较我们的模型和使用不同的跨语言参数共享的基线：（1）在低数据量训练时的稳定性和性能，（2）语码转换合成的发音准确度和语音质量。在训练中，我们使用了CSS10数据集和基于五种语言的常见语音记录的新型小数据集。我们的模型被证明能有效地跨语言共享信息，并且通过主观评价测试，它产生比基线更自然和准确的语码转换语音。</p><p><strong>参考</strong><br>[1] <em>Nekvinda T, Dušek O. One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech[J]. arXiv preprint arXiv:2008.00768, 2020.</em><a href="https://arxiv.org/abs/2008.00768">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>Multilingual_Text_to_Speech</em><a href="https://github.com/Tomiinek/Multilingual_Text_to_Speech">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音转换 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Tacotron2 </tag>
            
            <tag> 多语言/跨语言 </tag>
            
            <tag> WaveRNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>台湾大学_Interspeech2019_One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization</title>
      <link href="2020/12/28/Paper010/"/>
      <url>2020/12/28/Paper010/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>近年来，无需平行数据的语音转换（VC）已成功地应用于多目标场景，即训练单个模型将输入语音转换为多个不同的说话人。然而，这种模型存在着只能将语音转换为训练数据中说话人的局限性，这就限制了语言转换的适用场景。本文提出了一种新型的One-shot语音转换方法，该方法只需源说话人和目标说话人的一个示例话语就可以实现语音转换，而且在训练过程中甚至不需要出现源说话人和目标说话人。这是通过实例规范化（IN）实现说话人和内容表征分离。客观和主观评价表明，该模型能够生成与目标说话人相似的语音。除了性能测量之外，我们还证明了该模型能够在没有任何监督的情况下学习有意义的说话人表征。</p><p><strong>参考</strong><br>[1] <em>Chou J, Yeh C, Lee H. One-shot voice conversion by separating speaker and content representations with instance normalization[J]. arXiv preprint arXiv:1904.05742, 2019.</em><a href="https://arxiv.org/abs/1904.05742">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>adaptive_voice_conversion</em><a href="https://github.com/jjery2243542/adaptive_voice_conversion">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音转换 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> Tacotron </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Griffin-Lim </tag>
            
            <tag> 声纹编码器 </tag>
            
            <tag> 内容编码器 </tag>
            
            <tag> Instance Normalization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>香港中文大学_2016_Phonetic posteriorgrams for many-to-one voice conversion without parallel data training</title>
      <link href="2020/12/28/Paper009/"/>
      <url>2020/12/28/Paper009/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>本文提出了一种基于非平行训练数据的新型语音转换方法。该想法是通过与说话者无关的自动语音识别（SI-ASR）系统获得的语音后验图（PPGs）来建立说话人之间关系。假设这些PPGs能够在说话人规范化空间中表示语音的清晰度，并与说话人独立地对应。所提出的方法首先获得目标语音的PPGs。使用基于循环神经网络的深度双向长短时间记忆（DBLSTM）结构来学习目标语音的声学特征之间和PPGs的关系。要转换任意的一段源语音，需要从相同的SI-ASR获得其PPGs并将它们输入到训练好的DBLSTM中以生成转换语音。我们的方法有两个主要优点：（1）需要非平行训练数据；（2）训练好的模型可以将任意一个源说话人的语音转换成目标说话人的语音（即多对一转换）。实验表明我们的方法性能与最先进的系统在语音质量和说话人相似度方面相同或优之。</p><p><strong>参考</strong><br>[1] <em>Sun L, Li K, Wang H, et al. Phonetic posteriorgrams for many-to-one voice conversion without parallel data training[C]//2016 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2016: 1-6.</em><a href="http://www1.se.cuhk.edu.hk/~hccl/publications/pub/2016_paper_297.pdf">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>deep-voice-conversion</em><a href="https://github.com/andabi/deep-voice-conversion">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音转换 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> 声码器 </tag>
            
            <tag> 语音后验图 </tag>
            
            <tag> MCEP </tag>
            
            <tag> DTW </tag>
            
            <tag> DBLSTM </tag>
            
            <tag> Straight </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_20180323_Style Tokens：Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis</title>
      <link href="2020/12/28/Paper008/"/>
      <url>2020/12/28/Paper008/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>在这项工作中，我们提出了“全局风格标记”（GSTs），这是一个在最先进的端到端语音合成系统Tacotron中联合训练的嵌入库。嵌入训练没有明确的标签，但能够学习大范围的声学表征进行建模。GSTs产生了一系列丰富的显著结果。它们生成的可解释的软“标签”可用于以新型的方式控制合成，例如改变速度和独立于文本内容的说话风格。它们还可以用于风格转换，在整个长格式文本语料库中复制单个音频片段的说话风格。当对有噪声的、未标记的数据进行训练时，GSTs学习分解噪声和说话人身份信息，为实现高度可扩展但稳健的语音合成提供了一条途径。</p><p><strong>参考</strong><br>[1] <em>Wang Y, Stanton D, Zhang Y, et al. Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis[J]. arXiv preprint arXiv:1803.09017, 2018.</em><a href="https://arxiv.org/abs/1803.09017">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音转换 </tag>
            
            <tag> 说话人风格迁移 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> Tacotron </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Griffin-Lim </tag>
            
            <tag> WaveNet </tag>
            
            <tag> 声纹编码器 </tag>
            
            <tag> 韵律编码器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_ICML2018_Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron</title>
      <link href="2020/12/28/Paper007/"/>
      <url>2020/12/28/Paper007/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们提出一种学习潜在韵律嵌入空间的Tacotron扩展语音合成架构，其中所需要的韵律从一个参考声学表征中产生。我们发现，在这个学习的嵌入空间上训练Tacotron，即使在参考说话人与合成说话人不同的情况下，合成音频也能以精细的时间细节匹配到参考信号的韵律。此外，我们还发现，参考韵律嵌入可以用来合成不同于参考语句的文本。我们定义了几个评价韵律转换的定量和主观指标，并在一个韵律转换任务中报告了来自单个说话人和44个说话人Tacotron模型的相同音频样本的结果。</p><p><strong>参考</strong><br>[1] <em>Skerry-Ryan R J, Battenberg E, Xiao Y, et al. Towards end-to-end prosody transfer for expressive speech synthesis with tacotron[J]. arXiv preprint arXiv:1803.09047, 2018.</em><a href="https://arxiv.org/abs/1803.09047">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音转换 </tag>
            
            <tag> 说话人风格迁移 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> Tacotron </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Griffin-Lim </tag>
            
            <tag> WaveNet </tag>
            
            <tag> 韵律编码器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>斯坦福大学_2018_Storytime - End to end neural networks for audiobooks</title>
      <link href="2020/12/27/Paper006/"/>
      <url>2020/12/27/Paper006/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>有声读物是文本到语音合成的黄金标准。由于它们持续几个小时，所以它们韵律中的小特质很快就会使听众不知所措。Storytime的灵感源于我们可以使用神经网络将口语文本的流畅度提高到一个新的水平。更具体地说，我们探索了Google最近的学术出版物Tacotron的实现。它依靠编码器、解码器和注意力机制来模拟接近人类的声谱图。 截至该项目开始时，还没有公开的尝试复现其结果。 我们在探索他们的论文应用规格和建模决策的详细报告中取得了长足的进步，希望这可以推动最新技术发展，从而获得在实践中使用的类似模型。 最终，我们取得了模型概念的成功论证。</p><p><strong>参考</strong><br>[1] <em>Freeman P, Villegas E, Kamalu J. Storytime-End to end neural networks for audiobooks[J].</em><a href="http://static.tongtianta.site/paper_pdf/5ec16378-bf36-11e9-9da8-00163e08bb86.pdf">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 合成器 </tag>
            
            <tag> Tacotron </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Griffin-Lim </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_NeurIPS2018_Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis</title>
      <link href="2020/12/26/Paper005/"/>
      <url>2020/12/26/Paper005/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>我们描述了一种基于神经网络的文本到语音合成系统，该系统能够生成许多不同说话人音色的声音，包括在模型训练中未出现说话人的声音。我们的系统由三个独立训练的模块组成：（1）说话人编码器网络，使用来自成千上万说话人且没有抄本的独立带噪语音数据集对说话人识别任务进行训练，以从目标说话人的仅几秒参考语音中生成固定维度的嵌入矢量；（2）一个基于Tacotron2的序列到序列合成网络，该网络根据说话者的嵌入情况从文本生成梅尔频谱图；（3）一种基于WaveNet的自回归声码器，可将mel频谱图转换为一系列时域波形​​样本。我们证明了所提出的模型能够通过独立训练的说话人编码器把学到的说话人可变知识迁移到新任务（多说话人语音合成）中，并且能够合成在训练过程中未出现说话人的自然声音。我们量化了在大量多样的说话人数据集上训练说话人编码器的重要性，以获得最佳的泛化性能。最后表明，随机采样的说话人嵌入方法可用于合成不同于模型训练所使用说话人的新颖说话人音色的语音，这说明该模型已学到了高质量的说话人表征。</p><p><strong>参考</strong><br>[1] <em>Jia Y, Zhang Y, Weiss R, et al. Transfer learning from speaker verification to multispeaker text-to-speech synthesis[C]//Advances in neural information processing systems. 2018: 4480-4490.</em><a href="https://arxiv.org/abs/1806.04558">[pdf]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 语音转换 </tag>
            
            <tag> 说话人认证 </tag>
            
            <tag> 说话人风格迁移 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Tacotron2 </tag>
            
            <tag> WaveNet </tag>
            
            <tag> 声纹编码器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学_20200622_FastSpeech 2：Fast and High-Quality End-to-End Text to Speech</title>
      <link href="2020/12/26/Paper004/"/>
      <url>2020/12/26/Paper004/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>非自回归文本语音转换（TTS）模型（例如FastSpeech）能够比以往的自回归模型以可比的质量明显更快地合成语音。 FastSpeech模型的训练依赖于持续时间预测（以提供更多信息作为输入）和知识蒸馏（以简化输出中的数据分布）的自回归教师模型，这可以缓解一对多映射问题（即多个语音变化对应于TTS中的相同文本）。但是，FastSpeech有几个缺点：（1）师生蒸馏管道复杂且耗时；（2）从老师模型中提取的持续时间不够准确，并且从老师模型中提取的目标梅尔谱图由于数据简化导致信息损失，这两者都限制了语音质量。在本文中，我们提出FastSpeech2，它解决了FastSpeech中的问题，并通过以下方法可以更好地解决TTS中的一对多映射问题：（1）直接训练具有原始频谱的模型，而不是老师模型的简化输出；2）引入更多语音变化信息（例如音调、能量和更准确的持续时间）作为条件输入。具体来说，我们从语音波形中提取持续时间、音调和能量，并将它们直接用作训练中的条件输入，并在推理中使用预测值。我们进一步设计了FastSpeech2s，这是首次尝试直接从文本并行直接生成语音波形，从而享有完全端到端推理的优势。实验结果表明：（1）FastSpeech2的训练速度是FastSpeech的3倍，而FastSpeech2s的推理速度更快。（2）FastSpeech2和2s的语音质量优于FastSpeech，FastSpeech2甚至可以超越自回归模型。</p><p><strong>参考</strong><br>[1] <em>Ren Y, Hu C, Qin T, et al. FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech[J]. arXiv preprint arXiv:2006.04558, 2020.</em><a href="https://arxiv.org/abs/2006.04558">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>FastSpeech2</em><a href="https://github.com/ming024/FastSpeech2">[GitHub]</a><br>[2] <em>FastSpeech2</em><a href="https://github.com/rishikksh20/FastSpeech2">[GitHub]</a><br>[3] <em>TensorflowTTS</em><a href="https://github.com/TensorSpeech/TensorflowTTS">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> 声码器 </tag>
            
            <tag> WaveGlow </tag>
            
            <tag> FastSpeech2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浙江大学_NeurIP2019_FastSpeech：Fast, Robust and Controllable Text to Speech</title>
      <link href="2020/12/26/Paper003/"/>
      <url>2020/12/26/Paper003/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>基于神经网络的端到端文本到语音（TTS）大大提高了合成语音的质量。突出的方法（例如Tacotron2）通常首先从文本生成梅尔频谱图，然后使用诸如WaveNet的声码器从梅尔频谱图合成语音。与传统的拼接和统计参数方法相比，基于神经网络的端到端模型的推理速度较慢，并且合成语音通常不稳定（即某些单词被跳过或重复）且缺乏可控性（语音速度或韵律控制）。在这项工作中，我们提出了一种基于Transformer的新型前馈网络，可为TTS并行生成梅尔频谱图。具体来说，我们从基于编码器-解码器的教师模型中提取注意力对齐，以进行音素持续时间预测，长度调节器将其用于扩展源音素序列，以匹配目标梅尔谱图序列的长度，进而生成并行梅尔谱图。在LJSpeech数据集上的实验表明，我们的并行模型在语音质量方面与自回归模型旗鼓相当，几乎消除了在特别困难的情况下单词跳过和重复的问题，并且可以平滑地调整语音速度。最重要的是，与自回归Transformer TTS相比，我们的模型将梅尔频谱图生成速度提高了270倍，将端到端语音合成速度提高了38倍。因此，我们将我们的模型称为FastSpeech。</p><p><strong>参考</strong><br>[1] <em>Ren Y, Ruan Y, Tan X, et al. Fastspeech: Fast, robust and controllable text to speech[J]. Advances in Neural Information Processing Systems, 2019, 32: 3171-3180.</em><a href="https://arxiv.org/abs/1905.09263">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>espnet</em><a href="https://github.com/espnet/espnet">[GitHub]</a><br>[2] <em>Fastspeech_TensorFlow</em><a href="https://github.com/TensorSpeech/TensorflowTTS">[GitHub]</a><br>[3] <em>Fastspeech_PyTorch</em><a href="https://github.com/xcmyz/FastSpeech">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> 合成器 </tag>
            
            <tag> 声码器 </tag>
            
            <tag> FastSpeech </tag>
            
            <tag> WaveGlow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_ICASSP2018_Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions</title>
      <link href="2020/12/26/Paper002/"/>
      <url>2020/12/26/Paper002/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>本文介绍了Tacotron2，这是一种直接从文本进行语音合成的神经网络结构。该系统由一个递归的序列到序列特征预测网络组成，该网络将字符嵌入映射到梅尔谱图，然后是一个修改的WaveNet模型，该模型充当声码器，负责把梅尔谱图合成时域波形。我们的模型获得平均意见得分（MOS）为4.53，这与MOS为4.58的专业录音相当。为了验证我们的设计选择，我们提出了对系统关键组件的消融（控制变量）研究，并评估了使用梅尔谱图作为WaveNet的输入（而不是语言特征，持续时间和F0特征）的影响。我们进一步证明，使用紧凑的声学中间表征可以显著简化WaveNet架构。</p><p><strong>参考</strong><br>[1] <em>Shen J, Pang R, Weiss R J, et al. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions[C]//2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018: 4779-4783.</em><a href="https://arxiv.org/abs/1712.05884">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>Tacotron-2</em><a href="https://github.com/Rayhane-mamah/Tacotron-2">[GitHub]</a><br>[2] <em>tacotron</em><a href="https://github.com/keithito/tacotron">[GitHub]</a><br>[3] <em>tacotron</em><a href="https://github.com/begeekmyfriend/tacotron">[GitHub]</a><br>[4] <em>Tacotron</em><a href="https://github.com/barronalex/Tacotron">[GitHub]</a><br>[5] <em>tacotron</em><a href="https://github.com/Kyubyong/tacotron">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 合成器 </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Tacotron2 </tag>
            
            <tag> WaveNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>谷歌_Interspeech2017_Tacotron：Towards End-to-End Speech Synthesis</title>
      <link href="2020/12/23/Paper001/"/>
      <url>2020/12/23/Paper001/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>摘要</strong><br>文本到语音合成系统通常包含多个阶段，例如前端文本分析，声学模型和音频合成模块。构建这些组件通常需要广泛的领域专业知识，并且可能包含生硬的设计选择。在本文中，我们介绍了Tacotron，这是一种从文字直接合成语音的端到端生成文本到语音模型。给定文本及其音频，可以使用随机初始化从头开始完全训练模型。我们提出了几种关键技术，以使序列到序列框架能够很好地完成这一具有挑战性的任务。Tacotron在美国英语上获得主观五等级MOS为3.82，就自然度而言，其表现优于参数生成系统。此外，由于Tacotron在帧级生成语音，它比样本级自回归方法快得多。</p><p><strong>参考</strong><br>[1] <em>Wang Y, Skerry-Ryan R J, Stanton D, et al. Tacotron: Towards end-to-end speech synthesis[J]. arXiv preprint arXiv:1703.10135, 2017.</em><a href="https://arxiv.org/abs/1703.10135">[pdf]</a></p><p><strong>源码</strong><br>[1] <em>Tacotron_TensorFlow</em><a href="https://github.com/r9y9/Tacotron-2">[GitHub]</a><br>[2] <em>Tacotron_PyTorch</em><a href="https://github.com/r9y9/tacotron_pytorch">[GitHub]</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语音合成 </tag>
            
            <tag> End-to-End </tag>
            
            <tag> 合成器 </tag>
            
            <tag> Tacotron </tag>
            
            <tag> 声码器 </tag>
            
            <tag> Griffin-Lim </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速修改Ubuntu镜像源以加速下载与安装</title>
      <link href="2020/12/20/Blog003/"/>
      <url>2020/12/20/Blog003/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-修改apt-apt-get镜像源"><a href="#1-修改apt-apt-get镜像源" class="headerlink" title="(1)修改apt/apt-get镜像源"></a>(1)修改apt/apt-get镜像源</h1><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/apt/sources.list</span><br></pre></td></tr></table></figure><h2 id="注释掉旧镜像源，添加相应的内容"><a href="#注释掉旧镜像源，添加相应的内容" class="headerlink" title="注释掉旧镜像源，添加相应的内容"></a>注释掉旧镜像源，添加相应的内容</h2><p>（以清华镜像源为例）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 清华源</span></span><br><span class="line"><span class="string">deb</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/ubuntu/</span> <span class="string">bionic</span> <span class="string">main</span> <span class="string">restricted</span> <span class="string">universe</span> <span class="string">multiverse</span></span><br><span class="line"><span class="string">deb-src</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/ubuntu/</span> <span class="string">bionic</span> <span class="string">main</span> <span class="string">restricted</span> <span class="string">universe</span> <span class="string">multiverse</span></span><br><span class="line"><span class="string">deb</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/ubuntu/</span> <span class="string">bionic-updates</span> <span class="string">main</span> <span class="string">restricted</span> <span class="string">universe</span> <span class="string">multiverse</span></span><br><span class="line"><span class="string">deb-src</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/ubuntu/</span> <span class="string">bionic-updates</span> <span class="string">main</span> <span class="string">restricted</span> <span class="string">universe</span> <span class="string">multiverse</span></span><br><span class="line"><span class="string">deb</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/ubuntu/</span> <span class="string">bionic-backports</span> <span class="string">main</span> <span class="string">restricted</span> <span class="string">universe</span> <span class="string">multiverse</span></span><br><span class="line"><span class="string">deb-src</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/ubuntu/</span> <span class="string">bionic-backports</span> <span class="string">main</span> <span class="string">restricted</span> <span class="string">universe</span> <span class="string">multiverse</span></span><br><span class="line"><span class="string">deb</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/ubuntu/</span> <span class="string">bionic-security</span> <span class="string">main</span> <span class="string">restricted</span> <span class="string">universe</span> <span class="string">multiverse</span></span><br><span class="line"><span class="string">deb-src</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/ubuntu/</span> <span class="string">bionic-security</span> <span class="string">main</span> <span class="string">restricted</span> <span class="string">universe</span> <span class="string">multiverse</span></span><br><span class="line"><span class="string">deb</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/ubuntu/</span> <span class="string">bionic-proposed</span> <span class="string">main</span> <span class="string">restricted</span> <span class="string">universe</span> <span class="string">multiverse</span></span><br><span class="line"><span class="string">deb-src</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/ubuntu/</span> <span class="string">bionic-proposed</span> <span class="string">main</span> <span class="string">restricted</span> <span class="string">universe</span> <span class="string">multiverse</span></span><br><span class="line"><span class="string">```shell</span></span><br><span class="line"><span class="string">也可在软件和更新中选择清华源，更方便快捷。</span></span><br><span class="line"><span class="comment">## 最后更新软件列表即可</span></span><br><span class="line"><span class="string">```shell</span></span><br><span class="line"><span class="string">sudo</span> <span class="string">apt-get</span> <span class="string">update</span></span><br></pre></td></tr></table></figure><hr><h1 id="2-修改pip镜像源"><a href="#2-修改pip镜像源" class="headerlink" title="(2)修改pip镜像源"></a>(2)修改pip镜像源</h1><h2 id="修改配置文件-1"><a href="#修改配置文件-1" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit  ~/.pip/pip.conf</span><br></pre></td></tr></table></figure><h2 id="添加相应的内容即可"><a href="#添加相应的内容即可" class="headerlink" title="添加相应的内容即可"></a>添加相应的内容即可</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">global</span>]</span><br><span class="line"><span class="string">timeout</span> <span class="string">=</span> <span class="number">6000</span></span><br><span class="line"><span class="string">index-url</span> <span class="string">=</span> <span class="string">https://pypi.tuna.tsinghua.edu.cn/simple</span></span><br><span class="line"><span class="string">trusted-host</span> <span class="string">=</span> <span class="string">pypi.tuna.tsinghua.edu.cn</span></span><br></pre></td></tr></table></figure><hr><h1 id="3-修改conda镜像源"><a href="#3-修改conda镜像源" class="headerlink" title="(3)修改conda镜像源"></a>(3)修改conda镜像源</h1><h2 id="修改配置文件-2"><a href="#修改配置文件-2" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/.condarc</span><br></pre></td></tr></table></figure><h2 id="添加相应的内容即可-1"><a href="#添加相应的内容即可-1" class="headerlink" title="添加相应的内容即可"></a>添加相应的内容即可</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">channels:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span></span><br><span class="line"><span class="attr">ssl_verify:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络技术 </tag>
            
            <tag> Linux </tag>
            
            <tag> Ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速搭建与使用网络热点服务</title>
      <link href="2020/12/05/Blog002/"/>
      <url>2020/12/05/Blog002/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install  hostapd dnsmasq</span><br><span class="line">git clone https://github.com/binglel/create_ap</span><br><span class="line">cd create_ap</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure><h1 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h1><ul><li><p>No passphrase (open network)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create_ap wlp3s0 enp2s0 hulahula</span><br></pre></td></tr></table></figure></li><li><p>WPA + WPA2 passphrase</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create_ap wlp3s0 enp2s0 hulahula 13001150505</span><br></pre></td></tr></table></figure></li></ul><h1 id="监控客户端"><a href="#监控客户端" class="headerlink" title="监控客户端"></a>监控客户端</h1><ul><li><p>首先查看正在运行的网络热点服务及其PID</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create_ap --list-running</span><br></pre></td></tr></table></figure></li><li><p>然后通过PID查看连接当前网络热点服务的客户端（以7170为例）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create_ap --list-clients 7170</span><br></pre></td></tr></table></figure></li></ul><p>注意，若在服务启动过程中，有如下报错：</p><pre>Failed to initialize lock error</pre><p>则解决方法如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rm /tmp/create_ap.all.lock</span><br></pre></td></tr></table></figure><p>最后重新启动服务即可。</p>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络技术 </tag>
            
            <tag> Wi-Fi </tag>
            
            <tag> 网络热点服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>装修漏水致房屋受损，业主获赔3千却付了近两万的鉴定费</title>
      <link href="2020/11/16/Tale001/"/>
      <url>2020/11/16/Tale001/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>一场因装修引发的邻里纠纷中，杨女士获赔3千多元，却付出了近2万元的鉴定费。</p><p>因楼上装修导致房屋两次漏水，双方就赔偿事宜协商未果，业主杨女士将装修邻居马先生和装修公司诉至法院，要求赔礼道歉、赔偿财产和精神损失共计5.4万元。日前，海淀法院审结了此案。法院判决装修公司赔偿杨女士损失3064.62元。</p><h1 id="案情简介"><a href="#案情简介" class="headerlink" title="案情简介"></a>案情简介</h1><p>杨女士诉称，楼上住户马先生装修房屋，因装修公司施工不当导致家中天花板两次漏水。后双方协商赔偿事宜但没有达成一致，故诉至法院，要求马先生和装修公司赔礼道歉，并赔偿财产和精神损失共计5.4万元。</p><p>被告马先生辩称，渗水后其积极与杨女士协商，愿意给杨女士无偿修复好，也同意合理的赔偿，但杨女士要求的赔偿金额过高。</p><p>被告装修公司辩称，因装修时地面找平造成渗水，但并未造成其他损失，公司愿意承担所有责任，愿意免费修复并积极赔偿，但是杨女士主张的数额过高且不合理，无法同意其诉讼请求。</p><h1 id="法院判决"><a href="#法院判决" class="headerlink" title="法院判决"></a>法院判决</h1><p>法院经审理认为，在事发后以及本案审理过程中，装修公司均表示同意给杨女士家免费修复漏水受损部位，并赔偿一定数额的损失。因鉴定费用较高，反而可能会扩大双方的损失，法官建议杨女士选择有利于双方的调解方式。</p><p>但杨女士不同意装修公司进行修复，仍坚持申请对自家房屋因渗水造成的损失进行修复方案鉴定及造价评估。</p><p>在评估过程中，杨女士另外提出要求鉴定机构对受损部位之外的墙面、地面等多处的修复费用单方造价进行评估，主要理由是天花板粉刷后与墙壁会有色差，施工中会对四壁及周边有污损。</p><p>经鉴定，鉴定机构出具鉴定意见书认定受损部位造价仅为3064.62元，杨女士自行增加鉴定的部分造价5474.36元。杨女士共支付鉴定费18000元。</p><p>对于杨女士因渗水造成的损失，应由装修公司承担相应责任；但因双方在庭审中已经对受损部位进行了确认，因此杨女士在鉴定过程中提出的受损部位以外的修复费用，缺乏相应的事实和法律依据，法院不予支持；装修公司为具有相应资质的专业装修机构，已经表示同意免费维修并赔偿一定数额的损失，但杨女士仍然不同意装修公司进行维修，故本案鉴定费用是杨女士为证明其主张的举证支出，应由其自行承担。</p><p>最终，法院判决装修公司赔偿杨女士因渗水而造成损失3064.62元，鉴定费18000元由杨女士自行承担。</p><p>宣判后，双方均未提出上诉。</p><h1 id="法官提示"><a href="#法官提示" class="headerlink" title="法官提示"></a>法官提示</h1><p>漏水发生后，会严重影响楼下业主的生活，甚至会造成业主财产损失，楼下业主的焦虑心情可以理解。但是当我们遇上此类事情时，一定要理性对待，否则反而可能会像上述案例一样造成不必要的损失。</p><p>首先，装修公司因施工不当造成他人损失，一定要提供妥善的解决方案，避免进一步激化矛盾；</p><p>其次，作为受损方，在他人已经释出诚意时，能够协商解决的，尽量通过双方协商或者在居委会、法院等第三方的主持下，妥善解决问题。如果盲目提出高价索赔，不仅问题得不到解决，反而可能进一步将损失扩大；</p><p>最后，一般来说，漏水案件会涉及漏水成因、修复方案、修复方案造价等鉴定项目，特别是前两个鉴定，鉴定费均在万元以上。一些当事人可能因为不清楚诉讼及鉴定的成本而一味寻求诉讼的方式解决问题，法院作为中立的第三方，在遇到此类案件时会从专业的角度以及过往此类案件的审理结果等方面向当事人进行充分释明。如果当事人不能确定自己的损失，还可以通过多种渠道，如询问业内其他装修公司等，让双方都对自己的损失有一个合理的预期，更容易达成一致意见进行调解。这样不仅可以避免不必要的诉累，也可以避免其他社会资源的浪费。</p>]]></content>
      
      
      <categories>
          
          <category> Tale </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 法律故事 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速搭建与使用SOCKS5代理服务</title>
      <link href="2020/11/15/Blog001/"/>
      <url>2020/11/15/Blog001/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/binglel/socks5_python</span><br><span class="line">cd socks5_python</span><br><span class="line">python socks5.py -h</span><br><span class="line">python socks5.py start --port=1080 --auth=bulabula:31415926</span><br></pre></td></tr></table></figure><h1 id="停止"><a href="#停止" class="headerlink" title="停止"></a>停止</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python socks5.py stop</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> SOCK5 </tag>
            
            <tag> 网络技术 </tag>
            
            <tag> 代理服务 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
